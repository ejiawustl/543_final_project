{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "691df71a",
      "metadata": {
        "id": "691df71a"
      },
      "source": [
        "# Evaluating Model Performance for Predicting 30-Day Hospital Readmissions\n",
        "\n",
        "Eric Jia, Scott Yamamoto\n",
        "\n",
        "# Overview\n",
        "\n",
        "This is the second of two notebooks and contains the code used to compare different models' performances, factoring into account the differences in performance using standard/weighted BCE and also using only the structured data vs. the combined structured data + embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2374f9b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2374f9b3",
        "outputId": "8f7eef7a-15e6-4f15-b39c-c3f953d6e825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn pandas matplotlib seaborn numpy torch tqdm nbformat pytorch-tabnet --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fcc7e7a2",
      "metadata": {
        "id": "fcc7e7a2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np, time, json, matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, roc_curve)\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook reproduces all models and metrics:\n",
        "\n",
        "* Logistic Regression  \n",
        "* Multilayer Perceptron (MLP)\n",
        "* FT-Transformer\n",
        "* TabNet Classifier  \n",
        "* XGBoost\n",
        "\n",
        "We use the data was was generated from the previous notebook (543t_nb1)."
      ],
      "metadata": {
        "id": "QAq9cmu5yOK8"
      },
      "id": "QAq9cmu5yOK8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, torch, torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, precision_recall_curve, roc_curve)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "def metric_dict(y_true, y_pred, prob, rt):\n",
        "    return dict(\n",
        "        Accuracy  = accuracy_score(y_true, y_pred),\n",
        "        Precision = precision_score(y_true, y_pred, zero_division=0),\n",
        "        Recall    = recall_score(y_true, y_pred),\n",
        "        F1        = f1_score(y_true, y_pred),\n",
        "        ROC_AUC   = roc_auc_score(y_true, prob),\n",
        "        Runtime_s = rt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHYJek82ySEt",
        "outputId": "f152e8bc-4d74-4fc6-a9f4-5f453d89b271"
      },
      "id": "tHYJek82ySEt",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the data\n",
        "DATA_PATH = 'data/full_final_df.csv'\n",
        "df_full = pd.read_csv(DATA_PATH, index_col=0)"
      ],
      "metadata": {
        "id": "PtdC-vQ40OIB"
      },
      "id": "PtdC-vQ40OIB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Processing Steps\n",
        "\n",
        "Standarize the numerical data, apply one-hot encoding for the categorical data."
      ],
      "metadata": {
        "id": "URBZ8dtd-h-5"
      },
      "id": "URBZ8dtd-h-5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and continuous variables\n",
        "categorical_cols = [\"INSURANCE\", \"GENDER\", \"MARITAL_STATUS\"]\n",
        "continuous_cols = [\n",
        "    \"UREA_N_MIN\", \"UREA_N_MAX\", \"UREA_N_MEAN\", \"PLATELETS_MIN\",\n",
        "    \"PLATELETS_MAX\", \"PLATELETS_MEAN\", \"MAGNESIUM_MIN\", \"MAGNESIUM_MAX\",\n",
        "    \"MAGNESIUM_MEAN\", \"ALBUMIN_MIN\", \"ALBUMIN_MAX\", \"ALBUMIN_MEAN\",\n",
        "    \"CALCIUM_MIN\", \"CALCIUM_MAX\", \"CALCIUM_MEAN\", \"RESP_RATE_MIN\",\n",
        "    \"RESP_RATE_MAX\", \"RESP_RATE_MEAN\", \"HR_MIN\", \"HR_MAX\", \"HR_MEAN\",\n",
        "    \"SYSBP_MIN\", \"SYSBP_MAX\", \"SYSBP_MEAN\", \"DIASBP_MIN\", \"DIASBP_MAX\",\n",
        "    \"DIASBP_MEAN\", \"GLUCOSE_MIN\", \"GLUCOSE_MAX\", \"GLUCOSE_MEAN\"\n",
        "]\n",
        "\n",
        "target_col = [\"FUTURE_READMIT\", \"TEXT\"]\n",
        "selected_cols = categorical_cols + continuous_cols + target_col\n",
        "sampled_df = df_full[selected_cols]\n",
        "\n",
        "# apply one-hot encoding\n",
        "sampled_df_encoded = pd.get_dummies(sampled_df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# convert cols to nuermical values\n",
        "sampled_df_encoded[\"FUTURE_READMIT\"] = sampled_df_encoded[\"FUTURE_READMIT\"].map({\"Yes\": 1, \"No\": 0})\n",
        "bool_cols = sampled_df_encoded.select_dtypes(include=['bool']).columns\n",
        "sampled_df_encoded[bool_cols] = sampled_df_encoded[bool_cols].astype(int)\n",
        "\n",
        "# standardize data\n",
        "scaler = StandardScaler()\n",
        "sampled_df_encoded[continuous_cols] = scaler.fit_transform(sampled_df_encoded[continuous_cols])\n",
        "\n",
        "X = sampled_df_encoded.drop(columns=['FUTURE_READMIT']).values\n",
        "y = sampled_df_encoded['FUTURE_READMIT'].values.astype(int)"
      ],
      "metadata": {
        "id": "xv9-ml4N1pbb"
      },
      "id": "xv9-ml4N1pbb",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_COL = \"TEXT\"\n",
        "numeric_block = sampled_df_encoded.drop(columns=[TEXT_COL, 'FUTURE_READMIT']).values.astype(np.float32)\n",
        "texts         = sampled_df_encoded[TEXT_COL].fillna(\" \").tolist()\n",
        "\n",
        "print(\"Numeric block shape:\", numeric_block.shape)\n",
        "print(\"Number of texts    :\", len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a7WUn6E2b-6",
        "outputId": "bce938d6-3148-4879-addd-f60c9cd9ccbe"
      },
      "id": "0a7WUn6E2b-6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric block shape: (20948, 41)\n",
            "Number of texts    : 20948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed discharge summaries using ClinicalBERT\n",
        "\n",
        "Used the github repository found here: https://github.com/EmilyAlsentzer/clinicalBERT"
      ],
      "metadata": {
        "id": "FEdfYIWo2ju9"
      },
      "id": "FEdfYIWo2ju9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers sentencepiece tqdm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch, tqdm, numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model     = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device).eval()\n",
        "\n",
        "MAX_LEN   = 512\n",
        "BATCH_TXT = 16\n",
        "\n",
        "def embed_batch(txt_list):\n",
        "    enc = tokenizer(txt_list, padding=True, truncation=True,\n",
        "                    max_length=MAX_LEN, return_tensors='pt')\n",
        "    enc = {k: v.to(device) for k,v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc).last_hidden_state\n",
        "    mask = enc['attention_mask'].unsqueeze(-1)\n",
        "    # mean-pool\n",
        "    pooled = (out * mask).sum(1) / mask.sum(1)\n",
        "    return pooled.cpu().numpy().astype(np.float32)\n",
        "\n",
        "emb_list = []\n",
        "for i in tqdm.trange(0, len(texts), BATCH_TXT, desc=\"Embedding\"):\n",
        "    emb_list.append(embed_batch(texts[i:i+BATCH_TXT]))\n",
        "embeddings = np.vstack(emb_list)\n",
        "print(\"Embeddings shape:\", embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c21f92b8018345058b8e5c5a59b1ae94",
            "1f725be7102a4e9595e64f14a77a3941",
            "8c585ba63ae84cf28d879c1cfe445f5d",
            "a99e4cc07b9940ada94b459c9018c90d",
            "8443279c10364026b3550c0e1268edcd",
            "ea96df16b2004df4861062ee2023cbed",
            "308f3c686c324f8aa66edc82f06f7f07",
            "2a4fcc7f171e4336a9337321e86ba40f",
            "9adfb873bbf44d068aedbf0cc3eca483",
            "8e9ac14ad0b74ceebc0ccb378c46d1c6",
            "c489b9472fc449b69951cafa9bf567f1",
            "cb0281855bf04025b016a9e7540da629",
            "8e986e60c6cf44f6a92729ace62ca3cb",
            "3fcfc3ab4b85444ba038e4e4bb9d3fbf",
            "f15e958245b341c09eaa75c1f62cbe7d",
            "e916b6fd40e6457a9e25c9d7eb3c121b",
            "7fbb4c14ce954cf9a28973f43d0d6442",
            "eb3e87111e6046f5aeb295c0ff8419a2",
            "bb3d373ac40744e09bfd00241c7c2179",
            "38560c946b15427192506e07ff0d0efb",
            "5ba727246cd048a68e96c8a230a0ef02",
            "ec6350facef44aeea6abb70abbd3e706",
            "c5cce1173a434a09968b17a2c23eaebc",
            "b40710d99b1c4b119fa1b3052995ba43",
            "8761f49979ed484eab194fcfc2a4d858",
            "8481f6eecfce47c38d816c053b210741",
            "f40ad7eae10f46bc885c3aa38ac091e6",
            "61409a6c1fc64321838d5a7629675c20",
            "c8b8f43bf1c742f89211a2cab87d2987",
            "01a5cb46bf5f4c348e67cdf681ea69d2",
            "805b6750c1f44360813fc7fd334620f0",
            "2277fcbd1a4b4124ab8f86c81fde97cd",
            "44fc305f713d454cbfe97d7df0ba54a2",
            "ba0a0de6d7da4333b2ff55b063404361",
            "ae31b37030da4652b7efaa2e555815ea",
            "48d7fec75a4941258209a5755a4d6a6f",
            "0a623ee1b7a84cd281589fb668d26fb2",
            "d631c9b673f048e0ae50a9b0a7001beb",
            "f207d3ae9ede4ab594603223ddfdd5b8",
            "f1e707a16f4646a783f6fbe2c4b8fcbf",
            "a13e272dec364caf805525234291a901",
            "2aff07fd629b4ab59b5a384290c07fa4",
            "caa2ed2073514e86a338028a1c3e6883",
            "52e6d07630ad46e3935f2352408291ab"
          ]
        },
        "id": "Hrs8QN9x2iEe",
        "outputId": "7a93eff9-4399-4292-b1dc-e3484d157d1e"
      },
      "id": "Hrs8QN9x2iEe",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c21f92b8018345058b8e5c5a59b1ae94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb0281855bf04025b016a9e7540da629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5cce1173a434a09968b17a2c23eaebc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba0a0de6d7da4333b2ff55b063404361"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Embedding:   0%|          | 0/1310 [00:00<?, ?it/s]\u001b[A\n",
            "Embedding:   0%|          | 1/1310 [00:00<18:33,  1.18it/s]\u001b[A\n",
            "Embedding:   0%|          | 2/1310 [00:01<10:42,  2.04it/s]\u001b[A\n",
            "Embedding:   0%|          | 3/1310 [00:01<08:04,  2.70it/s]\u001b[A\n",
            "Embedding:   0%|          | 4/1310 [00:01<06:56,  3.13it/s]\u001b[A\n",
            "Embedding:   0%|          | 5/1310 [00:01<06:13,  3.49it/s]\u001b[A\n",
            "Embedding:   0%|          | 6/1310 [00:02<06:03,  3.59it/s]\u001b[A\n",
            "Embedding:   1%|          | 7/1310 [00:02<05:45,  3.77it/s]\u001b[A\n",
            "Embedding:   1%|          | 8/1310 [00:02<05:27,  3.97it/s]\u001b[A\n",
            "Embedding:   1%|          | 9/1310 [00:02<05:20,  4.06it/s]\u001b[A\n",
            "Embedding:   1%|          | 10/1310 [00:02<05:11,  4.17it/s]\u001b[A\n",
            "Embedding:   1%|          | 11/1310 [00:03<05:07,  4.23it/s]\u001b[A\n",
            "Embedding:   1%|          | 12/1310 [00:03<05:04,  4.26it/s]\u001b[A\n",
            "Embedding:   1%|          | 13/1310 [00:03<05:00,  4.32it/s]\u001b[A\n",
            "Embedding:   1%|          | 14/1310 [00:03<04:57,  4.36it/s]\u001b[A\n",
            "Embedding:   1%|          | 15/1310 [00:04<04:56,  4.37it/s]\u001b[A\n",
            "Embedding:   1%|          | 16/1310 [00:04<04:54,  4.40it/s]\u001b[A\n",
            "Embedding:   1%|▏         | 17/1310 [00:04<04:57,  4.35it/s]\u001b[A\n",
            "Embedding:   1%|▏         | 18/1310 [00:04<04:55,  4.38it/s]\u001b[A\n",
            "Embedding:   1%|▏         | 19/1310 [00:05<04:56,  4.36it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 20/1310 [00:05<04:53,  4.39it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 21/1310 [00:05<04:54,  4.37it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 22/1310 [00:05<04:56,  4.34it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 23/1310 [00:05<04:58,  4.31it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 24/1310 [00:06<04:56,  4.34it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 25/1310 [00:06<04:59,  4.29it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 26/1310 [00:06<04:57,  4.31it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 27/1310 [00:06<04:57,  4.31it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 28/1310 [00:07<04:56,  4.33it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 29/1310 [00:07<04:55,  4.34it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 30/1310 [00:07<04:55,  4.34it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 31/1310 [00:07<04:55,  4.33it/s]\u001b[A\n",
            "Embedding:   2%|▏         | 32/1310 [00:08<04:56,  4.31it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 33/1310 [00:08<04:52,  4.36it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 34/1310 [00:08<04:52,  4.36it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 35/1310 [00:08<04:51,  4.37it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 36/1310 [00:08<04:53,  4.33it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 37/1310 [00:09<04:50,  4.38it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 38/1310 [00:09<04:50,  4.38it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 39/1310 [00:09<04:50,  4.38it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 40/1310 [00:09<04:49,  4.38it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 41/1310 [00:10<04:48,  4.39it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 42/1310 [00:10<04:46,  4.43it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 43/1310 [00:10<04:46,  4.42it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 44/1310 [00:10<04:46,  4.41it/s]\u001b[A\n",
            "Embedding:   3%|▎         | 45/1310 [00:10<04:47,  4.41it/s]\u001b[A\n",
            "Embedding:   4%|▎         | 46/1310 [00:11<04:43,  4.46it/s]\u001b[A\n",
            "Embedding:   4%|▎         | 47/1310 [00:11<04:45,  4.42it/s]\u001b[A\n",
            "Embedding:   4%|▎         | 48/1310 [00:11<04:46,  4.40it/s]\u001b[A\n",
            "Embedding:   4%|▎         | 49/1310 [00:11<04:45,  4.42it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 50/1310 [00:12<04:48,  4.37it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 51/1310 [00:12<04:47,  4.37it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 52/1310 [00:12<04:49,  4.35it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 53/1310 [00:12<04:53,  4.29it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 54/1310 [00:13<04:50,  4.32it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 55/1310 [00:13<04:50,  4.32it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 56/1310 [00:13<04:50,  4.31it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 57/1310 [00:13<04:51,  4.30it/s]\u001b[A\n",
            "Embedding:   4%|▍         | 58/1310 [00:13<04:53,  4.27it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 59/1310 [00:14<04:52,  4.27it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 60/1310 [00:14<04:49,  4.31it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 61/1310 [00:14<04:48,  4.33it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 62/1310 [00:14<04:47,  4.35it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 63/1310 [00:15<04:52,  4.26it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 64/1310 [00:15<04:52,  4.26it/s]\u001b[A\n",
            "Embedding:   5%|▍         | 65/1310 [00:15<04:52,  4.25it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 66/1310 [00:15<04:51,  4.27it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 67/1310 [00:16<04:53,  4.23it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 68/1310 [00:16<04:53,  4.24it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 69/1310 [00:16<04:52,  4.24it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 70/1310 [00:16<04:50,  4.27it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 71/1310 [00:17<04:54,  4.20it/s]\u001b[A\n",
            "Embedding:   5%|▌         | 72/1310 [00:17<04:51,  4.24it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 73/1310 [00:17<04:49,  4.27it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 74/1310 [00:17<04:47,  4.30it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 75/1310 [00:17<04:47,  4.29it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 76/1310 [00:18<04:48,  4.28it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 77/1310 [00:18<04:47,  4.29it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 78/1310 [00:18<04:45,  4.32it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 79/1310 [00:18<04:42,  4.35it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 80/1310 [00:19<04:46,  4.30it/s]\u001b[A\n",
            "Embedding:   6%|▌         | 81/1310 [00:19<05:03,  4.05it/s]\u001b[A\n",
            "Embedding:   6%|▋         | 82/1310 [00:19<04:55,  4.15it/s]\u001b[A\n",
            "Embedding:   6%|▋         | 83/1310 [00:19<04:50,  4.22it/s]\u001b[A\n",
            "Embedding:   6%|▋         | 84/1310 [00:20<04:54,  4.16it/s]\u001b[A\n",
            "Embedding:   6%|▋         | 85/1310 [00:20<04:54,  4.16it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 86/1310 [00:20<04:49,  4.22it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 87/1310 [00:20<04:45,  4.28it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 88/1310 [00:21<04:47,  4.24it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 89/1310 [00:21<04:46,  4.27it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 90/1310 [00:21<04:44,  4.28it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 91/1310 [00:21<04:43,  4.29it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 92/1310 [00:21<04:46,  4.26it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 93/1310 [00:22<04:47,  4.23it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 94/1310 [00:22<04:45,  4.25it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 95/1310 [00:22<04:46,  4.24it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 96/1310 [00:22<04:41,  4.32it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 97/1310 [00:23<04:41,  4.30it/s]\u001b[A\n",
            "Embedding:   7%|▋         | 98/1310 [00:23<04:41,  4.31it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 99/1310 [00:23<04:46,  4.23it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 100/1310 [00:23<04:45,  4.23it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 101/1310 [00:24<04:43,  4.26it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 102/1310 [00:24<04:44,  4.24it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 103/1310 [00:24<04:43,  4.25it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 104/1310 [00:24<04:44,  4.24it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 105/1310 [00:25<04:43,  4.26it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 106/1310 [00:25<04:41,  4.27it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 107/1310 [00:25<04:40,  4.29it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 108/1310 [00:25<04:40,  4.29it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 109/1310 [00:25<04:40,  4.28it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 110/1310 [00:26<04:43,  4.24it/s]\u001b[A\n",
            "Embedding:   8%|▊         | 111/1310 [00:26<04:42,  4.25it/s]\u001b[A\n",
            "Embedding:   9%|▊         | 112/1310 [00:26<04:40,  4.27it/s]\u001b[A\n",
            "Embedding:   9%|▊         | 113/1310 [00:26<04:41,  4.25it/s]\u001b[A\n",
            "Embedding:   9%|▊         | 114/1310 [00:27<04:44,  4.21it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 115/1310 [00:27<04:43,  4.22it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 116/1310 [00:27<04:41,  4.24it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 117/1310 [00:27<04:41,  4.24it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 118/1310 [00:28<04:40,  4.25it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 119/1310 [00:28<04:40,  4.25it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 120/1310 [00:28<04:41,  4.23it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 121/1310 [00:28<04:40,  4.23it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 122/1310 [00:29<04:38,  4.26it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 123/1310 [00:29<04:38,  4.27it/s]\u001b[A\n",
            "Embedding:   9%|▉         | 124/1310 [00:29<04:39,  4.24it/s]\u001b[A\n",
            "Embedding:  10%|▉         | 125/1310 [00:29<04:41,  4.21it/s]\u001b[A\n",
            "Embedding:  10%|▉         | 126/1310 [00:29<04:40,  4.23it/s]\u001b[A\n",
            "Embedding:  10%|▉         | 127/1310 [00:30<04:38,  4.24it/s]\u001b[A\n",
            "Embedding:  10%|▉         | 128/1310 [00:30<04:43,  4.16it/s]\u001b[A\n",
            "Embedding:  10%|▉         | 129/1310 [00:30<04:43,  4.17it/s]\u001b[A\n",
            "Embedding:  10%|▉         | 130/1310 [00:30<04:40,  4.21it/s]\u001b[A\n",
            "Embedding:  10%|█         | 131/1310 [00:31<04:40,  4.20it/s]\u001b[A\n",
            "Embedding:  10%|█         | 132/1310 [00:31<04:39,  4.22it/s]\u001b[A\n",
            "Embedding:  10%|█         | 133/1310 [00:31<04:39,  4.21it/s]\u001b[A\n",
            "Embedding:  10%|█         | 134/1310 [00:31<04:39,  4.21it/s]\u001b[A\n",
            "Embedding:  10%|█         | 135/1310 [00:32<04:38,  4.22it/s]\u001b[A\n",
            "Embedding:  10%|█         | 136/1310 [00:32<04:36,  4.24it/s]\u001b[A\n",
            "Embedding:  10%|█         | 137/1310 [00:32<04:37,  4.23it/s]\u001b[A\n",
            "Embedding:  11%|█         | 138/1310 [00:32<04:38,  4.21it/s]\u001b[A\n",
            "Embedding:  11%|█         | 139/1310 [00:33<04:38,  4.20it/s]\u001b[A\n",
            "Embedding:  11%|█         | 140/1310 [00:33<04:38,  4.21it/s]\u001b[A\n",
            "Embedding:  11%|█         | 141/1310 [00:33<04:37,  4.21it/s]\u001b[A\n",
            "Embedding:  11%|█         | 142/1310 [00:33<04:41,  4.16it/s]\u001b[A\n",
            "Embedding:  11%|█         | 143/1310 [00:34<04:41,  4.14it/s]\u001b[A\n",
            "Embedding:  11%|█         | 144/1310 [00:34<04:49,  4.02it/s]\u001b[A\n",
            "Embedding:  11%|█         | 145/1310 [00:34<04:45,  4.08it/s]\u001b[A\n",
            "Embedding:  11%|█         | 146/1310 [00:34<04:42,  4.12it/s]\u001b[A\n",
            "Embedding:  11%|█         | 147/1310 [00:35<04:39,  4.16it/s]\u001b[A\n",
            "Embedding:  11%|█▏        | 148/1310 [00:35<04:39,  4.16it/s]\u001b[A\n",
            "Embedding:  11%|█▏        | 149/1310 [00:35<04:37,  4.19it/s]\u001b[A\n",
            "Embedding:  11%|█▏        | 150/1310 [00:35<04:35,  4.21it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 151/1310 [00:35<04:35,  4.20it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 152/1310 [00:36<04:34,  4.22it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 153/1310 [00:36<04:32,  4.24it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 154/1310 [00:36<04:33,  4.22it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 155/1310 [00:36<04:37,  4.17it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 156/1310 [00:37<04:35,  4.20it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 157/1310 [00:37<04:35,  4.19it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 158/1310 [00:37<04:36,  4.16it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 159/1310 [00:37<04:36,  4.16it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 160/1310 [00:38<04:30,  4.26it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 161/1310 [00:38<04:32,  4.22it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 162/1310 [00:38<04:32,  4.22it/s]\u001b[A\n",
            "Embedding:  12%|█▏        | 163/1310 [00:38<04:35,  4.16it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 164/1310 [00:39<04:35,  4.15it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 165/1310 [00:39<04:34,  4.18it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 166/1310 [00:39<04:35,  4.15it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 167/1310 [00:39<04:35,  4.16it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 168/1310 [00:40<04:34,  4.16it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 169/1310 [00:40<04:36,  4.13it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 170/1310 [00:40<04:37,  4.10it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 171/1310 [00:40<04:35,  4.13it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 172/1310 [00:40<04:33,  4.15it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 173/1310 [00:41<04:30,  4.20it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 174/1310 [00:41<04:32,  4.16it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 175/1310 [00:41<04:31,  4.18it/s]\u001b[A\n",
            "Embedding:  13%|█▎        | 176/1310 [00:41<04:31,  4.18it/s]\u001b[A\n",
            "Embedding:  14%|█▎        | 177/1310 [00:42<04:31,  4.17it/s]\u001b[A\n",
            "Embedding:  14%|█▎        | 178/1310 [00:42<04:30,  4.18it/s]\u001b[A\n",
            "Embedding:  14%|█▎        | 179/1310 [00:42<04:30,  4.18it/s]\u001b[A\n",
            "Embedding:  14%|█▎        | 180/1310 [00:42<04:34,  4.12it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 181/1310 [00:43<04:34,  4.12it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 182/1310 [00:43<04:30,  4.17it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 183/1310 [00:43<04:28,  4.20it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 184/1310 [00:43<04:29,  4.17it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 185/1310 [00:44<04:31,  4.14it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 186/1310 [00:44<04:31,  4.14it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 187/1310 [00:44<04:30,  4.15it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 188/1310 [00:44<04:31,  4.13it/s]\u001b[A\n",
            "Embedding:  14%|█▍        | 189/1310 [00:45<04:32,  4.11it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 190/1310 [00:45<04:30,  4.14it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 191/1310 [00:45<04:30,  4.13it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 192/1310 [00:45<04:32,  4.10it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 193/1310 [00:46<04:32,  4.09it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 194/1310 [00:46<04:34,  4.07it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 195/1310 [00:46<04:32,  4.09it/s]\u001b[A\n",
            "Embedding:  15%|█▍        | 196/1310 [00:46<04:30,  4.11it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 197/1310 [00:47<04:29,  4.12it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 198/1310 [00:47<04:28,  4.14it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 199/1310 [00:47<04:29,  4.13it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 200/1310 [00:47<04:26,  4.16it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 201/1310 [00:47<04:26,  4.16it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 202/1310 [00:48<04:26,  4.15it/s]\u001b[A\n",
            "Embedding:  15%|█▌        | 203/1310 [00:48<04:27,  4.14it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 204/1310 [00:48<04:28,  4.12it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 205/1310 [00:48<04:27,  4.13it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 206/1310 [00:49<04:29,  4.09it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 207/1310 [00:49<04:27,  4.13it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 208/1310 [00:49<04:25,  4.15it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 209/1310 [00:49<04:24,  4.17it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 210/1310 [00:50<04:27,  4.11it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 211/1310 [00:50<04:27,  4.11it/s]\u001b[A\n",
            "Embedding:  16%|█▌        | 212/1310 [00:50<04:26,  4.12it/s]\u001b[A\n",
            "Embedding:  16%|█▋        | 213/1310 [00:50<04:29,  4.07it/s]\u001b[A\n",
            "Embedding:  16%|█▋        | 214/1310 [00:51<04:29,  4.07it/s]\u001b[A\n",
            "Embedding:  16%|█▋        | 215/1310 [00:51<04:27,  4.09it/s]\u001b[A\n",
            "Embedding:  16%|█▋        | 216/1310 [00:51<04:26,  4.10it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 217/1310 [00:51<04:25,  4.11it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 218/1310 [00:52<04:29,  4.06it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 219/1310 [00:52<04:28,  4.07it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 220/1310 [00:52<04:24,  4.12it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 221/1310 [00:52<04:23,  4.13it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 222/1310 [00:53<04:26,  4.08it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 223/1310 [00:53<04:26,  4.08it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 224/1310 [00:53<04:26,  4.07it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 225/1310 [00:53<04:23,  4.11it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 226/1310 [00:54<04:23,  4.11it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 227/1310 [00:54<04:28,  4.03it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 228/1310 [00:54<04:27,  4.05it/s]\u001b[A\n",
            "Embedding:  17%|█▋        | 229/1310 [00:54<04:22,  4.12it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 230/1310 [00:55<04:24,  4.09it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 231/1310 [00:55<04:23,  4.09it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 232/1310 [00:55<04:21,  4.12it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 233/1310 [00:55<04:20,  4.14it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 234/1310 [00:56<04:23,  4.08it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 235/1310 [00:56<04:25,  4.05it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 236/1310 [00:56<04:22,  4.08it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 237/1310 [00:56<04:21,  4.10it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 238/1310 [00:57<04:22,  4.09it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 239/1310 [00:57<04:22,  4.08it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 240/1310 [00:57<04:22,  4.08it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 241/1310 [00:57<04:21,  4.08it/s]\u001b[A\n",
            "Embedding:  18%|█▊        | 242/1310 [00:58<04:21,  4.09it/s]\u001b[A\n",
            "Embedding:  19%|█▊        | 243/1310 [00:58<04:19,  4.12it/s]\u001b[A\n",
            "Embedding:  19%|█▊        | 244/1310 [00:58<04:18,  4.13it/s]\u001b[A\n",
            "Embedding:  19%|█▊        | 245/1310 [00:58<04:20,  4.10it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 246/1310 [00:58<04:21,  4.06it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 247/1310 [00:59<04:20,  4.09it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 248/1310 [00:59<04:20,  4.07it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 249/1310 [00:59<04:18,  4.10it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 250/1310 [00:59<04:20,  4.07it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 251/1310 [01:00<04:19,  4.08it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 252/1310 [01:00<04:18,  4.10it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 253/1310 [01:00<04:20,  4.07it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 254/1310 [01:00<04:18,  4.09it/s]\u001b[A\n",
            "Embedding:  19%|█▉        | 255/1310 [01:01<04:17,  4.09it/s]\u001b[A\n",
            "Embedding:  20%|█▉        | 256/1310 [01:01<04:17,  4.09it/s]\u001b[A\n",
            "Embedding:  20%|█▉        | 257/1310 [01:01<04:19,  4.07it/s]\u001b[A\n",
            "Embedding:  20%|█▉        | 258/1310 [01:01<04:18,  4.06it/s]\u001b[A\n",
            "Embedding:  20%|█▉        | 259/1310 [01:02<04:17,  4.08it/s]\u001b[A\n",
            "Embedding:  20%|█▉        | 260/1310 [01:02<04:16,  4.09it/s]\u001b[A\n",
            "Embedding:  20%|█▉        | 261/1310 [01:02<04:18,  4.06it/s]\u001b[A\n",
            "Embedding:  20%|██        | 262/1310 [01:02<04:20,  4.02it/s]\u001b[A\n",
            "Embedding:  20%|██        | 263/1310 [01:03<04:17,  4.07it/s]\u001b[A\n",
            "Embedding:  20%|██        | 264/1310 [01:03<04:16,  4.08it/s]\u001b[A\n",
            "Embedding:  20%|██        | 265/1310 [01:03<04:15,  4.09it/s]\u001b[A\n",
            "Embedding:  20%|██        | 266/1310 [01:03<04:16,  4.07it/s]\u001b[A\n",
            "Embedding:  20%|██        | 267/1310 [01:04<04:17,  4.05it/s]\u001b[A\n",
            "Embedding:  20%|██        | 268/1310 [01:04<04:15,  4.07it/s]\u001b[A\n",
            "Embedding:  21%|██        | 269/1310 [01:04<04:14,  4.09it/s]\u001b[A\n",
            "Embedding:  21%|██        | 270/1310 [01:04<04:17,  4.03it/s]\u001b[A\n",
            "Embedding:  21%|██        | 271/1310 [01:05<04:15,  4.06it/s]\u001b[A\n",
            "Embedding:  21%|██        | 272/1310 [01:05<04:18,  4.02it/s]\u001b[A\n",
            "Embedding:  21%|██        | 273/1310 [01:05<04:17,  4.03it/s]\u001b[A\n",
            "Embedding:  21%|██        | 274/1310 [01:05<04:15,  4.05it/s]\u001b[A\n",
            "Embedding:  21%|██        | 275/1310 [01:06<04:16,  4.03it/s]\u001b[A\n",
            "Embedding:  21%|██        | 276/1310 [01:06<04:15,  4.05it/s]\u001b[A\n",
            "Embedding:  21%|██        | 277/1310 [01:06<04:15,  4.04it/s]\u001b[A\n",
            "Embedding:  21%|██        | 278/1310 [01:06<04:16,  4.02it/s]\u001b[A\n",
            "Embedding:  21%|██▏       | 279/1310 [01:07<04:15,  4.03it/s]\u001b[A\n",
            "Embedding:  21%|██▏       | 280/1310 [01:07<04:14,  4.04it/s]\u001b[A\n",
            "Embedding:  21%|██▏       | 281/1310 [01:07<04:17,  4.00it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 282/1310 [01:07<04:15,  4.02it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 283/1310 [01:08<04:13,  4.05it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 284/1310 [01:08<04:12,  4.07it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 285/1310 [01:08<04:16,  4.00it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 286/1310 [01:08<04:14,  4.02it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 287/1310 [01:09<04:14,  4.02it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 288/1310 [01:09<04:14,  4.02it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 289/1310 [01:09<04:14,  4.02it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 290/1310 [01:09<04:12,  4.03it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 291/1310 [01:10<04:14,  4.00it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 292/1310 [01:10<04:13,  4.01it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 293/1310 [01:10<04:11,  4.04it/s]\u001b[A\n",
            "Embedding:  22%|██▏       | 294/1310 [01:10<04:11,  4.03it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 295/1310 [01:11<04:11,  4.03it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 296/1310 [01:11<04:12,  4.02it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 297/1310 [01:11<04:11,  4.03it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 298/1310 [01:11<04:09,  4.05it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 299/1310 [01:12<04:09,  4.06it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 300/1310 [01:12<04:10,  4.03it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 301/1310 [01:12<04:11,  4.01it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 302/1310 [01:12<04:09,  4.03it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 303/1310 [01:13<04:12,  3.99it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 304/1310 [01:13<04:12,  3.99it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 305/1310 [01:13<04:10,  4.01it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 306/1310 [01:13<04:11,  4.00it/s]\u001b[A\n",
            "Embedding:  23%|██▎       | 307/1310 [01:14<04:10,  4.00it/s]\u001b[A\n",
            "Embedding:  24%|██▎       | 308/1310 [01:14<04:09,  4.01it/s]\u001b[A\n",
            "Embedding:  24%|██▎       | 309/1310 [01:14<04:10,  3.99it/s]\u001b[A\n",
            "Embedding:  24%|██▎       | 310/1310 [01:14<04:09,  4.01it/s]\u001b[A\n",
            "Embedding:  24%|██▎       | 311/1310 [01:15<04:08,  4.02it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 312/1310 [01:15<04:07,  4.03it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 313/1310 [01:15<04:07,  4.03it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 314/1310 [01:15<04:08,  4.00it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 315/1310 [01:16<04:07,  4.01it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 316/1310 [01:16<04:07,  4.01it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 317/1310 [01:16<04:11,  3.95it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 318/1310 [01:16<04:11,  3.95it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 319/1310 [01:17<04:07,  4.00it/s]\u001b[A\n",
            "Embedding:  24%|██▍       | 320/1310 [01:17<04:06,  4.01it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 321/1310 [01:17<04:06,  4.02it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 322/1310 [01:17<04:04,  4.04it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 323/1310 [01:18<04:07,  3.98it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 324/1310 [01:18<04:06,  4.00it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 325/1310 [01:18<04:02,  4.05it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 326/1310 [01:18<04:04,  4.02it/s]\u001b[A\n",
            "Embedding:  25%|██▍       | 327/1310 [01:19<04:06,  3.98it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 328/1310 [01:19<04:04,  4.01it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 329/1310 [01:19<04:02,  4.05it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 330/1310 [01:19<04:05,  4.00it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 331/1310 [01:20<04:04,  4.00it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 332/1310 [01:20<04:02,  4.04it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 333/1310 [01:20<04:02,  4.04it/s]\u001b[A\n",
            "Embedding:  25%|██▌       | 334/1310 [01:20<04:01,  4.05it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 335/1310 [01:21<04:00,  4.05it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 336/1310 [01:21<04:00,  4.04it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 337/1310 [01:21<04:03,  3.99it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 338/1310 [01:21<04:04,  3.98it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 339/1310 [01:22<04:03,  3.98it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 340/1310 [01:22<04:02,  4.00it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 341/1310 [01:22<04:05,  3.95it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 342/1310 [01:22<04:07,  3.91it/s]\u001b[A\n",
            "Embedding:  26%|██▌       | 343/1310 [01:23<04:10,  3.86it/s]\u001b[A\n",
            "Embedding:  26%|██▋       | 344/1310 [01:23<04:08,  3.89it/s]\u001b[A\n",
            "Embedding:  26%|██▋       | 345/1310 [01:23<04:05,  3.93it/s]\u001b[A\n",
            "Embedding:  26%|██▋       | 346/1310 [01:23<04:04,  3.94it/s]\u001b[A\n",
            "Embedding:  26%|██▋       | 347/1310 [01:24<04:03,  3.95it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 348/1310 [01:24<04:02,  3.97it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 349/1310 [01:24<04:00,  3.99it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 350/1310 [01:24<04:03,  3.95it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 351/1310 [01:25<04:00,  3.98it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 352/1310 [01:25<03:59,  4.00it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 353/1310 [01:25<04:03,  3.93it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 354/1310 [01:25<04:05,  3.90it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 355/1310 [01:26<04:04,  3.91it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 356/1310 [01:26<04:01,  3.95it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 357/1310 [01:26<04:02,  3.93it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 358/1310 [01:26<04:02,  3.93it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 359/1310 [01:27<03:59,  3.96it/s]\u001b[A\n",
            "Embedding:  27%|██▋       | 360/1310 [01:27<03:57,  3.99it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 361/1310 [01:27<04:02,  3.91it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 362/1310 [01:27<03:59,  3.95it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 363/1310 [01:28<03:56,  4.00it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 364/1310 [01:28<03:56,  4.00it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 365/1310 [01:28<03:59,  3.94it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 366/1310 [01:28<03:58,  3.97it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 367/1310 [01:29<03:57,  3.97it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 368/1310 [01:29<03:56,  3.98it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 369/1310 [01:29<03:54,  4.00it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 370/1310 [01:29<03:54,  4.00it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 371/1310 [01:30<03:54,  4.00it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 372/1310 [01:30<03:53,  4.01it/s]\u001b[A\n",
            "Embedding:  28%|██▊       | 373/1310 [01:30<03:54,  4.00it/s]\u001b[A\n",
            "Embedding:  29%|██▊       | 374/1310 [01:30<03:53,  4.00it/s]\u001b[A\n",
            "Embedding:  29%|██▊       | 375/1310 [01:31<03:55,  3.96it/s]\u001b[A\n",
            "Embedding:  29%|██▊       | 376/1310 [01:31<03:55,  3.97it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 377/1310 [01:31<03:54,  3.97it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 378/1310 [01:31<03:53,  4.00it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 379/1310 [01:32<03:52,  4.00it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 380/1310 [01:32<03:53,  3.99it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 381/1310 [01:32<03:52,  3.99it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 382/1310 [01:32<03:51,  4.00it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 383/1310 [01:33<03:52,  3.98it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 384/1310 [01:33<03:54,  3.95it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 385/1310 [01:33<03:51,  3.99it/s]\u001b[A\n",
            "Embedding:  29%|██▉       | 386/1310 [01:33<03:52,  3.97it/s]\u001b[A\n",
            "Embedding:  30%|██▉       | 387/1310 [01:34<03:55,  3.92it/s]\u001b[A\n",
            "Embedding:  30%|██▉       | 388/1310 [01:34<03:55,  3.92it/s]\u001b[A\n",
            "Embedding:  30%|██▉       | 389/1310 [01:34<03:55,  3.91it/s]\u001b[A\n",
            "Embedding:  30%|██▉       | 390/1310 [01:34<03:52,  3.95it/s]\u001b[A\n",
            "Embedding:  30%|██▉       | 391/1310 [01:35<03:53,  3.94it/s]\u001b[A\n",
            "Embedding:  30%|██▉       | 392/1310 [01:35<03:54,  3.91it/s]\u001b[A\n",
            "Embedding:  30%|███       | 393/1310 [01:35<03:55,  3.89it/s]\u001b[A\n",
            "Embedding:  30%|███       | 394/1310 [01:35<03:52,  3.93it/s]\u001b[A\n",
            "Embedding:  30%|███       | 395/1310 [01:36<03:52,  3.94it/s]\u001b[A\n",
            "Embedding:  30%|███       | 396/1310 [01:36<03:54,  3.89it/s]\u001b[A\n",
            "Embedding:  30%|███       | 397/1310 [01:36<03:49,  3.98it/s]\u001b[A\n",
            "Embedding:  30%|███       | 398/1310 [01:36<03:47,  4.00it/s]\u001b[A\n",
            "Embedding:  30%|███       | 399/1310 [01:37<03:48,  3.98it/s]\u001b[A\n",
            "Embedding:  31%|███       | 400/1310 [01:37<03:47,  4.00it/s]\u001b[A\n",
            "Embedding:  31%|███       | 401/1310 [01:37<03:47,  4.00it/s]\u001b[A\n",
            "Embedding:  31%|███       | 402/1310 [01:37<03:45,  4.02it/s]\u001b[A\n",
            "Embedding:  31%|███       | 403/1310 [01:38<03:46,  4.01it/s]\u001b[A\n",
            "Embedding:  31%|███       | 404/1310 [01:38<03:45,  4.01it/s]\u001b[A\n",
            "Embedding:  31%|███       | 405/1310 [01:38<03:45,  4.01it/s]\u001b[A\n",
            "Embedding:  31%|███       | 406/1310 [01:38<03:45,  4.01it/s]\u001b[A\n",
            "Embedding:  31%|███       | 407/1310 [01:39<03:44,  4.01it/s]\u001b[A\n",
            "Embedding:  31%|███       | 408/1310 [01:39<03:43,  4.03it/s]\u001b[A\n",
            "Embedding:  31%|███       | 409/1310 [01:39<03:42,  4.04it/s]\u001b[A\n",
            "Embedding:  31%|███▏      | 410/1310 [01:39<03:43,  4.02it/s]\u001b[A\n",
            "Embedding:  31%|███▏      | 411/1310 [01:40<03:44,  4.00it/s]\u001b[A\n",
            "Embedding:  31%|███▏      | 412/1310 [01:40<03:48,  3.92it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 413/1310 [01:40<03:44,  3.99it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 414/1310 [01:40<03:47,  3.95it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 415/1310 [01:41<03:48,  3.92it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 416/1310 [01:41<03:46,  3.95it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 417/1310 [01:41<03:45,  3.97it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 418/1310 [01:41<03:45,  3.96it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 419/1310 [01:42<03:47,  3.92it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 420/1310 [01:42<03:42,  4.00it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 421/1310 [01:42<03:45,  3.94it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 422/1310 [01:43<03:49,  3.87it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 423/1310 [01:43<03:46,  3.91it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 424/1310 [01:43<03:43,  3.96it/s]\u001b[A\n",
            "Embedding:  32%|███▏      | 425/1310 [01:43<03:45,  3.93it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 426/1310 [01:44<03:47,  3.88it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 427/1310 [01:44<03:42,  3.96it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 428/1310 [01:44<03:44,  3.93it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 429/1310 [01:44<03:45,  3.92it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 430/1310 [01:45<03:45,  3.90it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 431/1310 [01:45<03:43,  3.92it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 432/1310 [01:45<03:42,  3.94it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 433/1310 [01:45<03:43,  3.92it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 434/1310 [01:46<03:40,  3.96it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 435/1310 [01:46<03:40,  3.97it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 436/1310 [01:46<03:43,  3.91it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 437/1310 [01:46<03:39,  3.98it/s]\u001b[A\n",
            "Embedding:  33%|███▎      | 438/1310 [01:47<03:36,  4.02it/s]\u001b[A\n",
            "Embedding:  34%|███▎      | 439/1310 [01:47<03:37,  4.01it/s]\u001b[A\n",
            "Embedding:  34%|███▎      | 440/1310 [01:47<03:36,  4.01it/s]\u001b[A\n",
            "Embedding:  34%|███▎      | 441/1310 [01:47<03:36,  4.01it/s]\u001b[A\n",
            "Embedding:  34%|███▎      | 442/1310 [01:48<03:34,  4.05it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 443/1310 [01:48<03:34,  4.04it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 444/1310 [01:48<03:36,  4.00it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 445/1310 [01:48<03:36,  4.00it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 446/1310 [01:49<03:33,  4.05it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 447/1310 [01:49<03:35,  4.00it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 448/1310 [01:49<03:34,  4.02it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 449/1310 [01:49<03:33,  4.04it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 450/1310 [01:50<03:32,  4.04it/s]\u001b[A\n",
            "Embedding:  34%|███▍      | 451/1310 [01:50<03:34,  4.00it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 452/1310 [01:50<03:35,  3.98it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 453/1310 [01:50<03:34,  4.00it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 454/1310 [01:51<03:35,  3.97it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 455/1310 [01:51<03:34,  3.98it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 456/1310 [01:51<03:33,  4.00it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 457/1310 [01:51<03:32,  4.02it/s]\u001b[A\n",
            "Embedding:  35%|███▍      | 458/1310 [01:52<03:31,  4.03it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 459/1310 [01:52<03:32,  4.00it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 460/1310 [01:52<03:34,  3.95it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 461/1310 [01:52<03:34,  3.97it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 462/1310 [01:53<03:31,  4.01it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 463/1310 [01:53<03:33,  3.97it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 464/1310 [01:53<03:31,  4.00it/s]\u001b[A\n",
            "Embedding:  35%|███▌      | 465/1310 [01:53<03:29,  4.04it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 466/1310 [01:54<03:28,  4.04it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 467/1310 [01:54<03:32,  3.96it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 468/1310 [01:54<03:30,  4.01it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 469/1310 [01:54<03:29,  4.01it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 470/1310 [01:55<03:29,  4.02it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 471/1310 [01:55<03:28,  4.03it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 472/1310 [01:55<03:26,  4.05it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 473/1310 [01:55<03:29,  4.00it/s]\u001b[A\n",
            "Embedding:  36%|███▌      | 474/1310 [01:56<03:28,  4.01it/s]\u001b[A\n",
            "Embedding:  36%|███▋      | 475/1310 [01:56<03:27,  4.03it/s]\u001b[A\n",
            "Embedding:  36%|███▋      | 476/1310 [01:56<03:27,  4.02it/s]\u001b[A\n",
            "Embedding:  36%|███▋      | 477/1310 [01:56<03:28,  4.00it/s]\u001b[A\n",
            "Embedding:  36%|███▋      | 478/1310 [01:57<03:28,  4.00it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 479/1310 [01:57<03:27,  4.01it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 480/1310 [01:57<03:25,  4.05it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 481/1310 [01:57<03:27,  4.00it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 482/1310 [01:58<03:27,  3.99it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 483/1310 [01:58<03:28,  3.97it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 484/1310 [01:58<03:25,  4.02it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 485/1310 [01:58<03:26,  4.00it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 486/1310 [01:59<03:25,  4.02it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 487/1310 [01:59<03:26,  3.99it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 488/1310 [01:59<03:23,  4.04it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 489/1310 [01:59<03:21,  4.06it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 490/1310 [02:00<03:21,  4.06it/s]\u001b[A\n",
            "Embedding:  37%|███▋      | 491/1310 [02:00<03:21,  4.07it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 492/1310 [02:00<03:21,  4.05it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 493/1310 [02:00<03:20,  4.08it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 494/1310 [02:00<03:20,  4.07it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 495/1310 [02:01<03:21,  4.04it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 496/1310 [02:01<03:23,  4.01it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 497/1310 [02:01<03:21,  4.03it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 498/1310 [02:01<03:20,  4.04it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 499/1310 [02:02<03:19,  4.07it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 500/1310 [02:02<03:19,  4.07it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 501/1310 [02:02<03:19,  4.05it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 502/1310 [02:02<03:17,  4.08it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 503/1310 [02:03<03:17,  4.10it/s]\u001b[A\n",
            "Embedding:  38%|███▊      | 504/1310 [02:03<03:16,  4.09it/s]\u001b[A\n",
            "Embedding:  39%|███▊      | 505/1310 [02:03<03:18,  4.05it/s]\u001b[A\n",
            "Embedding:  39%|███▊      | 506/1310 [02:03<03:20,  4.01it/s]\u001b[A\n",
            "Embedding:  39%|███▊      | 507/1310 [02:04<03:16,  4.09it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 508/1310 [02:04<03:16,  4.08it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 509/1310 [02:04<03:17,  4.06it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 510/1310 [02:04<03:15,  4.09it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 511/1310 [02:05<03:15,  4.09it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 512/1310 [02:05<03:14,  4.10it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 513/1310 [02:05<03:13,  4.12it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 514/1310 [02:05<03:14,  4.09it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 515/1310 [02:06<03:16,  4.05it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 516/1310 [02:06<03:14,  4.08it/s]\u001b[A\n",
            "Embedding:  39%|███▉      | 517/1310 [02:06<03:14,  4.08it/s]\u001b[A\n",
            "Embedding:  40%|███▉      | 518/1310 [02:06<03:15,  4.05it/s]\u001b[A\n",
            "Embedding:  40%|███▉      | 519/1310 [02:07<03:14,  4.06it/s]\u001b[A\n",
            "Embedding:  40%|███▉      | 520/1310 [02:07<03:13,  4.08it/s]\u001b[A\n",
            "Embedding:  40%|███▉      | 521/1310 [02:07<03:13,  4.09it/s]\u001b[A\n",
            "Embedding:  40%|███▉      | 522/1310 [02:07<03:12,  4.09it/s]\u001b[A\n",
            "Embedding:  40%|███▉      | 523/1310 [02:08<03:12,  4.09it/s]\u001b[A\n",
            "Embedding:  40%|████      | 524/1310 [02:08<03:09,  4.15it/s]\u001b[A\n",
            "Embedding:  40%|████      | 525/1310 [02:08<03:11,  4.10it/s]\u001b[A\n",
            "Embedding:  40%|████      | 526/1310 [02:08<03:14,  4.04it/s]\u001b[A\n",
            "Embedding:  40%|████      | 527/1310 [02:09<03:12,  4.07it/s]\u001b[A\n",
            "Embedding:  40%|████      | 528/1310 [02:09<03:10,  4.10it/s]\u001b[A\n",
            "Embedding:  40%|████      | 529/1310 [02:09<03:10,  4.10it/s]\u001b[A\n",
            "Embedding:  40%|████      | 530/1310 [02:09<03:10,  4.10it/s]\u001b[A\n",
            "Embedding:  41%|████      | 531/1310 [02:10<03:10,  4.08it/s]\u001b[A\n",
            "Embedding:  41%|████      | 532/1310 [02:10<03:11,  4.07it/s]\u001b[A\n",
            "Embedding:  41%|████      | 533/1310 [02:10<03:09,  4.10it/s]\u001b[A\n",
            "Embedding:  41%|████      | 534/1310 [02:10<03:09,  4.10it/s]\u001b[A\n",
            "Embedding:  41%|████      | 535/1310 [02:11<03:08,  4.11it/s]\u001b[A\n",
            "Embedding:  41%|████      | 536/1310 [02:11<03:09,  4.07it/s]\u001b[A\n",
            "Embedding:  41%|████      | 537/1310 [02:11<03:09,  4.07it/s]\u001b[A\n",
            "Embedding:  41%|████      | 538/1310 [02:11<03:08,  4.09it/s]\u001b[A\n",
            "Embedding:  41%|████      | 539/1310 [02:12<03:10,  4.06it/s]\u001b[A\n",
            "Embedding:  41%|████      | 540/1310 [02:12<03:09,  4.06it/s]\u001b[A\n",
            "Embedding:  41%|████▏     | 541/1310 [02:12<03:08,  4.08it/s]\u001b[A\n",
            "Embedding:  41%|████▏     | 542/1310 [02:12<03:07,  4.09it/s]\u001b[A\n",
            "Embedding:  41%|████▏     | 543/1310 [02:13<03:06,  4.11it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 544/1310 [02:13<03:08,  4.06it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 545/1310 [02:13<03:09,  4.04it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 546/1310 [02:13<03:09,  4.03it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 547/1310 [02:14<03:08,  4.05it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 548/1310 [02:14<03:06,  4.08it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 549/1310 [02:14<03:09,  4.02it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 550/1310 [02:14<03:07,  4.04it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 551/1310 [02:14<03:06,  4.07it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 552/1310 [02:15<03:05,  4.09it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 553/1310 [02:15<03:05,  4.07it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 554/1310 [02:15<03:05,  4.08it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 555/1310 [02:15<03:06,  4.05it/s]\u001b[A\n",
            "Embedding:  42%|████▏     | 556/1310 [02:16<03:05,  4.06it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 557/1310 [02:16<03:04,  4.07it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 558/1310 [02:16<03:05,  4.06it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 559/1310 [02:16<03:05,  4.05it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 560/1310 [02:17<03:03,  4.08it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 561/1310 [02:17<03:04,  4.06it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 562/1310 [02:17<03:05,  4.04it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 563/1310 [02:17<03:03,  4.06it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 564/1310 [02:18<03:03,  4.08it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 565/1310 [02:18<03:01,  4.10it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 566/1310 [02:18<03:00,  4.12it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 567/1310 [02:18<03:02,  4.08it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 568/1310 [02:19<03:00,  4.10it/s]\u001b[A\n",
            "Embedding:  43%|████▎     | 569/1310 [02:19<03:00,  4.11it/s]\u001b[A\n",
            "Embedding:  44%|████▎     | 570/1310 [02:19<03:01,  4.08it/s]\u001b[A\n",
            "Embedding:  44%|████▎     | 571/1310 [02:19<03:01,  4.07it/s]\u001b[A\n",
            "Embedding:  44%|████▎     | 572/1310 [02:20<03:01,  4.06it/s]\u001b[A\n",
            "Embedding:  44%|████▎     | 573/1310 [02:20<03:00,  4.09it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 574/1310 [02:20<03:01,  4.05it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 575/1310 [02:20<03:00,  4.08it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 576/1310 [02:21<02:59,  4.09it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 577/1310 [02:21<03:01,  4.04it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 578/1310 [02:21<03:03,  3.99it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 579/1310 [02:21<03:01,  4.02it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 580/1310 [02:22<03:00,  4.04it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 581/1310 [02:22<03:01,  4.01it/s]\u001b[A\n",
            "Embedding:  44%|████▍     | 582/1310 [02:22<03:01,  4.02it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 583/1310 [02:22<03:00,  4.02it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 584/1310 [02:23<02:59,  4.05it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 585/1310 [02:23<02:57,  4.08it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 586/1310 [02:23<02:58,  4.06it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 587/1310 [02:23<02:59,  4.04it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 588/1310 [02:24<02:57,  4.07it/s]\u001b[A\n",
            "Embedding:  45%|████▍     | 589/1310 [02:24<02:57,  4.07it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 590/1310 [02:24<02:57,  4.06it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 591/1310 [02:24<02:56,  4.06it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 592/1310 [02:25<02:56,  4.07it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 593/1310 [02:25<02:55,  4.08it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 594/1310 [02:25<02:56,  4.06it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 595/1310 [02:25<02:56,  4.06it/s]\u001b[A\n",
            "Embedding:  45%|████▌     | 596/1310 [02:26<02:55,  4.08it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 597/1310 [02:26<02:54,  4.08it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 598/1310 [02:26<02:55,  4.05it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 599/1310 [02:26<02:57,  4.00it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 600/1310 [02:27<02:57,  4.00it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 601/1310 [02:27<02:55,  4.04it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 602/1310 [02:27<02:56,  4.02it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 603/1310 [02:27<02:54,  4.05it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 604/1310 [02:28<02:54,  4.04it/s]\u001b[A\n",
            "Embedding:  46%|████▌     | 605/1310 [02:28<02:57,  3.98it/s]\u001b[A\n",
            "Embedding:  46%|████▋     | 606/1310 [02:28<02:57,  3.98it/s]\u001b[A\n",
            "Embedding:  46%|████▋     | 607/1310 [02:28<02:55,  4.01it/s]\u001b[A\n",
            "Embedding:  46%|████▋     | 608/1310 [02:29<02:55,  4.00it/s]\u001b[A\n",
            "Embedding:  46%|████▋     | 609/1310 [02:29<02:53,  4.05it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 610/1310 [02:29<02:55,  3.99it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 611/1310 [02:29<02:56,  3.97it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 612/1310 [02:30<02:52,  4.04it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 613/1310 [02:30<02:53,  4.01it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 614/1310 [02:30<02:53,  4.00it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 615/1310 [02:30<02:52,  4.04it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 616/1310 [02:31<02:54,  3.98it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 617/1310 [02:31<02:53,  4.00it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 618/1310 [02:31<02:52,  4.01it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 619/1310 [02:31<02:51,  4.03it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 620/1310 [02:32<02:52,  4.00it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 621/1310 [02:32<02:50,  4.03it/s]\u001b[A\n",
            "Embedding:  47%|████▋     | 622/1310 [02:32<02:49,  4.07it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 623/1310 [02:32<02:48,  4.08it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 624/1310 [02:33<02:49,  4.05it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 625/1310 [02:33<02:48,  4.06it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 626/1310 [02:33<02:49,  4.03it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 627/1310 [02:33<02:49,  4.03it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 628/1310 [02:34<02:50,  4.01it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 629/1310 [02:34<02:48,  4.03it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 630/1310 [02:34<02:48,  4.03it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 631/1310 [02:34<02:48,  4.02it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 632/1310 [02:35<02:48,  4.02it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 633/1310 [02:35<02:46,  4.06it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 634/1310 [02:35<02:47,  4.04it/s]\u001b[A\n",
            "Embedding:  48%|████▊     | 635/1310 [02:35<02:47,  4.03it/s]\u001b[A\n",
            "Embedding:  49%|████▊     | 636/1310 [02:35<02:45,  4.07it/s]\u001b[A\n",
            "Embedding:  49%|████▊     | 637/1310 [02:36<02:45,  4.06it/s]\u001b[A\n",
            "Embedding:  49%|████▊     | 638/1310 [02:36<02:47,  4.02it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 639/1310 [02:36<02:46,  4.03it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 640/1310 [02:36<02:45,  4.05it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 641/1310 [02:37<02:46,  4.02it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 642/1310 [02:37<02:46,  4.01it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 643/1310 [02:37<02:45,  4.02it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 644/1310 [02:37<02:46,  4.01it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 645/1310 [02:38<02:46,  3.98it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 646/1310 [02:38<02:45,  4.01it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 647/1310 [02:38<02:46,  3.99it/s]\u001b[A\n",
            "Embedding:  49%|████▉     | 648/1310 [02:39<02:47,  3.95it/s]\u001b[A\n",
            "Embedding:  50%|████▉     | 649/1310 [02:39<02:46,  3.97it/s]\u001b[A\n",
            "Embedding:  50%|████▉     | 650/1310 [02:39<02:48,  3.92it/s]\u001b[A\n",
            "Embedding:  50%|████▉     | 651/1310 [02:39<02:48,  3.91it/s]\u001b[A\n",
            "Embedding:  50%|████▉     | 652/1310 [02:40<02:48,  3.91it/s]\u001b[A\n",
            "Embedding:  50%|████▉     | 653/1310 [02:40<02:45,  3.96it/s]\u001b[A\n",
            "Embedding:  50%|████▉     | 654/1310 [02:40<02:45,  3.96it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 655/1310 [02:40<02:46,  3.92it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 656/1310 [02:41<02:46,  3.94it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 657/1310 [02:41<02:46,  3.93it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 658/1310 [02:41<02:44,  3.96it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 659/1310 [02:41<02:44,  3.95it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 660/1310 [02:42<02:43,  3.97it/s]\u001b[A\n",
            "Embedding:  50%|█████     | 661/1310 [02:42<02:45,  3.93it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 662/1310 [02:42<02:43,  3.96it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 663/1310 [02:42<02:42,  3.98it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 664/1310 [02:43<02:41,  3.99it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 665/1310 [02:43<02:42,  3.98it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 666/1310 [02:43<02:41,  3.99it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 667/1310 [02:43<02:41,  3.99it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 668/1310 [02:44<02:40,  4.00it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 669/1310 [02:44<02:40,  4.00it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 670/1310 [02:44<02:41,  3.95it/s]\u001b[A\n",
            "Embedding:  51%|█████     | 671/1310 [02:44<02:40,  3.97it/s]\u001b[A\n",
            "Embedding:  51%|█████▏    | 672/1310 [02:45<02:40,  3.98it/s]\u001b[A\n",
            "Embedding:  51%|█████▏    | 673/1310 [02:45<02:41,  3.95it/s]\u001b[A\n",
            "Embedding:  51%|█████▏    | 674/1310 [02:45<02:40,  3.97it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 675/1310 [02:45<02:40,  3.96it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 676/1310 [02:46<02:41,  3.92it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 677/1310 [02:46<02:39,  3.97it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 678/1310 [02:46<02:39,  3.96it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 679/1310 [02:46<02:40,  3.93it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 680/1310 [02:47<02:40,  3.93it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 681/1310 [02:47<02:44,  3.81it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 682/1310 [02:47<02:41,  3.89it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 683/1310 [02:47<02:39,  3.94it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 684/1310 [02:48<02:38,  3.95it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 685/1310 [02:48<02:36,  3.99it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 686/1310 [02:48<02:36,  4.00it/s]\u001b[A\n",
            "Embedding:  52%|█████▏    | 687/1310 [02:48<02:35,  4.01it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 688/1310 [02:49<02:34,  4.02it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 689/1310 [02:49<02:35,  4.00it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 690/1310 [02:49<02:36,  3.97it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 691/1310 [02:49<02:35,  3.98it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 692/1310 [02:50<02:33,  4.02it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 693/1310 [02:50<02:33,  4.02it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 694/1310 [02:50<02:34,  3.99it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 695/1310 [02:50<02:34,  3.99it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 696/1310 [02:51<02:34,  3.98it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 697/1310 [02:51<02:35,  3.94it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 698/1310 [02:51<02:33,  3.98it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 699/1310 [02:51<02:34,  3.96it/s]\u001b[A\n",
            "Embedding:  53%|█████▎    | 700/1310 [02:52<02:33,  3.97it/s]\u001b[A\n",
            "Embedding:  54%|█████▎    | 701/1310 [02:52<02:37,  3.87it/s]\u001b[A\n",
            "Embedding:  54%|█████▎    | 702/1310 [02:52<02:33,  3.95it/s]\u001b[A\n",
            "Embedding:  54%|█████▎    | 703/1310 [02:52<02:33,  3.95it/s]\u001b[A\n",
            "Embedding:  54%|█████▎    | 704/1310 [02:53<02:33,  3.94it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 705/1310 [02:53<02:32,  3.96it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 706/1310 [02:53<02:32,  3.96it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 707/1310 [02:53<02:31,  3.99it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 708/1310 [02:54<02:30,  4.01it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 709/1310 [02:54<02:29,  4.01it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 710/1310 [02:54<02:29,  4.01it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 711/1310 [02:54<02:33,  3.90it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 712/1310 [02:55<02:33,  3.89it/s]\u001b[A\n",
            "Embedding:  54%|█████▍    | 713/1310 [02:55<02:30,  3.97it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 714/1310 [02:55<02:30,  3.95it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 715/1310 [02:55<02:30,  3.95it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 716/1310 [02:56<02:30,  3.94it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 717/1310 [02:56<02:30,  3.93it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 718/1310 [02:56<02:30,  3.94it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 719/1310 [02:56<02:29,  3.95it/s]\u001b[A\n",
            "Embedding:  55%|█████▍    | 720/1310 [02:57<02:29,  3.94it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 721/1310 [02:57<02:29,  3.94it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 722/1310 [02:57<02:31,  3.87it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 723/1310 [02:57<02:31,  3.87it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 724/1310 [02:58<02:29,  3.93it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 725/1310 [02:58<02:29,  3.91it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 726/1310 [02:58<02:29,  3.91it/s]\u001b[A\n",
            "Embedding:  55%|█████▌    | 727/1310 [02:58<02:27,  3.96it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 728/1310 [02:59<02:27,  3.94it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 729/1310 [02:59<02:28,  3.91it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 730/1310 [02:59<02:28,  3.90it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 731/1310 [03:00<02:26,  3.95it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 732/1310 [03:00<02:24,  3.99it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 733/1310 [03:00<02:25,  3.96it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 734/1310 [03:00<02:25,  3.95it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 735/1310 [03:01<02:26,  3.93it/s]\u001b[A\n",
            "Embedding:  56%|█████▌    | 736/1310 [03:01<02:25,  3.95it/s]\u001b[A\n",
            "Embedding:  56%|█████▋    | 737/1310 [03:01<02:24,  3.97it/s]\u001b[A\n",
            "Embedding:  56%|█████▋    | 738/1310 [03:01<02:22,  4.00it/s]\u001b[A\n",
            "Embedding:  56%|█████▋    | 739/1310 [03:02<02:24,  3.96it/s]\u001b[A\n",
            "Embedding:  56%|█████▋    | 740/1310 [03:02<02:23,  3.98it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 741/1310 [03:02<02:22,  3.99it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 742/1310 [03:02<02:22,  3.98it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 743/1310 [03:03<02:22,  3.99it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 744/1310 [03:03<02:21,  4.00it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 745/1310 [03:03<02:21,  4.00it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 746/1310 [03:03<02:21,  4.00it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 747/1310 [03:04<02:20,  4.00it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 748/1310 [03:04<02:21,  3.97it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 749/1310 [03:04<02:21,  3.97it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 750/1310 [03:04<02:20,  4.00it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 751/1310 [03:05<02:20,  3.99it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 752/1310 [03:05<02:20,  3.98it/s]\u001b[A\n",
            "Embedding:  57%|█████▋    | 753/1310 [03:05<02:19,  4.00it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 754/1310 [03:05<02:19,  4.00it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 755/1310 [03:06<02:19,  3.99it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 756/1310 [03:06<02:18,  4.00it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 757/1310 [03:06<02:17,  4.02it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 758/1310 [03:06<02:18,  3.99it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 759/1310 [03:07<02:18,  3.97it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 760/1310 [03:07<02:17,  3.99it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 761/1310 [03:07<02:17,  4.00it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 762/1310 [03:07<02:17,  3.99it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 763/1310 [03:08<02:18,  3.96it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 764/1310 [03:08<02:16,  3.99it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 765/1310 [03:08<02:17,  3.98it/s]\u001b[A\n",
            "Embedding:  58%|█████▊    | 766/1310 [03:08<02:17,  3.94it/s]\u001b[A\n",
            "Embedding:  59%|█████▊    | 767/1310 [03:09<02:18,  3.92it/s]\u001b[A\n",
            "Embedding:  59%|█████▊    | 768/1310 [03:09<02:16,  3.97it/s]\u001b[A\n",
            "Embedding:  59%|█████▊    | 769/1310 [03:09<02:14,  4.01it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 770/1310 [03:09<02:14,  4.01it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 771/1310 [03:10<02:15,  3.98it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 772/1310 [03:10<02:14,  4.00it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 773/1310 [03:10<02:13,  4.03it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 774/1310 [03:10<02:14,  3.98it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 775/1310 [03:11<02:14,  3.98it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 776/1310 [03:11<02:13,  4.00it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 777/1310 [03:11<02:14,  3.96it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 778/1310 [03:11<02:13,  3.99it/s]\u001b[A\n",
            "Embedding:  59%|█████▉    | 779/1310 [03:12<02:12,  4.01it/s]\u001b[A\n",
            "Embedding:  60%|█████▉    | 780/1310 [03:12<02:12,  3.99it/s]\u001b[A\n",
            "Embedding:  60%|█████▉    | 781/1310 [03:12<02:13,  3.95it/s]\u001b[A\n",
            "Embedding:  60%|█████▉    | 782/1310 [03:12<02:13,  3.94it/s]\u001b[A\n",
            "Embedding:  60%|█████▉    | 783/1310 [03:13<02:12,  3.97it/s]\u001b[A\n",
            "Embedding:  60%|█████▉    | 784/1310 [03:13<02:12,  3.97it/s]\u001b[A\n",
            "Embedding:  60%|█████▉    | 785/1310 [03:13<02:11,  3.98it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 786/1310 [03:13<02:12,  3.97it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 787/1310 [03:14<02:12,  3.94it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 788/1310 [03:14<02:10,  4.00it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 789/1310 [03:14<02:11,  3.96it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 790/1310 [03:14<02:11,  3.95it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 791/1310 [03:15<02:10,  3.98it/s]\u001b[A\n",
            "Embedding:  60%|██████    | 792/1310 [03:15<02:09,  4.01it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 793/1310 [03:15<02:09,  3.98it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 794/1310 [03:15<02:10,  3.94it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 795/1310 [03:16<02:09,  3.98it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 796/1310 [03:16<02:11,  3.91it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 797/1310 [03:16<02:09,  3.96it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 798/1310 [03:16<02:07,  4.00it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 799/1310 [03:17<02:09,  3.95it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 800/1310 [03:17<02:09,  3.93it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 801/1310 [03:17<02:09,  3.93it/s]\u001b[A\n",
            "Embedding:  61%|██████    | 802/1310 [03:17<02:08,  3.96it/s]\u001b[A\n",
            "Embedding:  61%|██████▏   | 803/1310 [03:18<02:07,  3.98it/s]\u001b[A\n",
            "Embedding:  61%|██████▏   | 804/1310 [03:18<02:07,  3.97it/s]\u001b[A\n",
            "Embedding:  61%|██████▏   | 805/1310 [03:18<02:06,  3.98it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 806/1310 [03:18<02:06,  3.99it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 807/1310 [03:19<02:06,  3.97it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 808/1310 [03:19<02:06,  3.96it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 809/1310 [03:19<02:07,  3.92it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 810/1310 [03:19<02:07,  3.92it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 811/1310 [03:20<02:08,  3.88it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 812/1310 [03:20<02:09,  3.84it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 813/1310 [03:20<02:08,  3.87it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 814/1310 [03:20<02:05,  3.94it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 815/1310 [03:21<02:05,  3.93it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 816/1310 [03:21<02:05,  3.94it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 817/1310 [03:21<02:05,  3.94it/s]\u001b[A\n",
            "Embedding:  62%|██████▏   | 818/1310 [03:21<02:05,  3.93it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 819/1310 [03:22<02:03,  3.97it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 820/1310 [03:22<02:03,  3.96it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 821/1310 [03:22<02:03,  3.97it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 822/1310 [03:22<02:03,  3.94it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 823/1310 [03:23<02:03,  3.95it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 824/1310 [03:23<02:03,  3.92it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 825/1310 [03:23<02:05,  3.87it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 826/1310 [03:23<02:04,  3.88it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 827/1310 [03:24<02:06,  3.81it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 828/1310 [03:24<02:03,  3.91it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 829/1310 [03:24<02:01,  3.94it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 830/1310 [03:24<02:01,  3.96it/s]\u001b[A\n",
            "Embedding:  63%|██████▎   | 831/1310 [03:25<02:00,  3.96it/s]\u001b[A\n",
            "Embedding:  64%|██████▎   | 832/1310 [03:25<01:59,  3.99it/s]\u001b[A\n",
            "Embedding:  64%|██████▎   | 833/1310 [03:25<01:59,  3.98it/s]\u001b[A\n",
            "Embedding:  64%|██████▎   | 834/1310 [03:25<02:00,  3.96it/s]\u001b[A\n",
            "Embedding:  64%|██████▎   | 835/1310 [03:26<02:00,  3.95it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 836/1310 [03:26<01:59,  3.98it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 837/1310 [03:26<01:59,  3.97it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 838/1310 [03:26<01:58,  3.97it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 839/1310 [03:27<01:58,  3.97it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 840/1310 [03:27<01:57,  3.99it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 841/1310 [03:27<01:57,  4.00it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 842/1310 [03:27<01:57,  4.00it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 843/1310 [03:28<01:57,  3.97it/s]\u001b[A\n",
            "Embedding:  64%|██████▍   | 844/1310 [03:28<01:58,  3.92it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 845/1310 [03:28<01:56,  3.98it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 846/1310 [03:29<01:57,  3.96it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 847/1310 [03:29<01:57,  3.95it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 848/1310 [03:29<01:58,  3.90it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 849/1310 [03:29<01:57,  3.92it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 850/1310 [03:30<01:56,  3.96it/s]\u001b[A\n",
            "Embedding:  65%|██████▍   | 851/1310 [03:30<01:56,  3.94it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 852/1310 [03:30<01:56,  3.95it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 853/1310 [03:30<01:56,  3.94it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 854/1310 [03:31<01:56,  3.91it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 855/1310 [03:31<01:55,  3.92it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 856/1310 [03:31<01:54,  3.95it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 857/1310 [03:31<01:55,  3.93it/s]\u001b[A\n",
            "Embedding:  65%|██████▌   | 858/1310 [03:32<01:54,  3.96it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 859/1310 [03:32<01:53,  3.96it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 860/1310 [03:32<01:53,  3.96it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 861/1310 [03:32<01:53,  3.97it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 862/1310 [03:33<01:53,  3.95it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 863/1310 [03:33<01:53,  3.93it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 864/1310 [03:33<01:53,  3.93it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 865/1310 [03:33<01:53,  3.92it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 866/1310 [03:34<01:52,  3.94it/s]\u001b[A\n",
            "Embedding:  66%|██████▌   | 867/1310 [03:34<01:52,  3.94it/s]\u001b[A\n",
            "Embedding:  66%|██████▋   | 868/1310 [03:34<01:51,  3.96it/s]\u001b[A\n",
            "Embedding:  66%|██████▋   | 869/1310 [03:34<01:51,  3.94it/s]\u001b[A\n",
            "Embedding:  66%|██████▋   | 870/1310 [03:35<01:52,  3.93it/s]\u001b[A\n",
            "Embedding:  66%|██████▋   | 871/1310 [03:35<01:50,  3.96it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 872/1310 [03:35<01:50,  3.95it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 873/1310 [03:35<01:51,  3.93it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 874/1310 [03:36<01:51,  3.90it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 875/1310 [03:36<01:52,  3.88it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 876/1310 [03:36<01:51,  3.90it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 877/1310 [03:36<01:49,  3.94it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 878/1310 [03:37<01:48,  3.97it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 879/1310 [03:37<01:50,  3.90it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 880/1310 [03:37<01:50,  3.89it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 881/1310 [03:37<01:51,  3.86it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 882/1310 [03:38<01:49,  3.89it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 883/1310 [03:38<01:48,  3.94it/s]\u001b[A\n",
            "Embedding:  67%|██████▋   | 884/1310 [03:38<01:48,  3.92it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 885/1310 [03:38<01:48,  3.90it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 886/1310 [03:39<01:48,  3.92it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 887/1310 [03:39<01:47,  3.93it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 888/1310 [03:39<01:46,  3.97it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 889/1310 [03:39<01:46,  3.94it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 890/1310 [03:40<01:46,  3.94it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 891/1310 [03:40<01:45,  3.96it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 892/1310 [03:40<01:45,  3.95it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 893/1310 [03:40<01:45,  3.95it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 894/1310 [03:41<01:45,  3.95it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 895/1310 [03:41<01:44,  3.97it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 896/1310 [03:41<01:45,  3.94it/s]\u001b[A\n",
            "Embedding:  68%|██████▊   | 897/1310 [03:41<01:44,  3.97it/s]\u001b[A\n",
            "Embedding:  69%|██████▊   | 898/1310 [03:42<01:43,  3.97it/s]\u001b[A\n",
            "Embedding:  69%|██████▊   | 899/1310 [03:42<01:43,  3.96it/s]\u001b[A\n",
            "Embedding:  69%|██████▊   | 900/1310 [03:42<01:44,  3.91it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 901/1310 [03:42<01:43,  3.96it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 902/1310 [03:43<01:43,  3.94it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 903/1310 [03:43<01:43,  3.93it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 904/1310 [03:43<01:42,  3.95it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 905/1310 [03:44<01:43,  3.92it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 906/1310 [03:44<01:43,  3.91it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 907/1310 [03:44<01:42,  3.92it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 908/1310 [03:44<01:41,  3.95it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 909/1310 [03:45<01:42,  3.92it/s]\u001b[A\n",
            "Embedding:  69%|██████▉   | 910/1310 [03:45<01:42,  3.92it/s]\u001b[A\n",
            "Embedding:  70%|██████▉   | 911/1310 [03:45<01:42,  3.88it/s]\u001b[A\n",
            "Embedding:  70%|██████▉   | 912/1310 [03:45<01:40,  3.95it/s]\u001b[A\n",
            "Embedding:  70%|██████▉   | 913/1310 [03:46<01:40,  3.94it/s]\u001b[A\n",
            "Embedding:  70%|██████▉   | 914/1310 [03:46<01:40,  3.95it/s]\u001b[A\n",
            "Embedding:  70%|██████▉   | 915/1310 [03:46<01:39,  3.96it/s]\u001b[A\n",
            "Embedding:  70%|██████▉   | 916/1310 [03:46<01:38,  3.99it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 917/1310 [03:47<01:38,  3.99it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 918/1310 [03:47<01:38,  3.99it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 919/1310 [03:47<01:38,  3.98it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 920/1310 [03:47<01:38,  3.97it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 921/1310 [03:48<01:37,  3.97it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 922/1310 [03:48<01:37,  3.97it/s]\u001b[A\n",
            "Embedding:  70%|███████   | 923/1310 [03:48<01:37,  3.97it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 924/1310 [03:48<01:37,  3.96it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 925/1310 [03:49<01:37,  3.93it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 926/1310 [03:49<01:37,  3.94it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 927/1310 [03:49<01:36,  3.95it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 928/1310 [03:49<01:36,  3.96it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 929/1310 [03:50<01:36,  3.96it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 930/1310 [03:50<01:35,  3.98it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 931/1310 [03:50<01:35,  3.95it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 932/1310 [03:50<01:37,  3.88it/s]\u001b[A\n",
            "Embedding:  71%|███████   | 933/1310 [03:51<01:36,  3.91it/s]\u001b[A\n",
            "Embedding:  71%|███████▏  | 934/1310 [03:51<01:35,  3.96it/s]\u001b[A\n",
            "Embedding:  71%|███████▏  | 935/1310 [03:51<01:34,  3.96it/s]\u001b[A\n",
            "Embedding:  71%|███████▏  | 936/1310 [03:51<01:34,  3.94it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 937/1310 [03:52<01:34,  3.96it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 938/1310 [03:52<01:35,  3.90it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 939/1310 [03:52<01:34,  3.92it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 940/1310 [03:52<01:35,  3.87it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 941/1310 [03:53<01:34,  3.90it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 942/1310 [03:53<01:33,  3.93it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 943/1310 [03:53<01:33,  3.92it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 944/1310 [03:53<01:33,  3.90it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 945/1310 [03:54<01:31,  3.97it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 946/1310 [03:54<01:31,  3.96it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 947/1310 [03:54<01:31,  3.98it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 948/1310 [03:54<01:31,  3.95it/s]\u001b[A\n",
            "Embedding:  72%|███████▏  | 949/1310 [03:55<01:31,  3.92it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 950/1310 [03:55<01:31,  3.93it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 951/1310 [03:55<01:31,  3.91it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 952/1310 [03:55<01:30,  3.95it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 953/1310 [03:56<01:29,  3.97it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 954/1310 [03:56<01:31,  3.91it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 955/1310 [03:56<01:30,  3.93it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 956/1310 [03:56<01:29,  3.95it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 957/1310 [03:57<01:30,  3.90it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 958/1310 [03:57<01:29,  3.95it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 959/1310 [03:57<01:27,  4.00it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 960/1310 [03:57<01:28,  3.94it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 961/1310 [03:58<01:28,  3.95it/s]\u001b[A\n",
            "Embedding:  73%|███████▎  | 962/1310 [03:58<01:28,  3.94it/s]\u001b[A\n",
            "Embedding:  74%|███████▎  | 963/1310 [03:58<01:27,  3.96it/s]\u001b[A\n",
            "Embedding:  74%|███████▎  | 964/1310 [03:58<01:26,  3.98it/s]\u001b[A\n",
            "Embedding:  74%|███████▎  | 965/1310 [03:59<01:26,  3.98it/s]\u001b[A\n",
            "Embedding:  74%|███████▎  | 966/1310 [03:59<01:26,  3.97it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 967/1310 [03:59<01:27,  3.94it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 968/1310 [03:59<01:26,  3.97it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 969/1310 [04:00<01:25,  3.97it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 970/1310 [04:00<01:26,  3.94it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 971/1310 [04:00<01:26,  3.94it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 972/1310 [04:00<01:25,  3.96it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 973/1310 [04:01<01:25,  3.96it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 974/1310 [04:01<01:24,  3.97it/s]\u001b[A\n",
            "Embedding:  74%|███████▍  | 975/1310 [04:01<01:24,  3.96it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 976/1310 [04:01<01:23,  3.99it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 977/1310 [04:02<01:23,  3.97it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 978/1310 [04:02<01:22,  4.00it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 979/1310 [04:02<01:23,  3.97it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 980/1310 [04:02<01:22,  3.98it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 981/1310 [04:03<01:22,  3.99it/s]\u001b[A\n",
            "Embedding:  75%|███████▍  | 982/1310 [04:03<01:22,  3.97it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 983/1310 [04:03<01:21,  3.99it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 984/1310 [04:03<01:22,  3.97it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 985/1310 [04:04<01:22,  3.96it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 986/1310 [04:04<01:23,  3.90it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 987/1310 [04:04<01:22,  3.93it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 988/1310 [04:05<01:21,  3.94it/s]\u001b[A\n",
            "Embedding:  75%|███████▌  | 989/1310 [04:05<01:21,  3.92it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 990/1310 [04:05<01:20,  3.96it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 991/1310 [04:05<01:19,  3.99it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 992/1310 [04:06<01:20,  3.97it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 993/1310 [04:06<01:20,  3.96it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 994/1310 [04:06<01:20,  3.94it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 995/1310 [04:06<01:19,  3.95it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 996/1310 [04:07<01:20,  3.90it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 997/1310 [04:07<01:19,  3.92it/s]\u001b[A\n",
            "Embedding:  76%|███████▌  | 998/1310 [04:07<01:18,  3.95it/s]\u001b[A\n",
            "Embedding:  76%|███████▋  | 999/1310 [04:07<01:18,  3.96it/s]\u001b[A\n",
            "Embedding:  76%|███████▋  | 1000/1310 [04:08<01:18,  3.93it/s]\u001b[A\n",
            "Embedding:  76%|███████▋  | 1001/1310 [04:08<01:19,  3.91it/s]\u001b[A\n",
            "Embedding:  76%|███████▋  | 1002/1310 [04:08<01:18,  3.95it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1003/1310 [04:08<01:17,  3.95it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1004/1310 [04:09<01:17,  3.94it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1005/1310 [04:09<01:17,  3.94it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1006/1310 [04:09<01:16,  3.98it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1007/1310 [04:09<01:16,  3.95it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1008/1310 [04:10<01:16,  3.96it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1009/1310 [04:10<01:15,  3.97it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1010/1310 [04:10<01:15,  3.99it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1011/1310 [04:10<01:15,  3.97it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1012/1310 [04:11<01:15,  3.97it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1013/1310 [04:11<01:15,  3.93it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1014/1310 [04:11<01:14,  3.95it/s]\u001b[A\n",
            "Embedding:  77%|███████▋  | 1015/1310 [04:11<01:14,  3.94it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1016/1310 [04:12<01:15,  3.90it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1017/1310 [04:12<01:15,  3.89it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1018/1310 [04:12<01:14,  3.92it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1019/1310 [04:12<01:14,  3.92it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1020/1310 [04:13<01:13,  3.92it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1021/1310 [04:13<01:13,  3.91it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1022/1310 [04:13<01:13,  3.94it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1023/1310 [04:13<01:12,  3.93it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1024/1310 [04:14<01:13,  3.92it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1025/1310 [04:14<01:11,  3.96it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1026/1310 [04:14<01:12,  3.91it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1027/1310 [04:14<01:11,  3.94it/s]\u001b[A\n",
            "Embedding:  78%|███████▊  | 1028/1310 [04:15<01:11,  3.92it/s]\u001b[A\n",
            "Embedding:  79%|███████▊  | 1029/1310 [04:15<01:12,  3.89it/s]\u001b[A\n",
            "Embedding:  79%|███████▊  | 1030/1310 [04:15<01:11,  3.90it/s]\u001b[A\n",
            "Embedding:  79%|███████▊  | 1031/1310 [04:15<01:10,  3.96it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1032/1310 [04:16<01:11,  3.91it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1033/1310 [04:16<01:10,  3.93it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1034/1310 [04:16<01:09,  3.96it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1035/1310 [04:16<01:09,  3.94it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1036/1310 [04:17<01:09,  3.97it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1037/1310 [04:17<01:09,  3.92it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1038/1310 [04:17<01:09,  3.94it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1039/1310 [04:17<01:09,  3.92it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1040/1310 [04:18<01:08,  3.94it/s]\u001b[A\n",
            "Embedding:  79%|███████▉  | 1041/1310 [04:18<01:08,  3.95it/s]\u001b[A\n",
            "Embedding:  80%|███████▉  | 1042/1310 [04:18<01:07,  3.95it/s]\u001b[A\n",
            "Embedding:  80%|███████▉  | 1043/1310 [04:18<01:08,  3.91it/s]\u001b[A\n",
            "Embedding:  80%|███████▉  | 1044/1310 [04:19<01:08,  3.86it/s]\u001b[A\n",
            "Embedding:  80%|███████▉  | 1045/1310 [04:19<01:09,  3.83it/s]\u001b[A\n",
            "Embedding:  80%|███████▉  | 1046/1310 [04:19<01:08,  3.87it/s]\u001b[A\n",
            "Embedding:  80%|███████▉  | 1047/1310 [04:20<01:07,  3.91it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1048/1310 [04:20<01:06,  3.92it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1049/1310 [04:20<01:06,  3.95it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1050/1310 [04:20<01:05,  3.97it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1051/1310 [04:21<01:05,  3.93it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1052/1310 [04:21<01:04,  3.98it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1053/1310 [04:21<01:05,  3.95it/s]\u001b[A\n",
            "Embedding:  80%|████████  | 1054/1310 [04:21<01:05,  3.93it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1055/1310 [04:22<01:04,  3.94it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1056/1310 [04:22<01:04,  3.94it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1057/1310 [04:22<01:05,  3.89it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1058/1310 [04:22<01:04,  3.93it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1059/1310 [04:23<01:04,  3.91it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1060/1310 [04:23<01:04,  3.89it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1061/1310 [04:23<01:03,  3.93it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1062/1310 [04:23<01:02,  3.95it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1063/1310 [04:24<01:02,  3.94it/s]\u001b[A\n",
            "Embedding:  81%|████████  | 1064/1310 [04:24<01:02,  3.96it/s]\u001b[A\n",
            "Embedding:  81%|████████▏ | 1065/1310 [04:24<01:01,  3.98it/s]\u001b[A\n",
            "Embedding:  81%|████████▏ | 1066/1310 [04:24<01:01,  3.98it/s]\u001b[A\n",
            "Embedding:  81%|████████▏ | 1067/1310 [04:25<01:01,  3.97it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1068/1310 [04:25<01:00,  4.00it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1069/1310 [04:25<00:59,  4.02it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1070/1310 [04:25<00:59,  4.01it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1071/1310 [04:26<00:59,  4.00it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1072/1310 [04:26<00:59,  3.99it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1073/1310 [04:26<00:59,  3.97it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1074/1310 [04:26<00:59,  3.96it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1075/1310 [04:27<00:59,  3.97it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1076/1310 [04:27<00:59,  3.96it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1077/1310 [04:27<00:58,  3.98it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1078/1310 [04:27<00:58,  3.95it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1079/1310 [04:28<00:59,  3.87it/s]\u001b[A\n",
            "Embedding:  82%|████████▏ | 1080/1310 [04:28<00:58,  3.94it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1081/1310 [04:28<00:57,  3.96it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1082/1310 [04:28<00:57,  3.95it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1083/1310 [04:29<00:57,  3.96it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1084/1310 [04:29<00:56,  3.97it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1085/1310 [04:29<00:56,  3.95it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1086/1310 [04:29<00:56,  3.94it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1087/1310 [04:30<00:56,  3.97it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1088/1310 [04:30<00:56,  3.94it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1089/1310 [04:30<00:55,  3.97it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1090/1310 [04:30<00:55,  3.96it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1091/1310 [04:31<00:55,  3.95it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1092/1310 [04:31<00:55,  3.96it/s]\u001b[A\n",
            "Embedding:  83%|████████▎ | 1093/1310 [04:31<00:55,  3.94it/s]\u001b[A\n",
            "Embedding:  84%|████████▎ | 1094/1310 [04:31<00:54,  3.96it/s]\u001b[A\n",
            "Embedding:  84%|████████▎ | 1095/1310 [04:32<00:54,  3.95it/s]\u001b[A\n",
            "Embedding:  84%|████████▎ | 1096/1310 [04:32<00:54,  3.92it/s]\u001b[A\n",
            "Embedding:  84%|████████▎ | 1097/1310 [04:32<00:54,  3.92it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1098/1310 [04:32<00:53,  3.94it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1099/1310 [04:33<00:54,  3.91it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1100/1310 [04:33<00:53,  3.91it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1101/1310 [04:33<00:53,  3.93it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1102/1310 [04:33<00:52,  3.97it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1103/1310 [04:34<00:52,  3.97it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1104/1310 [04:34<00:52,  3.93it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1105/1310 [04:34<00:52,  3.93it/s]\u001b[A\n",
            "Embedding:  84%|████████▍ | 1106/1310 [04:34<00:51,  3.93it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1107/1310 [04:35<00:51,  3.95it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1108/1310 [04:35<00:50,  3.98it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1109/1310 [04:35<00:50,  3.95it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1110/1310 [04:35<00:50,  3.93it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1111/1310 [04:36<00:50,  3.96it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1112/1310 [04:36<00:50,  3.95it/s]\u001b[A\n",
            "Embedding:  85%|████████▍ | 1113/1310 [04:36<00:50,  3.88it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1114/1310 [04:36<00:50,  3.89it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1115/1310 [04:37<00:49,  3.93it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1116/1310 [04:37<00:49,  3.95it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1117/1310 [04:37<00:49,  3.94it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1118/1310 [04:38<00:49,  3.90it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1119/1310 [04:38<00:48,  3.91it/s]\u001b[A\n",
            "Embedding:  85%|████████▌ | 1120/1310 [04:38<00:48,  3.90it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1121/1310 [04:38<00:48,  3.90it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1122/1310 [04:39<00:48,  3.89it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1123/1310 [04:39<00:47,  3.91it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1124/1310 [04:39<00:47,  3.90it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1125/1310 [04:39<00:46,  3.95it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1126/1310 [04:40<00:46,  3.94it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1127/1310 [04:40<00:46,  3.93it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1128/1310 [04:40<00:45,  3.98it/s]\u001b[A\n",
            "Embedding:  86%|████████▌ | 1129/1310 [04:40<00:45,  3.94it/s]\u001b[A\n",
            "Embedding:  86%|████████▋ | 1130/1310 [04:41<00:45,  3.97it/s]\u001b[A\n",
            "Embedding:  86%|████████▋ | 1131/1310 [04:41<00:45,  3.97it/s]\u001b[A\n",
            "Embedding:  86%|████████▋ | 1132/1310 [04:41<00:44,  3.97it/s]\u001b[A\n",
            "Embedding:  86%|████████▋ | 1133/1310 [04:41<00:44,  3.98it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1134/1310 [04:42<00:44,  3.95it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1135/1310 [04:42<00:44,  3.96it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1136/1310 [04:42<00:44,  3.92it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1137/1310 [04:42<00:44,  3.88it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1138/1310 [04:43<00:44,  3.86it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1139/1310 [04:43<00:43,  3.95it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1140/1310 [04:43<00:43,  3.90it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1141/1310 [04:43<00:43,  3.84it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1142/1310 [04:44<00:43,  3.82it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1143/1310 [04:44<00:43,  3.88it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1144/1310 [04:44<00:42,  3.88it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1145/1310 [04:44<00:42,  3.90it/s]\u001b[A\n",
            "Embedding:  87%|████████▋ | 1146/1310 [04:45<00:41,  3.94it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1147/1310 [04:45<00:41,  3.93it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1148/1310 [04:45<00:40,  3.96it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1149/1310 [04:45<00:40,  3.97it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1150/1310 [04:46<00:40,  3.93it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1151/1310 [04:46<00:40,  3.88it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1152/1310 [04:46<00:40,  3.89it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1153/1310 [04:46<00:40,  3.92it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1154/1310 [04:47<00:39,  3.95it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1155/1310 [04:47<00:39,  3.93it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1156/1310 [04:47<00:39,  3.94it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1157/1310 [04:47<00:38,  3.94it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1158/1310 [04:48<00:38,  3.93it/s]\u001b[A\n",
            "Embedding:  88%|████████▊ | 1159/1310 [04:48<00:38,  3.92it/s]\u001b[A\n",
            "Embedding:  89%|████████▊ | 1160/1310 [04:48<00:38,  3.91it/s]\u001b[A\n",
            "Embedding:  89%|████████▊ | 1161/1310 [04:48<00:38,  3.86it/s]\u001b[A\n",
            "Embedding:  89%|████████▊ | 1162/1310 [04:49<00:38,  3.85it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1163/1310 [04:49<00:37,  3.88it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1164/1310 [04:49<00:37,  3.93it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1165/1310 [04:49<00:36,  3.94it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1166/1310 [04:50<00:36,  3.96it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1167/1310 [04:50<00:35,  3.99it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1168/1310 [04:50<00:35,  3.97it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1169/1310 [04:51<00:35,  3.97it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1170/1310 [04:51<00:35,  3.99it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1171/1310 [04:51<00:34,  3.97it/s]\u001b[A\n",
            "Embedding:  89%|████████▉ | 1172/1310 [04:51<00:34,  3.95it/s]\u001b[A\n",
            "Embedding:  90%|████████▉ | 1173/1310 [04:52<00:34,  3.98it/s]\u001b[A\n",
            "Embedding:  90%|████████▉ | 1174/1310 [04:52<00:34,  3.99it/s]\u001b[A\n",
            "Embedding:  90%|████████▉ | 1175/1310 [04:52<00:33,  3.99it/s]\u001b[A\n",
            "Embedding:  90%|████████▉ | 1176/1310 [04:52<00:33,  3.95it/s]\u001b[A\n",
            "Embedding:  90%|████████▉ | 1177/1310 [04:53<00:34,  3.86it/s]\u001b[A\n",
            "Embedding:  90%|████████▉ | 1178/1310 [04:53<00:33,  3.90it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1179/1310 [04:53<00:33,  3.90it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1180/1310 [04:53<00:33,  3.89it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1181/1310 [04:54<00:33,  3.90it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1182/1310 [04:54<00:32,  3.91it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1183/1310 [04:54<00:32,  3.96it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1184/1310 [04:54<00:32,  3.88it/s]\u001b[A\n",
            "Embedding:  90%|█████████ | 1185/1310 [04:55<00:32,  3.90it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1186/1310 [04:55<00:31,  3.96it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1187/1310 [04:55<00:31,  3.94it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1188/1310 [04:55<00:30,  3.96it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1189/1310 [04:56<00:30,  3.93it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1190/1310 [04:56<00:30,  3.93it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1191/1310 [04:56<00:30,  3.94it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1192/1310 [04:56<00:30,  3.93it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1193/1310 [04:57<00:29,  3.95it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1194/1310 [04:57<00:29,  3.95it/s]\u001b[A\n",
            "Embedding:  91%|█████████ | 1195/1310 [04:57<00:29,  3.95it/s]\u001b[A\n",
            "Embedding:  91%|█████████▏| 1196/1310 [04:57<00:29,  3.87it/s]\u001b[A\n",
            "Embedding:  91%|█████████▏| 1197/1310 [04:58<00:28,  3.92it/s]\u001b[A\n",
            "Embedding:  91%|█████████▏| 1198/1310 [04:58<00:28,  3.95it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1199/1310 [04:58<00:28,  3.94it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1200/1310 [04:58<00:27,  3.95it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1201/1310 [04:59<00:27,  3.97it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1202/1310 [04:59<00:27,  3.96it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1203/1310 [04:59<00:27,  3.86it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1204/1310 [04:59<00:27,  3.85it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1205/1310 [05:00<00:27,  3.87it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1206/1310 [05:00<00:26,  3.93it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1207/1310 [05:00<00:26,  3.91it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1208/1310 [05:00<00:25,  3.94it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1209/1310 [05:01<00:25,  3.95it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1210/1310 [05:01<00:25,  3.94it/s]\u001b[A\n",
            "Embedding:  92%|█████████▏| 1211/1310 [05:01<00:25,  3.91it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1212/1310 [05:01<00:24,  3.94it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1213/1310 [05:02<00:24,  3.96it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1214/1310 [05:02<00:24,  3.96it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1215/1310 [05:02<00:24,  3.95it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1216/1310 [05:02<00:24,  3.88it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1217/1310 [05:03<00:23,  3.90it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1218/1310 [05:03<00:23,  3.89it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1219/1310 [05:03<00:23,  3.94it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1220/1310 [05:03<00:23,  3.89it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1221/1310 [05:04<00:22,  3.93it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1222/1310 [05:04<00:22,  3.94it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1223/1310 [05:04<00:22,  3.95it/s]\u001b[A\n",
            "Embedding:  93%|█████████▎| 1224/1310 [05:04<00:21,  3.95it/s]\u001b[A\n",
            "Embedding:  94%|█████████▎| 1225/1310 [05:05<00:21,  3.93it/s]\u001b[A\n",
            "Embedding:  94%|█████████▎| 1226/1310 [05:05<00:21,  3.89it/s]\u001b[A\n",
            "Embedding:  94%|█████████▎| 1227/1310 [05:05<00:21,  3.85it/s]\u001b[A\n",
            "Embedding:  94%|█████████▎| 1228/1310 [05:06<00:21,  3.86it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1229/1310 [05:06<00:20,  3.86it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1230/1310 [05:06<00:20,  3.85it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1231/1310 [05:06<00:20,  3.87it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1232/1310 [05:07<00:20,  3.83it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1233/1310 [05:07<00:20,  3.85it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1234/1310 [05:07<00:19,  3.91it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1235/1310 [05:07<00:19,  3.92it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1236/1310 [05:08<00:18,  3.90it/s]\u001b[A\n",
            "Embedding:  94%|█████████▍| 1237/1310 [05:08<00:18,  3.90it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1238/1310 [05:08<00:18,  3.88it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1239/1310 [05:08<00:18,  3.88it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1240/1310 [05:09<00:18,  3.87it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1241/1310 [05:09<00:17,  3.88it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1242/1310 [05:09<00:17,  3.84it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1243/1310 [05:09<00:17,  3.88it/s]\u001b[A\n",
            "Embedding:  95%|█████████▍| 1244/1310 [05:10<00:16,  3.91it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1245/1310 [05:10<00:16,  3.91it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1246/1310 [05:10<00:16,  3.92it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1247/1310 [05:10<00:15,  3.96it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1248/1310 [05:11<00:15,  3.97it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1249/1310 [05:11<00:15,  3.94it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1250/1310 [05:11<00:15,  3.96it/s]\u001b[A\n",
            "Embedding:  95%|█████████▌| 1251/1310 [05:11<00:14,  3.93it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1252/1310 [05:12<00:14,  3.97it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1253/1310 [05:12<00:14,  3.90it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1254/1310 [05:12<00:14,  3.92it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1255/1310 [05:12<00:13,  3.95it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1256/1310 [05:13<00:13,  3.96it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1257/1310 [05:13<00:13,  3.93it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1258/1310 [05:13<00:13,  3.91it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1259/1310 [05:13<00:13,  3.88it/s]\u001b[A\n",
            "Embedding:  96%|█████████▌| 1260/1310 [05:14<00:12,  3.92it/s]\u001b[A\n",
            "Embedding:  96%|█████████▋| 1261/1310 [05:14<00:12,  3.93it/s]\u001b[A\n",
            "Embedding:  96%|█████████▋| 1262/1310 [05:14<00:12,  3.89it/s]\u001b[A\n",
            "Embedding:  96%|█████████▋| 1263/1310 [05:14<00:12,  3.92it/s]\u001b[A\n",
            "Embedding:  96%|█████████▋| 1264/1310 [05:15<00:11,  3.91it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1265/1310 [05:15<00:11,  3.90it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1266/1310 [05:15<00:11,  3.92it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1267/1310 [05:16<00:10,  3.93it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1268/1310 [05:16<00:10,  3.84it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1269/1310 [05:16<00:10,  3.87it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1270/1310 [05:16<00:10,  3.89it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1271/1310 [05:17<00:10,  3.87it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1272/1310 [05:17<00:09,  3.88it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1273/1310 [05:17<00:09,  3.89it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1274/1310 [05:17<00:09,  3.87it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1275/1310 [05:18<00:09,  3.88it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1276/1310 [05:18<00:09,  3.76it/s]\u001b[A\n",
            "Embedding:  97%|█████████▋| 1277/1310 [05:18<00:08,  3.81it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1278/1310 [05:18<00:08,  3.85it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1279/1310 [05:19<00:07,  3.88it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1280/1310 [05:19<00:07,  3.89it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1281/1310 [05:19<00:07,  3.84it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1282/1310 [05:19<00:07,  3.89it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1283/1310 [05:20<00:06,  3.89it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1284/1310 [05:20<00:06,  3.90it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1285/1310 [05:20<00:06,  3.95it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1286/1310 [05:20<00:06,  3.91it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1287/1310 [05:21<00:05,  3.90it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1288/1310 [05:21<00:05,  3.93it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1289/1310 [05:21<00:05,  3.90it/s]\u001b[A\n",
            "Embedding:  98%|█████████▊| 1290/1310 [05:21<00:05,  3.88it/s]\u001b[A\n",
            "Embedding:  99%|█████████▊| 1291/1310 [05:22<00:04,  3.87it/s]\u001b[A\n",
            "Embedding:  99%|█████████▊| 1292/1310 [05:22<00:04,  3.87it/s]\u001b[A\n",
            "Embedding:  99%|█████████▊| 1293/1310 [05:22<00:04,  3.88it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1294/1310 [05:22<00:04,  3.86it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1295/1310 [05:23<00:03,  3.86it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1296/1310 [05:23<00:03,  3.87it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1297/1310 [05:23<00:03,  3.88it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1298/1310 [05:24<00:03,  3.92it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1299/1310 [05:24<00:02,  3.96it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1300/1310 [05:24<00:02,  3.92it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1301/1310 [05:24<00:02,  3.93it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1302/1310 [05:25<00:02,  3.94it/s]\u001b[A\n",
            "Embedding:  99%|█████████▉| 1303/1310 [05:25<00:01,  3.97it/s]\u001b[A\n",
            "Embedding: 100%|█████████▉| 1304/1310 [05:25<00:01,  3.95it/s]\u001b[A\n",
            "Embedding: 100%|█████████▉| 1305/1310 [05:25<00:01,  3.94it/s]\u001b[A\n",
            "Embedding: 100%|█████████▉| 1306/1310 [05:26<00:01,  3.92it/s]\u001b[A\n",
            "Embedding: 100%|█████████▉| 1307/1310 [05:26<00:00,  3.95it/s]\u001b[A\n",
            "Embedding: 100%|█████████▉| 1308/1310 [05:26<00:00,  3.92it/s]\u001b[A\n",
            "Embedding: 100%|██████████| 1310/1310 [05:26<00:00,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (20948, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain final data / split into train and test sets"
      ],
      "metadata": {
        "id": "npg1ABIG2x4h"
      },
      "id": "npg1ABIG2x4h"
    },
    {
      "cell_type": "code",
      "source": [
        "X_full = np.concatenate([numeric_block, embeddings], axis=1)\n",
        "print(\"Final feature matrix:\", X_full.shape)\n",
        "# embeddings have dim 768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf2wSkPi2iHJ",
        "outputId": "ccc55bab-4414-49b1-9441-1884270d0442"
      },
      "id": "Yf2wSkPi2iHJ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final feature matrix: (20948, 809)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the same identical split of the data to train/test all models for fair comparison\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_full, y, test_size=0.20, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "SyTL3BQY2iKc"
      },
      "id": "SyTL3BQY2iKc",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Comparisons\n",
        "\n",
        "Using the same train and test sets, we will compare the performance and optimization solution quality of various models on the same data splits. We use a manual grid search to try and identify the optimal hyperparamters for each model architecture to compare how the best models measure up to each other."
      ],
      "metadata": {
        "id": "j7p3AWcvGeFl"
      },
      "id": "j7p3AWcvGeFl"
    },
    {
      "cell_type": "code",
      "source": [
        "# helper func for getting metrics\n",
        "def metric_dict(y_true, y_pred, prob, runtime):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"ROC_AUC\": roc_auc_score(y_true, prob),\n",
        "        \"Runtime_s\": runtime\n",
        "    }"
      ],
      "metadata": {
        "id": "_o4Z9GfTPsOs"
      },
      "id": "_o4Z9GfTPsOs",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_grid_search(model_fn, param_grid_list, train_fn, X_train, y_train, X_test, y_test, score_key='F1'):\n",
        "    best_score = -float('inf')\n",
        "    best_params = None\n",
        "    best_metrics = None\n",
        "\n",
        "    for param_dict in param_grid_list:\n",
        "        print(f\"Testing: {param_dict}\")\n",
        "        model = model_fn(**param_dict)\n",
        "        model, metrics = train_fn(model, X_train, y_train, X_test, y_test)\n",
        "        score = metrics[score_key]\n",
        "        print(f\"{score_key}: {score:.4f}\\\\n\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = param_dict\n",
        "            best_metrics = metrics\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(f\"Best {score_key}: {best_score:.4f}\")\n",
        "    return best_params, best_metrics\n"
      ],
      "metadata": {
        "id": "B8ckkuie7h50"
      },
      "id": "B8ckkuie7h50",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Models"
      ],
      "metadata": {
        "id": "JKVwW11gP_85"
      },
      "id": "JKVwW11gP_85"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_sklearn_model(model, X_train, y_train, X_test, y_test):\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    prob = model.predict_proba(X_test)[:, 1]\n",
        "    pred = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, pred, prob, time.time() - start)\n"
      ],
      "metadata": {
        "id": "F9qlgj7WET-p"
      },
      "id": "F9qlgj7WET-p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "These first two cells include the embeddings in model training / hyperparameter tuning and look at the effects of using weights / no weights for BCE."
      ],
      "metadata": {
        "id": "SQs3O6u0EYsO"
      },
      "id": "SQs3O6u0EYsO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE with embeddings"
      ],
      "metadata": {
        "id": "QzhLKRCdIfwZ"
      },
      "id": "QzhLKRCdIfwZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid of hyperparameters to test\n",
        "logreg_grid = [\n",
        "    {'C': 0.01, 'class_weight': None},\n",
        "    {'C': 0.1,  'class_weight': None},\n",
        "    {'C': 0.2, 'class_weight': None},\n",
        "    {'C': 0.3, 'class_weight': None},\n",
        "    {'C': 0.4, 'class_weight': None},\n",
        "    {'C': 0.5, 'class_weight': None},\n",
        "    {'C': 0.6, 'class_weight': None},\n",
        "    {'C': 0.7, 'class_weight': None},\n",
        "    {'C': 0.8, 'class_weight': None},\n",
        "    {'C': 0.9, 'class_weight': None},\n",
        "    {'C': 1.0,  'class_weight': None},\n",
        "    {'C': 10.0, 'class_weight': None}\n",
        "]\n",
        "\n",
        "logreg_fn = lambda **kwargs: LogisticRegression(max_iter=1000, solver='lbfgs', **kwargs)\n",
        "\n",
        "manual_grid_search(logreg_fn, logreg_grid, train_sklearn_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KhFnlMT8wiD",
        "outputId": "88dc8ae4-63fe-41b1-c9ef-44c37d9f01a1"
      },
      "id": "8KhFnlMT8wiD",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'C': 0.01, 'class_weight': None}\n",
            "F1: 0.0483\\n\n",
            "Testing: {'C': 0.1, 'class_weight': None}\n",
            "F1: 0.0795\\n\n",
            "Testing: {'C': 0.2, 'class_weight': None}\n",
            "F1: 0.0820\\n\n",
            "Testing: {'C': 0.3, 'class_weight': None}\n",
            "F1: 0.0816\\n\n",
            "Testing: {'C': 0.4, 'class_weight': None}\n",
            "F1: 0.0878\\n\n",
            "Testing: {'C': 0.5, 'class_weight': None}\n",
            "F1: 0.0909\\n\n",
            "Testing: {'C': 0.6, 'class_weight': None}\n",
            "F1: 0.0911\\n\n",
            "Testing: {'C': 0.7, 'class_weight': None}\n",
            "F1: 0.0906\\n\n",
            "Testing: {'C': 0.8, 'class_weight': None}\n",
            "F1: 0.0902\\n\n",
            "Testing: {'C': 0.9, 'class_weight': None}\n",
            "F1: 0.0931\\n\n",
            "Testing: {'C': 1.0, 'class_weight': None}\n",
            "F1: 0.0900\\n\n",
            "Testing: {'C': 10.0, 'class_weight': None}\n",
            "F1: 0.1193\\n\n",
            "✅ Best Parameters: {'C': 10.0, 'class_weight': None}\n",
            "✅ Best F1: 0.1193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 10.0, 'class_weight': None},\n",
              " {'Accuracy': 0.8625298329355608,\n",
              "  'Precision': 0.38613861386138615,\n",
              "  'Recall': 0.0705244122965642,\n",
              "  'F1': 0.11926605504587157,\n",
              "  'ROC_AUC': np.float64(0.7034154194806144),\n",
              "  'Runtime_s': 66.59792733192444})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE with embeddings"
      ],
      "metadata": {
        "id": "w-gAQkLcIkjt"
      },
      "id": "w-gAQkLcIkjt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: weighted BCE and embeddings\n",
        "\n",
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# Grid of hyperparameters to test\n",
        "logreg_grid = [\n",
        "    {'C': 0.01, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.1,  'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.2, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.3, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.4, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.5, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.6, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.7, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.8, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.9, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 1.0,  'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 10.0, 'class_weight': {0:1, 1:pos_weight}}\n",
        "]\n",
        "\n",
        "logreg_fn = lambda **kwargs: LogisticRegression(max_iter=1000, solver='lbfgs', **kwargs)\n",
        "\n",
        "manual_grid_search(logreg_fn, logreg_grid, train_sklearn_model, X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JCIT-T2O957",
        "outputId": "a1144935-57a2-4d09-aa1e-ef7732e36dfb"
      },
      "id": "-JCIT-T2O957",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'C': 0.01, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3369\\n\n",
            "Testing: {'C': 0.1, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3400\\n\n",
            "Testing: {'C': 0.2, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3404\\n\n",
            "Testing: {'C': 0.3, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3395\\n\n",
            "Testing: {'C': 0.4, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3438\\n\n",
            "Testing: {'C': 0.5, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3391\\n\n",
            "Testing: {'C': 0.6, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3379\\n\n",
            "Testing: {'C': 0.7, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3386\\n\n",
            "Testing: {'C': 0.8, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3379\\n\n",
            "Testing: {'C': 0.9, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3374\\n\n",
            "Testing: {'C': 1.0, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3400\\n\n",
            "Testing: {'C': 10.0, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3223\\n\n",
            "✅ Best Parameters: {'C': 0.4, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "✅ Best F1: 0.3438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 0.4, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}},\n",
              " {'Accuracy': 0.6947494033412888,\n",
              "  'Precision': 0.23997134670487105,\n",
              "  'Recall': 0.6057866184448463,\n",
              "  'F1': 0.3437660338635197,\n",
              "  'ROC_AUC': np.float64(0.7137288497116983),\n",
              "  'Runtime_s': 31.037291765213013})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These next two sets of models do not include the word embeddings and simply use only the structured clinical data for model training. Similar to the two cells above, these two cells look at the differences in model performance with/wihout weights in BCE."
      ],
      "metadata": {
        "id": "C205QylV-Err"
      },
      "id": "C205QylV-Err"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE without embeddings"
      ],
      "metadata": {
        "id": "Ip3KrVplImZm"
      },
      "id": "Ip3KrVplImZm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid of hyperparameters to test\n",
        "logreg_grid = [\n",
        "    {'C': 0.01, 'class_weight': None},\n",
        "    {'C': 0.1,  'class_weight': None},\n",
        "    {'C': 0.2, 'class_weight': None},\n",
        "    {'C': 0.3, 'class_weight': None},\n",
        "    {'C': 0.4, 'class_weight': None},\n",
        "    {'C': 0.5, 'class_weight': None},\n",
        "    {'C': 0.6, 'class_weight': None},\n",
        "    {'C': 0.7, 'class_weight': None},\n",
        "    {'C': 0.8, 'class_weight': None},\n",
        "    {'C': 0.9, 'class_weight': None},\n",
        "    {'C': 1.0,  'class_weight': None},\n",
        "    {'C': 10.0, 'class_weight': None}\n",
        "]\n",
        "\n",
        "logreg_fn = lambda **kwargs: LogisticRegression(max_iter=1000, solver='lbfgs', **kwargs)\n",
        "\n",
        "X_train_structured = np.array([arr[:41] for arr in X_train])\n",
        "X_test_structured = np.array([arr[:41] for arr in X_test])\n",
        "\n",
        "manual_grid_search(logreg_fn, logreg_grid, train_sklearn_model, X_train_structured, y_train, X_test_structured, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfkAGwwc-Ey3",
        "outputId": "a4fe3d04-fa6d-4c1e-dd6b-3209327ccbe5"
      },
      "id": "XfkAGwwc-Ey3",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'C': 0.01, 'class_weight': None}\n",
            "F1: 0.0280\\n\n",
            "Testing: {'C': 0.1, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.2, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.3, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.4, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.5, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.6, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.7, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.8, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 0.9, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 1.0, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'C': 10.0, 'class_weight': None}\n",
            "F1: 0.0314\\n\n",
            "✅ Best Parameters: {'C': 0.1, 'class_weight': None}\n",
            "✅ Best F1: 0.0314\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 0.1, 'class_weight': None},\n",
              " {'Accuracy': 0.8673031026252983,\n",
              "  'Precision': 0.42857142857142855,\n",
              "  'Recall': 0.0162748643761302,\n",
              "  'F1': 0.0313588850174216,\n",
              "  'ROC_AUC': np.float64(0.6882284298258655),\n",
              "  'Runtime_s': 3.6940088272094727})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE without embeddings"
      ],
      "metadata": {
        "id": "e_BNRpJDIomr"
      },
      "id": "e_BNRpJDIomr"
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_grid = [\n",
        "    {'C': 0.01, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.1,  'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.2, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.3, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.4, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.5, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.6, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.7, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.8, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 0.9, 'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 1.0,  'class_weight': {0:1, 1:pos_weight}},\n",
        "    {'C': 10.0, 'class_weight': {0:1, 1:pos_weight}}\n",
        "]\n",
        "\n",
        "logreg_fn = lambda **kwargs: LogisticRegression(max_iter=1000, solver='lbfgs', **kwargs)\n",
        "\n",
        "manual_grid_search(logreg_fn, logreg_grid, train_sklearn_model, X_train_structured, y_train, X_test_structured, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvFuasQS_DL9",
        "outputId": "97ff4a0e-ad58-4e51-c1de-977a5c247408"
      },
      "id": "LvFuasQS_DL9",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'C': 0.01, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3242\\n\n",
            "Testing: {'C': 0.1, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3252\\n\n",
            "Testing: {'C': 0.2, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3257\\n\n",
            "Testing: {'C': 0.3, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3247\\n\n",
            "Testing: {'C': 0.4, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3255\\n\n",
            "Testing: {'C': 0.5, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3255\\n\n",
            "Testing: {'C': 0.6, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3257\\n\n",
            "Testing: {'C': 0.7, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3257\\n\n",
            "Testing: {'C': 0.8, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3255\\n\n",
            "Testing: {'C': 0.9, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3255\\n\n",
            "Testing: {'C': 1.0, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3255\\n\n",
            "Testing: {'C': 10.0, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "F1: 0.3247\\n\n",
            "✅ Best Parameters: {'C': 0.2, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}}\n",
            "✅ Best F1: 0.3257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 0.2, 'class_weight': {0: 1, 1: np.float64(6.579375848032565)}},\n",
              " {'Accuracy': 0.6778042959427207,\n",
              "  'Precision': 0.22498274672187715,\n",
              "  'Recall': 0.5895117540687161,\n",
              "  'F1': 0.3256743256743257,\n",
              "  'ROC_AUC': np.float64(0.6899880224396535),\n",
              "  'Runtime_s': 2.336470127105713})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "yfaczT3vQGdf"
      },
      "id": "yfaczT3vQGdf"
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# More granular grid\n",
        "xgb_grid = [\n",
        "    {\n",
        "        'learning_rate': lr,\n",
        "        'max_depth': md,\n",
        "        'n_estimators': n,\n",
        "        'subsample': ss,\n",
        "        'colsample_bytree': cb,\n",
        "        'scale_pos_weight': pos_weight,\n",
        "        'seed': 42\n",
        "    }\n",
        "    for lr in [0.005, 0.05, 0.1]\n",
        "    for md in [5, 7, 9]\n",
        "    for n in [100, 250, 400]\n",
        "    for ss in [0.6, 0.8, 1.0]\n",
        "    for cb in [0.6, 0.8, 1.0]\n",
        "]\n",
        "\n",
        "xgb_fn = lambda **kwargs: XGBClassifier(\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    nthread=4,\n",
        "    verbosity=0,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "def train_sklearn_model(model, X_train, y_train, X_test, y_test):\n",
        "    import time\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    prob = model.predict_proba(X_test)[:, 1]\n",
        "    pred = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, pred, prob, time.time() - start)\n"
      ],
      "metadata": {
        "id": "2h2ar7lBI9SD"
      },
      "id": "2h2ar7lBI9SD",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE with embeddings"
      ],
      "metadata": {
        "id": "PmYmi4SeI-L0"
      },
      "id": "PmYmi4SeI-L0"
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=xgb_fn,\n",
        "    param_grid_list=xgb_grid,\n",
        "    train_fn=train_sklearn_model,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhrvbmmBPM78",
        "outputId": "01bb06d0-9abc-4fb9-b549-b3d66b35777e"
      },
      "id": "xhrvbmmBPM78",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3497\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3475\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3406\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3401\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3445\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3369\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3386\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3330\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3217\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3459\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3449\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3430\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3472\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3420\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3427\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3391\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3387\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3378\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3448\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3414\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3399\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3470\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3436\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3434\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3480\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3447\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3423\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3409\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3331\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3390\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3509\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3352\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3393\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3389\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3389\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3285\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3375\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3410\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3333\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3490\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3373\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3384\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3396\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3416\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3287\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3326\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3324\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3319\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3361\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3450\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3402\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3437\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3357\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3232\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3128\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3096\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3018\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3169\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3132\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3114\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3148\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3153\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3228\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3157\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3086\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3017\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3058\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3160\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3078\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3132\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3056\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3224\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3096\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3017\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2942\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3002\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3052\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2974\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3003\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3001\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3074\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3377\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3410\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3373\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3361\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3294\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3296\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3464\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3385\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3414\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3128\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3041\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3084\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3118\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3236\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2928\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3175\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3264\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3198\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2748\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2778\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2812\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2787\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3030\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2749\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2900\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3085\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3034\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3034\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3211\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3253\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2984\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2998\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3072\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3142\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3057\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3191\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2470\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2714\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2388\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2389\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2436\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2449\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2596\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2524\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2535\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2085\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2075\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1891\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1902\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1922\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1943\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2136\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1865\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2059\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2587\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2799\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2574\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2539\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2733\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2618\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2519\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2687\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2744\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1818\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1826\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1952\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1600\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1660\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1650\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1523\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1525\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1811\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1416\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1509\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1536\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1252\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1280\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1279\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1296\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1317\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1239\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3192\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3181\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3154\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3305\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3144\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3156\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3268\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3269\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3069\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2748\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2743\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2667\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2836\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2800\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2811\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2852\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2841\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2777\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2388\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2288\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2317\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2568\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2243\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2259\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2421\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2422\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2366\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2764\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2722\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2683\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2757\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2733\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2750\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2852\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2790\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2606\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2029\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1682\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1729\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1600\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1628\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1836\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1480\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1722\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1752\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1726\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1474\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1399\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1351\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1100\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1611\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1203\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1507\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1435\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2114\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2344\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2201\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1932\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2143\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2068\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1929\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1948\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1995\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1437\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1311\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1456\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1301\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1461\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1559\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1236\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1202\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1159\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1484\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1220\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1422\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1231\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1290\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1252\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1189\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1154\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1087\\n\n",
            "✅ Best Parameters: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "✅ Best F1: 0.3509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell shows the metrics used to generate the results for the paper. We accidentally forgot to have the metrics display at the end of each hyperparamter sweep, so we need to find them here. You need to change the X_train_eval and X_test_eval and the best_params based on which model's results you want."
      ],
      "metadata": {
        "id": "RvrkvJa-IvpS"
      },
      "id": "RvrkvJa-IvpS"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_structured = np.array([arr[:41] for arr in X_train])\n",
        "X_test_structured = np.array([arr[:41] for arr in X_test])"
      ],
      "metadata": {
        "id": "pnl5Vlo7JAXu"
      },
      "id": "pnl5Vlo7JAXu",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Use this depending on the dataset variant you evaluated:\n",
        "X_train_eval = X_train  # or X_train\n",
        "X_test_eval = X_test    # or X_test\n",
        "\n",
        "# INSERT the best params found here\n",
        "best_params = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
        "\n",
        "# Retrain best model\n",
        "model = XGBClassifier(eval_metric='logloss', use_label_encoder=False, nthread=4, verbosity=0, **best_params)\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train_eval, y_train)\n",
        "runtime = time.time() - start\n",
        "\n",
        "# Predict and evaluate\n",
        "prob = model.predict_proba(X_test_eval)[:, 1]\n",
        "pred = (prob >= 0.5).astype(int)\n",
        "\n",
        "# Compute and print metrics\n",
        "xgb_eval_metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, pred),\n",
        "    \"Precision\": precision_score(y_test, pred),\n",
        "    \"Recall\": recall_score(y_test, pred),\n",
        "    \"F1-score\": f1_score(y_test, pred),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, prob),\n",
        "    \"Training Time (s)\": runtime\n",
        "}\n",
        "\n",
        "print(\"Evaluation of Best XGBoost Model:\")\n",
        "for k, v in xgb_eval_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoHQBKLCGXB0",
        "outputId": "0d767ece-57b0-471c-9d5b-43db751d8979"
      },
      "id": "ZoHQBKLCGXB0",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation of Best XGBoost Model:\n",
            "Accuracy: 0.8635\n",
            "Precision: 0.3956\n",
            "Recall: 0.0651\n",
            "F1-score: 0.1118\n",
            "ROC-AUC: 0.6897\n",
            "Training Time (s): 26.8761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE without embeddings"
      ],
      "metadata": {
        "id": "cMe8E0-hJBGR"
      },
      "id": "cMe8E0-hJBGR"
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=xgb_fn,\n",
        "    param_grid_list=xgb_grid,\n",
        "    train_fn=train_sklearn_model,\n",
        "    X_train=X_train_structured,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test_structured,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqGiAzI1_zoH",
        "outputId": "8342aa61-3e65-497f-e600-a16b5eb38cf1"
      },
      "id": "kqGiAzI1_zoH",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3197\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3155\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3189\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3161\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3180\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3151\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3128\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3137\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3118\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3237\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3234\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3239\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3238\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3221\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3196\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3201\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3175\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3178\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3238\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3220\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3230\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3241\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3256\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3203\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3215\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3194\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3154\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3238\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3181\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3242\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3253\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3185\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3211\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3238\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3222\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3138\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3249\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3204\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3265\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3250\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3161\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3164\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3279\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3209\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3131\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3278\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3233\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3235\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3230\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3202\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3225\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3267\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3201\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3144\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3212\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3184\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3208\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3253\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3178\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3119\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3185\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3110\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3050\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3221\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3186\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3191\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3256\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3185\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3174\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3164\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3144\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3127\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3210\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3103\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3128\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3234\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3177\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3102\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3174\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3148\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3083\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3266\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3274\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3186\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3161\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3204\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3256\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3188\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3177\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3222\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3227\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3267\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3255\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3158\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3220\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3267\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3156\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3129\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3113\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3140\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3178\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3118\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3164\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3114\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3095\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3131\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3223\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3057\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3126\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3137\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3244\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3162\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3123\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3152\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3138\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3169\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3048\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2926\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3015\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2878\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3067\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3028\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2774\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2982\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3044\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2901\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2680\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2663\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2625\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2539\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2713\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2544\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2690\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2832\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2684\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2861\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2991\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3030\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2976\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2951\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3036\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2897\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2884\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2805\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2281\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2140\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2463\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2341\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2467\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2301\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2389\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2527\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2580\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2003\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1995\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1837\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1912\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1938\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1789\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1995\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2025\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2123\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3232\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3241\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3172\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3072\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3161\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3270\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3211\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3116\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3192\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3028\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3053\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2962\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3122\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3045\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3060\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3086\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3054\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3117\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2901\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2833\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2767\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2951\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2986\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2855\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2970\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2855\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2950\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3032\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2973\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3003\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3020\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3076\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2859\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.3183\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2924\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2932\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2522\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2435\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2342\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2465\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2348\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2229\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2543\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2556\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2534\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2201\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2243\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2188\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2172\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2062\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1918\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2138\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2220\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2233\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2537\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2669\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2383\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2552\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2698\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2576\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2448\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2741\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2705\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1959\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2010\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1893\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1653\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2011\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1757\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1704\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1919\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.2065\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1607\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1743\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1593\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1385\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1734\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1558\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1543\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1547\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "F1: 0.1798\\n\n",
            "✅ Best Parameters: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': np.float64(6.579375848032565), 'seed': 42}\n",
            "✅ Best F1: 0.3279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove \"scale_pos_weight\" param from grid to use standard BCE\n",
        "xgb_grid = [\n",
        "    {\n",
        "        'learning_rate': lr,\n",
        "        'max_depth': md,\n",
        "        'n_estimators': n,\n",
        "        'subsample': ss,\n",
        "        'colsample_bytree': cb,\n",
        "        'seed': 42\n",
        "    }\n",
        "    for lr in [0.005, 0.05, 0.1]\n",
        "    for md in [5, 7, 9]\n",
        "    for n in [100, 250, 400]\n",
        "    for ss in [0.6, 0.8, 1.0]\n",
        "    for cb in [0.6, 0.8, 1.0]\n",
        "]"
      ],
      "metadata": {
        "id": "RUU0gIR5JCea"
      },
      "id": "RUU0gIR5JCea",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE with embeddings"
      ],
      "metadata": {
        "id": "4HPVXl7LJClx"
      },
      "id": "4HPVXl7LJClx"
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=xgb_fn,\n",
        "    param_grid_list=xgb_grid,\n",
        "    train_fn=train_sklearn_model,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfVKa2hNJCr8",
        "outputId": "9f411913-80f9-4dea-8627-e443307d650c"
      },
      "id": "bfVKa2hNJCr8",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0108\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0143\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0212\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0212\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0247\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0317\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0385\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0420\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0349\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0281\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0245\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0279\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0246\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0280\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0282\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0177\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0141\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0281\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0737\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0828\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0829\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0604\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0635\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0574\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0575\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0581\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0706\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0906\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1054\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1022\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0840\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0911\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0942\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0727\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0698\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0945\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0612\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0580\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0512\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0547\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0611\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0444\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0446\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0512\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0481\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0747\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0783\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0717\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0720\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0692\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0814\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0841\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0752\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0780\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0749\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0812\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0690\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0753\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0818\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0816\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0810\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0752\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0749\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0758\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0784\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0660\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0694\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0661\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0690\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0660\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0666\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0721\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0753\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0783\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0787\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0759\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0725\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0691\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0689\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0722\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0787\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0693\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0816\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0753\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0789\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0724\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0725\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0719\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0757\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0692\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0539\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0763\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0704\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0508\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0570\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0607\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0377\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0510\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0546\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1108\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1087\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1054\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0829\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0915\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0970\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0968\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0965\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0899\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1034\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1104\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1118\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0954\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0972\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1094\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1013\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0896\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1078\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0919\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1016\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0808\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0875\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0777\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0808\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0990\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0810\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0962\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0863\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0918\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0896\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0875\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0841\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0960\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0931\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0905\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0872\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0893\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1021\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0875\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0782\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0868\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0870\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0846\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0846\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0818\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0903\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1086\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0810\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0752\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0810\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0809\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0840\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0908\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0779\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0963\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0903\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0809\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0808\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0874\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0778\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0784\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0753\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0813\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0965\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0906\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0710\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0722\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0780\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0749\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0692\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0780\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0813\\n\n",
            "✅ Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "✅ Best F1: 0.1118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE without embeddings"
      ],
      "metadata": {
        "id": "LE5RuPK7JCyD"
      },
      "id": "LE5RuPK7JCyD"
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=xgb_fn,\n",
        "    param_grid_list=xgb_grid,\n",
        "    train_fn=train_sklearn_model,\n",
        "    X_train=X_train_structured,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test_structured,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWYOIu5dJC4S",
        "outputId": "c9a4052f-bbb3-49dc-a377-da7c489593dc"
      },
      "id": "rWYOIu5dJC4S",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0000\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.005, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0144\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0179\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0179\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0072\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0036\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0315\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0454\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0419\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0351\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0314\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0317\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0143\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0351\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0420\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0547\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0547\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0683\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0549\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0545\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0587\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0385\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0384\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0515\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0178\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0213\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0248\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0249\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0248\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0283\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0179\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0178\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0213\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0642\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0641\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0732\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0803\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0608\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0826\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0614\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0414\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0613\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0908\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0812\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0899\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0985\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0940\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1002\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0664\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0667\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0797\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0382\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0451\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0417\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0484\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0312\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0552\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0520\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0313\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0383\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0783\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0882\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0813\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0821\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0877\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0969\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0694\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0695\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0722\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0843\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0906\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0932\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0878\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0839\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0998\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0753\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0875\\n\n",
            "Testing: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0840\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0282\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0209\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0383\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0213\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0246\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0176\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0283\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0142\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0213\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0562\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0692\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0701\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0569\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0765\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0767\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0617\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0445\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0478\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0939\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1087\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1090\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0813\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0867\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1141\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0762\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0820\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0693\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0534\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0821\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0667\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0406\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0514\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0612\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0382\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0415\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0546\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1095\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1051\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1053\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0962\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0890\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0983\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0660\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0981\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0809\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1133\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1014\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1135\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0952\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0967\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0955\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1017\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1031\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0984\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0841\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0935\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0837\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0867\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0848\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0931\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0796\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0725\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0820\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0915\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1083\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0983\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0987\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0925\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1022\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0906\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0903\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0828\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1033\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1289\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1071\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.1076\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.1051\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.1079\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.6, 'seed': 42}\n",
            "F1: 0.0905\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "F1: 0.0989\\n\n",
            "Testing: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'seed': 42}\n",
            "F1: 0.0895\\n\n",
            "✅ Best Parameters: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400, 'subsample': 0.6, 'colsample_bytree': 0.8, 'seed': 42}\n",
            "✅ Best F1: 0.1289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TabNet model"
      ],
      "metadata": {
        "id": "j1Rng6IwSEoC"
      },
      "id": "j1Rng6IwSEoC"
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "tabnet_class_weights = [1.0, pos_weight]\n",
        "\n",
        "def train_tabnet(model, X_train, y_train, X_test, y_test):\n",
        "    start = time.time()\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    y_train_1d = y_train.astype(np.int64)\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "        X_train, y_train_1d, test_size=0.2, stratify=y_train_1d, random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=['auc'],\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=1024,\n",
        "        # set weights to be 1 to ensure that BCE is weighted: https://pypi.org/project/pytorch-tabnet/\n",
        "        weights = 1\n",
        "    )\n",
        "\n",
        "    prob = model.predict_proba(X_test)[:, 1]\n",
        "    pred = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, pred, prob, time.time()-start)\n",
        "\n"
      ],
      "metadata": {
        "id": "zk5XhECLE3Q8"
      },
      "id": "zk5XhECLE3Q8",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE and embeddings"
      ],
      "metadata": {
        "id": "gPOoWe1nE3uV"
      },
      "id": "gPOoWe1nE3uV"
    },
    {
      "cell_type": "code",
      "source": [
        "tabnet_grid = [\n",
        "    {'n_d': d, 'n_a': a, 'n_steps': s, 'gamma': g}\n",
        "    for d in [16, 32, 64]\n",
        "    for a in [16, 32, 64]\n",
        "    for s in [3, 5, 7]\n",
        "    for g in [1.0, 1.5, 2.0]\n",
        "]\n",
        "\n",
        "tabnet_fn = lambda **kwargs: TabNetClassifier(\n",
        "    n_independent=2, n_shared=2, seed=42, verbose=0, **kwargs\n",
        ")\n",
        "\n",
        "manual_grid_search(tabnet_fn, tabnet_grid, train_tabnet,\n",
        "                   X_train, y_train, X_test, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcYvMc4UQ2YY",
        "outputId": "948c5b8d-ce93-48d8-9f11-8c9a4bf72e01"
      },
      "id": "mcYvMc4UQ2YY",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.66103\n",
            "F1: 0.2818\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.65418\n",
            "F1: 0.2774\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.66457\n",
            "F1: 0.2959\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.67524\n",
            "F1: 0.3060\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.63031\n",
            "F1: 0.2639\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.60747\n",
            "F1: 0.2527\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.65599\n",
            "F1: 0.2864\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.65921\n",
            "F1: 0.2725\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_auc = 0.66045\n",
            "F1: 0.2990\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.65568\n",
            "F1: 0.2963\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.67225\n",
            "F1: 0.2935\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.67149\n",
            "F1: 0.2837\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 54 and best_val_0_auc = 0.66079\n",
            "F1: 0.2913\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.63319\n",
            "F1: 0.2743\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.60005\n",
            "F1: 0.2621\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.64354\n",
            "F1: 0.2920\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_0_auc = 0.63836\n",
            "F1: 0.2762\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.61075\n",
            "F1: 0.2593\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.67433\n",
            "F1: 0.2825\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.67961\n",
            "F1: 0.3157\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.66694\n",
            "F1: 0.2770\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.66669\n",
            "F1: 0.2946\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.63557\n",
            "F1: 0.2829\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.63641\n",
            "F1: 0.2584\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.66458\n",
            "F1: 0.2898\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.66767\n",
            "F1: 0.2898\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_0_auc = 0.61416\n",
            "F1: 0.2759\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.68026\n",
            "F1: 0.3073\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.68406\n",
            "F1: 0.3092\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.67105\n",
            "F1: 0.3003\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.60446\n",
            "F1: 0.2572\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.61209\n",
            "F1: 0.2542\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_0_auc = 0.65394\n",
            "F1: 0.2814\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.6587\n",
            "F1: 0.2857\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.63416\n",
            "F1: 0.2750\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.59652\n",
            "F1: 0.2423\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.68928\n",
            "F1: 0.3137\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.67243\n",
            "F1: 0.3055\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.6416\n",
            "F1: 0.2884\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.65188\n",
            "F1: 0.2906\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_0_auc = 0.63973\n",
            "F1: 0.2882\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_0_auc = 0.63592\n",
            "F1: 0.2925\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.63271\n",
            "F1: 0.2754\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.61692\n",
            "F1: 0.2553\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.63339\n",
            "F1: 0.2641\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.66035\n",
            "F1: 0.2795\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.65827\n",
            "F1: 0.3003\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.65906\n",
            "F1: 0.2899\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.63093\n",
            "F1: 0.2656\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.64382\n",
            "F1: 0.2687\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.60739\n",
            "F1: 0.2439\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.63052\n",
            "F1: 0.2339\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_auc = 0.61161\n",
            "F1: 0.2709\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.60863\n",
            "F1: 0.2738\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.67202\n",
            "F1: 0.3017\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.66468\n",
            "F1: 0.2914\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.66278\n",
            "F1: 0.2915\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.67694\n",
            "F1: 0.3125\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.63627\n",
            "F1: 0.2479\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.61492\n",
            "F1: 0.2399\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 45 and best_val_0_auc = 0.66981\n",
            "F1: 0.3067\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.64788\n",
            "F1: 0.2725\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.60838\n",
            "F1: 0.2608\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.6672\n",
            "F1: 0.2868\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.68307\n",
            "F1: 0.3036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.67941\n",
            "F1: 0.3184\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.66546\n",
            "F1: 0.2900\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_0_auc = 0.63904\n",
            "F1: 0.2703\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.64619\n",
            "F1: 0.2735\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.64983\n",
            "F1: 0.2716\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.62315\n",
            "F1: 0.2535\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.60071\n",
            "F1: 0.2590\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.68031\n",
            "F1: 0.3088\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.66885\n",
            "F1: 0.2971\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.65103\n",
            "F1: 0.2922\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.65981\n",
            "F1: 0.2991\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.62741\n",
            "F1: 0.2791\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_0_auc = 0.6233\n",
            "F1: 0.2519\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.65475\n",
            "F1: 0.2771\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.63476\n",
            "F1: 0.2657\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.6059\n",
            "F1: 0.2696\\n\n",
            "✅ Best Parameters: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "✅ Best F1: 0.3184\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0},\n",
              " {'Accuracy': 0.6761336515513127,\n",
              "  'Precision': 0.22044506258692628,\n",
              "  'Recall': 0.5732368896925859,\n",
              "  'F1': 0.3184329482672024,\n",
              "  'ROC_AUC': np.float64(0.6800977098447192),\n",
              "  'Runtime_s': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE, no embeddings"
      ],
      "metadata": {
        "id": "IJjWQG5aE7nn"
      },
      "id": "IJjWQG5aE7nn"
    },
    {
      "cell_type": "code",
      "source": [
        "tabnet_grid = [\n",
        "    {'n_d': d, 'n_a': a, 'n_steps': s, 'gamma': g}\n",
        "    for d in [16, 32, 64]\n",
        "    for a in [16, 32, 64]\n",
        "    for s in [3, 5, 7]\n",
        "    for g in [1.0, 1.5, 2.0]\n",
        "]\n",
        "\n",
        "tabnet_fn = lambda **kwargs: TabNetClassifier(\n",
        "    n_independent=2, n_shared=2, seed=42, verbose=0, **kwargs\n",
        ")\n",
        "\n",
        "X_train_structured = np.array([arr[:41] for arr in X_train])\n",
        "X_test_structured = np.array([arr[:41] for arr in X_test])\n",
        "\n",
        "manual_grid_search(tabnet_fn, tabnet_grid, train_tabnet,\n",
        "                   X_train_structured, y_train, X_test_structured, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCuFmKkl_cGX",
        "outputId": "899d4d4d-cadd-4df1-c05d-36640ba37fa0"
      },
      "id": "qCuFmKkl_cGX",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_0_auc = 0.66837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2876\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.66998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3150\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.66163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2884\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.67923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3019\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.64566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2837\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_val_0_auc = 0.68462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3050\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.68541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3023\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.6622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2968\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.67342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2740\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.65625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2975\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.6679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2740\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.66148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2975\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 45 and best_val_0_auc = 0.67663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3149\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.66547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3057\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.66934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2953\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.67862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3051\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.65478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2846\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_auc = 0.65988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2853\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_auc = 0.64815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2763\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.66246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2994\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.67061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2884\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.66886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3076\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_0_auc = 0.67587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3079\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.65755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2798\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.67429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3029\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.68315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2898\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_val_0_auc = 0.6778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2957\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.67236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2945\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.66264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3131\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.66252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3009\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.6576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2995\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.66553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2892\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.6691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2980\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.68331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3157\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.66076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2882\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 94 with best_epoch = 74 and best_val_0_auc = 0.68313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3002\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.66078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2936\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.66571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2941\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.66773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2887\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.67305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3101\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 92 and best_val_0_auc = 0.6885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2963\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.67604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3057\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.67691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3120\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.66282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2981\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.66722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2793\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.6701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3016\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.66652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2731\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.66389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2910\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 91 with best_epoch = 71 and best_val_0_auc = 0.67666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3010\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.6677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2745\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.68346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2990\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.66728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2987\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_auc = 0.67862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3026\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.67695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2937\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.65799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2837\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.64844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2936\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.65927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3009\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.66293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3058\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.68709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3046\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 66 and best_val_0_auc = 0.6743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2915\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.66529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2969\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.66849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2910\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.66137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2832\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.66506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3020\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.64167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2914\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.6656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3086\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.67638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3159\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.66806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3212\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.64923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2803\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.66115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2947\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.66523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3025\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.66648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2801\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.66491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2738\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.66845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2918\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.66627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2960\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.66983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3058\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.65108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2856\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_0_auc = 0.67627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2992\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.67913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2903\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.66485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2942\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.66531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2997\\n\n",
            "Best Parameters: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "Best F1: 0.3212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5},\n",
              " {'Accuracy': 0.6439140811455847,\n",
              "  'Precision': 0.21458966565349544,\n",
              "  'Recall': 0.6383363471971067,\n",
              "  'F1': 0.32120109190172885,\n",
              "  'ROC_AUC': np.float64(0.6745504437265973),\n",
              "  'Runtime_s': 33.384838581085205})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tabnet(model, X_train, y_train, X_test, y_test):\n",
        "    start = time.time()\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    y_train_1d = y_train.astype(np.int64)\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "        X_train, y_train_1d, test_size=0.2, stratify=y_train_1d, random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=['auc'],\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=1024,\n",
        "    )\n",
        "\n",
        "    prob = model.predict_proba(X_test)[:, 1]\n",
        "    pred = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, pred, prob, time.time()-start)"
      ],
      "metadata": {
        "id": "aYg26DbnFhfa"
      },
      "id": "aYg26DbnFhfa",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE, embeddings"
      ],
      "metadata": {
        "id": "cr0PXQk2FPC2"
      },
      "id": "cr0PXQk2FPC2"
    },
    {
      "cell_type": "code",
      "source": [
        "manual_grid_search(tabnet_fn, tabnet_grid, train_tabnet,\n",
        "                   X_train, y_train, X_test, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk0ssM_jFPMV",
        "outputId": "323852a8-d477-4d97-d2fd-34baafea8a31"
      },
      "id": "jk0ssM_jFPMV",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.67357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0436\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.66189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0451\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.64159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.64503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0106\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.61694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.63679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.65104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.61076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.5952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.63671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0211\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.66413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0670\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.67095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0404\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 99 with best_epoch = 79 and best_val_0_auc = 0.66882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0344\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.60209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.59918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.6292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.61237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.58591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.66433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0106\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_0_auc = 0.66281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1184\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.66917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0661\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_0_auc = 0.66528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.63905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.58208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.62015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.61998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.61788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0108\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_0_auc = 0.67395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0409\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.67829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.66191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1392\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.62943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.65117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.62231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_val_0_auc = 0.67563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0143\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 78 with best_epoch = 58 and best_val_0_auc = 0.6334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.64194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.64711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.66053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_val_0_auc = 0.65815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1023\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.66045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.6204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.63462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_0_auc = 0.66417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.60335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_0_auc = 0.60663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.65758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.65978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0071\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.67832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.66909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_0_auc = 0.63108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.62145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_val_0_auc = 0.66342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.62979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.65345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.68084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.67154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0374\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.65765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.67116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.63206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.63447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.64271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_0_auc = 0.61904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.60681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.67222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0498\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.65701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0393\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.65092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.66725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.64169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.63395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.6367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_0_auc = 0.6213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.60822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.62523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0426\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.67145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0107\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.66099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0381\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.6711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.63569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0072\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_0_auc = 0.61786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_0_auc = 0.60385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_0_auc = 0.64198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.58342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Best Parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "Best F1: 0.1392\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0},\n",
              " {'Accuracy': 0.8553699284009546,\n",
              "  'Precision': 0.32450331125827814,\n",
              "  'Recall': 0.08860759493670886,\n",
              "  'F1': 0.13920454545454544,\n",
              "  'ROC_AUC': np.float64(0.6259247308032125),\n",
              "  'Runtime_s': 32.45985412597656})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE, no embeddings"
      ],
      "metadata": {
        "id": "KyggUqC0FPY1"
      },
      "id": "KyggUqC0FPY1"
    },
    {
      "cell_type": "code",
      "source": [
        "manual_grid_search(tabnet_fn, tabnet_grid, train_tabnet,\n",
        "                   X_train_structured, y_train, X_test_structured, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYMjxdzpFPid",
        "outputId": "3675c7c5-d357-4ba1-eb8f-6e1c2ce8a206"
      },
      "id": "VYMjxdzpFPid",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.67089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0071\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.6524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.66431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.68321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.67238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.65866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_val_0_auc = 0.68437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0072\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.66064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0106\\n\n",
            "Testing: {'n_d': 16, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.66478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_0_auc = 0.66798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0999\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.66255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.66867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0072\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.6694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 81 and best_val_0_auc = 0.67876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.65792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.67561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.66875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.66577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.65851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0280\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.65454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0276\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.66334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.66505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.66724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.6538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.66499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.65898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 16, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_0_auc = 0.66923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0108\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.68143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0475\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.65994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.67628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0606\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.66656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0072\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_val_0_auc = 0.66919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.64306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.67423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0105\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_0_auc = 0.68745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.66231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0072\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.67033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0107\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.67329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0401\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.67413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_auc = 0.66106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0108\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.66399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.65347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.68249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0211\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.66545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_val_0_auc = 0.678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.6703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0176\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.67447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.65984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0072\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.67017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.66897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.66864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_0_auc = 0.68994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.66783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 32, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.67222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0141\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.64951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0594\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.66153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0247\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.65899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_0_auc = 0.68111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0142\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.68115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0108\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.65863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.68441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0143\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.66652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 16, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.66257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.65963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0106\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.66494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0469\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.68794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0311\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.6789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.65443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0141\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_0_auc = 0.67435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0106\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.67182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.65925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 32, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.65544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0345\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.65753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0243\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.66003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0175\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 3, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.66585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 76 with best_epoch = 56 and best_val_0_auc = 0.68049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.6606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 54 and best_val_0_auc = 0.66601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0036\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.0}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.68492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0143\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 1.5}\n",
            "\n",
            "Early stopping occurred at epoch 93 with best_epoch = 73 and best_val_0_auc = 0.69421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Testing: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0}\n",
            "\n",
            "Early stopping occurred at epoch 76 with best_epoch = 56 and best_val_0_auc = 0.66675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0000\\n\n",
            "Best Parameters: {'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0}\n",
            "Best F1: 0.0999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'n_d': 16, 'n_a': 32, 'n_steps': 3, 'gamma': 1.0},\n",
              " {'Accuracy': 0.8494033412887828,\n",
              "  'Precision': 0.23648648648648649,\n",
              "  'Recall': 0.06329113924050633,\n",
              "  'F1': 0.09985734664764621,\n",
              "  'ROC_AUC': np.float64(0.6458905631839925),\n",
              "  'Runtime_s': 27.727808713912964})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FT Transformer"
      ],
      "metadata": {
        "id": "hl9N_zIrSVjr"
      },
      "id": "hl9N_zIrSVjr"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_torch_model(model, train_dl, X_test, y_test, criterion=None, optimizer=None, n_epochs=30, lr=3e-4, verbose=True):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    if criterion is None:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    if optimizer is None:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    for ep in range(n_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device).float().unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if verbose:\n",
        "            print(f\"Epoch {ep+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_te_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "        logits = model(X_te_tensor)\n",
        "        prob = torch.sigmoid(logits).cpu().numpy().ravel()\n",
        "\n",
        "    runtime = time.time() - start_time\n",
        "    preds = (prob >= 0.5).astype(int)\n",
        "    metrics = metric_dict(y_test, preds, prob, runtime)\n",
        "    return model, metrics\n",
        "\n",
        "def train_ft(model, X_train, y_train, X_test, y_test):\n",
        "    import torch\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "    pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()], dtype=torch.float32)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "    X_tr = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_tr = torch.tensor(y_train, dtype=torch.float32)\n",
        "    train_dl = DataLoader(TensorDataset(X_tr, y_tr), batch_size=256, shuffle=True)\n",
        "\n",
        "    return train_and_evaluate_torch_model(model, train_dl, X_test, y_test, criterion=criterion, n_epochs=30)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def __init__(self, n_feat, d):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.randn(n_feat, d) * 0.02)\n",
        "        self.b = nn.Parameter(torch.zeros(n_feat, d))\n",
        "        self.cls = nn.Parameter(torch.zeros(1, 1, d))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        tok = x.unsqueeze(-1) * self.w + self.b\n",
        "        cls = self.cls.expand(B, -1, -1)\n",
        "        return torch.cat([cls, tok], 1)\n",
        "\n",
        "def make_encoder(d, heads, ff, layers):\n",
        "    enc = nn.TransformerEncoderLayer(d_model=d, nhead=heads, dim_feedforward=ff,\n",
        "                                     dropout=0.1, batch_first=True, activation='gelu')\n",
        "    return nn.TransformerEncoder(enc, num_layers=layers)\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, n_feat, d=32, heads=4, layers=2, ff=64):\n",
        "        super().__init__()\n",
        "        self.tok = FeatureTokenizer(n_feat, d)\n",
        "        self.enc = make_encoder(d, heads, ff, layers)\n",
        "        self.head = nn.Sequential(nn.LayerNorm(d), nn.Linear(d, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.enc(self.tok(x))\n",
        "        return self.head(h[:, 0, :])\n",
        "\n"
      ],
      "metadata": {
        "id": "AXVicrWJ2sl-"
      },
      "id": "AXVicrWJ2sl-",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE with embeddings"
      ],
      "metadata": {
        "id": "wF0dZ9fEHik6"
      },
      "id": "wF0dZ9fEHik6"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ft_grid = [\n",
        "    {'d': d, 'heads': h, 'layers': l, 'ff': 64}\n",
        "    for d in [32, 64]\n",
        "    for h in [2, 4]\n",
        "    for l in [1, 2]\n",
        "]\n",
        "\n",
        "\n",
        "ft_fn = lambda d, heads, layers, ff: FTTransformer(\n",
        "    n_feat=X_train.shape[1], d=d, heads=heads, layers=layers, ff=ff\n",
        ")\n",
        "\n",
        "manual_grid_search(ft_fn, ft_grid, train_ft,\n",
        "                   X_train, y_train, X_test, y_test,\n",
        "                   score_key='F1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERVkWphtSD-Z",
        "outputId": "239037dd-9e4d-417b-fa01-c38513ea14d9"
      },
      "id": "ERVkWphtSD-Z",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'d': 32, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.3392\n",
            "Epoch 2/30, Loss: 1.1897\n",
            "Epoch 3/30, Loss: 1.1296\n",
            "Epoch 4/30, Loss: 1.3980\n",
            "Epoch 5/30, Loss: 1.2399\n",
            "Epoch 6/30, Loss: 1.1571\n",
            "Epoch 7/30, Loss: 1.0145\n",
            "Epoch 8/30, Loss: 1.2721\n",
            "Epoch 9/30, Loss: 0.9868\n",
            "Epoch 10/30, Loss: 0.9622\n",
            "Epoch 11/30, Loss: 1.0901\n",
            "Epoch 12/30, Loss: 0.8465\n",
            "Epoch 13/30, Loss: 1.0054\n",
            "Epoch 14/30, Loss: 1.1165\n",
            "Epoch 15/30, Loss: 1.1860\n",
            "Epoch 16/30, Loss: 1.1128\n",
            "Epoch 17/30, Loss: 1.1432\n",
            "Epoch 18/30, Loss: 0.9380\n",
            "Epoch 19/30, Loss: 1.1214\n",
            "Epoch 20/30, Loss: 1.3903\n",
            "Epoch 21/30, Loss: 1.2046\n",
            "Epoch 22/30, Loss: 0.8796\n",
            "Epoch 23/30, Loss: 1.0171\n",
            "Epoch 24/30, Loss: 0.9299\n",
            "Epoch 25/30, Loss: 0.8286\n",
            "Epoch 26/30, Loss: 0.9199\n",
            "Epoch 27/30, Loss: 1.0266\n",
            "Epoch 28/30, Loss: 0.9659\n",
            "Epoch 29/30, Loss: 1.0466\n",
            "Epoch 30/30, Loss: 1.2263\n",
            "F1: 0.3183\\n\n",
            "Testing: {'d': 32, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.4527\n",
            "Epoch 2/30, Loss: 1.1950\n",
            "Epoch 3/30, Loss: 1.1985\n",
            "Epoch 4/30, Loss: 1.0629\n",
            "Epoch 5/30, Loss: 1.0977\n",
            "Epoch 6/30, Loss: 1.1812\n",
            "Epoch 7/30, Loss: 1.1617\n",
            "Epoch 8/30, Loss: 1.0085\n",
            "Epoch 9/30, Loss: 1.0569\n",
            "Epoch 10/30, Loss: 1.1030\n",
            "Epoch 11/30, Loss: 1.3224\n",
            "Epoch 12/30, Loss: 1.0602\n",
            "Epoch 13/30, Loss: 0.8971\n",
            "Epoch 14/30, Loss: 1.1482\n",
            "Epoch 15/30, Loss: 1.2553\n",
            "Epoch 16/30, Loss: 1.1025\n",
            "Epoch 17/30, Loss: 1.0364\n",
            "Epoch 18/30, Loss: 1.3100\n",
            "Epoch 19/30, Loss: 1.2644\n",
            "Epoch 20/30, Loss: 1.3544\n",
            "Epoch 21/30, Loss: 0.9778\n",
            "Epoch 22/30, Loss: 1.1550\n",
            "Epoch 23/30, Loss: 0.9559\n",
            "Epoch 24/30, Loss: 1.2351\n",
            "Epoch 25/30, Loss: 1.2088\n",
            "Epoch 26/30, Loss: 1.2124\n",
            "Epoch 27/30, Loss: 0.9102\n",
            "Epoch 28/30, Loss: 1.1050\n",
            "Epoch 29/30, Loss: 1.0882\n",
            "Epoch 30/30, Loss: 0.8106\n",
            "F1: 0.3282\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.9998\n",
            "Epoch 2/30, Loss: 1.2241\n",
            "Epoch 3/30, Loss: 1.1224\n",
            "Epoch 4/30, Loss: 1.1705\n",
            "Epoch 5/30, Loss: 1.0855\n",
            "Epoch 6/30, Loss: 1.2862\n",
            "Epoch 7/30, Loss: 0.9707\n",
            "Epoch 8/30, Loss: 1.3989\n",
            "Epoch 9/30, Loss: 1.1755\n",
            "Epoch 10/30, Loss: 1.0674\n",
            "Epoch 11/30, Loss: 1.0498\n",
            "Epoch 12/30, Loss: 1.1012\n",
            "Epoch 13/30, Loss: 1.1464\n",
            "Epoch 14/30, Loss: 1.1187\n",
            "Epoch 15/30, Loss: 1.0269\n",
            "Epoch 16/30, Loss: 1.1679\n",
            "Epoch 17/30, Loss: 1.2584\n",
            "Epoch 18/30, Loss: 1.1329\n",
            "Epoch 19/30, Loss: 1.2479\n",
            "Epoch 20/30, Loss: 0.9846\n",
            "Epoch 21/30, Loss: 1.0819\n",
            "Epoch 22/30, Loss: 1.2539\n",
            "Epoch 23/30, Loss: 1.0606\n",
            "Epoch 24/30, Loss: 1.1741\n",
            "Epoch 25/30, Loss: 0.9080\n",
            "Epoch 26/30, Loss: 0.8523\n",
            "Epoch 27/30, Loss: 1.2659\n",
            "Epoch 28/30, Loss: 1.2442\n",
            "Epoch 29/30, Loss: 0.9802\n",
            "Epoch 30/30, Loss: 1.1127\n",
            "F1: 0.3272\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.3582\n",
            "Epoch 2/30, Loss: 1.0782\n",
            "Epoch 3/30, Loss: 1.0561\n",
            "Epoch 4/30, Loss: 1.3644\n",
            "Epoch 5/30, Loss: 1.4866\n",
            "Epoch 6/30, Loss: 1.1848\n",
            "Epoch 7/30, Loss: 1.4014\n",
            "Epoch 8/30, Loss: 1.1941\n",
            "Epoch 9/30, Loss: 0.9355\n",
            "Epoch 10/30, Loss: 1.1220\n",
            "Epoch 11/30, Loss: 1.1668\n",
            "Epoch 12/30, Loss: 1.0491\n",
            "Epoch 13/30, Loss: 1.1303\n",
            "Epoch 14/30, Loss: 1.2304\n",
            "Epoch 15/30, Loss: 1.1372\n",
            "Epoch 16/30, Loss: 0.9591\n",
            "Epoch 17/30, Loss: 0.8887\n",
            "Epoch 18/30, Loss: 0.9688\n",
            "Epoch 19/30, Loss: 0.9331\n",
            "Epoch 20/30, Loss: 0.7685\n",
            "Epoch 21/30, Loss: 1.2647\n",
            "Epoch 22/30, Loss: 1.1983\n",
            "Epoch 23/30, Loss: 1.0049\n",
            "Epoch 24/30, Loss: 1.0594\n",
            "Epoch 25/30, Loss: 0.9923\n",
            "Epoch 26/30, Loss: 1.0085\n",
            "Epoch 27/30, Loss: 1.0891\n",
            "Epoch 28/30, Loss: 1.1772\n",
            "Epoch 29/30, Loss: 1.2700\n",
            "Epoch 30/30, Loss: 0.9989\n",
            "F1: 0.3235\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.9960\n",
            "Epoch 2/30, Loss: 1.2746\n",
            "Epoch 3/30, Loss: 1.1092\n",
            "Epoch 4/30, Loss: 1.0582\n",
            "Epoch 5/30, Loss: 1.3624\n",
            "Epoch 6/30, Loss: 1.2172\n",
            "Epoch 7/30, Loss: 0.9687\n",
            "Epoch 8/30, Loss: 1.1411\n",
            "Epoch 9/30, Loss: 1.1170\n",
            "Epoch 10/30, Loss: 0.9882\n",
            "Epoch 11/30, Loss: 0.9166\n",
            "Epoch 12/30, Loss: 0.9356\n",
            "Epoch 13/30, Loss: 1.2064\n",
            "Epoch 14/30, Loss: 1.0242\n",
            "Epoch 15/30, Loss: 1.2508\n",
            "Epoch 16/30, Loss: 1.1221\n",
            "Epoch 17/30, Loss: 1.2838\n",
            "Epoch 18/30, Loss: 1.0515\n",
            "Epoch 19/30, Loss: 1.1248\n",
            "Epoch 20/30, Loss: 1.1545\n",
            "Epoch 21/30, Loss: 1.0943\n",
            "Epoch 22/30, Loss: 1.1555\n",
            "Epoch 23/30, Loss: 0.8286\n",
            "Epoch 24/30, Loss: 0.9211\n",
            "Epoch 25/30, Loss: 0.9569\n",
            "Epoch 26/30, Loss: 1.0408\n",
            "Epoch 27/30, Loss: 1.3085\n",
            "Epoch 28/30, Loss: 1.0733\n",
            "Epoch 29/30, Loss: 0.8809\n",
            "Epoch 30/30, Loss: 1.2262\n",
            "F1: 0.3074\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.1070\n",
            "Epoch 2/30, Loss: 1.0508\n",
            "Epoch 3/30, Loss: 1.1210\n",
            "Epoch 4/30, Loss: 1.2391\n",
            "Epoch 5/30, Loss: 0.9645\n",
            "Epoch 6/30, Loss: 1.1521\n",
            "Epoch 7/30, Loss: 1.1276\n",
            "Epoch 8/30, Loss: 1.0912\n",
            "Epoch 9/30, Loss: 1.1237\n",
            "Epoch 10/30, Loss: 1.0863\n",
            "Epoch 11/30, Loss: 1.0555\n",
            "Epoch 12/30, Loss: 0.9312\n",
            "Epoch 13/30, Loss: 0.9265\n",
            "Epoch 14/30, Loss: 1.2985\n",
            "Epoch 15/30, Loss: 0.9441\n",
            "Epoch 16/30, Loss: 1.1010\n",
            "Epoch 17/30, Loss: 0.9107\n",
            "Epoch 18/30, Loss: 0.9234\n",
            "Epoch 19/30, Loss: 1.0823\n",
            "Epoch 20/30, Loss: 1.0441\n",
            "Epoch 21/30, Loss: 1.1502\n",
            "Epoch 22/30, Loss: 0.9254\n",
            "Epoch 23/30, Loss: 1.1766\n",
            "Epoch 24/30, Loss: 1.1111\n",
            "Epoch 25/30, Loss: 1.1196\n",
            "Epoch 26/30, Loss: 0.8667\n",
            "Epoch 27/30, Loss: 1.4732\n",
            "Epoch 28/30, Loss: 1.1321\n",
            "Epoch 29/30, Loss: 1.1047\n",
            "Epoch 30/30, Loss: 1.0389\n",
            "F1: 0.3327\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.9440\n",
            "Epoch 2/30, Loss: 1.3714\n",
            "Epoch 3/30, Loss: 1.1024\n",
            "Epoch 4/30, Loss: 1.1719\n",
            "Epoch 5/30, Loss: 1.2086\n",
            "Epoch 6/30, Loss: 1.0939\n",
            "Epoch 7/30, Loss: 1.1774\n",
            "Epoch 8/30, Loss: 1.2455\n",
            "Epoch 9/30, Loss: 1.0312\n",
            "Epoch 10/30, Loss: 1.1437\n",
            "Epoch 11/30, Loss: 1.1163\n",
            "Epoch 12/30, Loss: 1.1005\n",
            "Epoch 13/30, Loss: 1.1501\n",
            "Epoch 14/30, Loss: 1.0999\n",
            "Epoch 15/30, Loss: 1.2483\n",
            "Epoch 16/30, Loss: 1.2462\n",
            "Epoch 17/30, Loss: 1.1953\n",
            "Epoch 18/30, Loss: 0.9396\n",
            "Epoch 19/30, Loss: 0.9240\n",
            "Epoch 20/30, Loss: 0.8173\n",
            "Epoch 21/30, Loss: 0.9233\n",
            "Epoch 22/30, Loss: 1.2688\n",
            "Epoch 23/30, Loss: 1.0778\n",
            "Epoch 24/30, Loss: 1.0567\n",
            "Epoch 25/30, Loss: 0.9481\n",
            "Epoch 26/30, Loss: 1.1945\n",
            "Epoch 27/30, Loss: 1.1828\n",
            "Epoch 28/30, Loss: 1.2277\n",
            "Epoch 29/30, Loss: 1.2258\n",
            "Epoch 30/30, Loss: 1.0265\n",
            "F1: 0.3064\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.3580\n",
            "Epoch 2/30, Loss: 1.1921\n",
            "Epoch 3/30, Loss: 1.3111\n",
            "Epoch 4/30, Loss: 1.1100\n",
            "Epoch 5/30, Loss: 1.1840\n",
            "Epoch 6/30, Loss: 1.1686\n",
            "Epoch 7/30, Loss: 1.1244\n",
            "Epoch 8/30, Loss: 1.0502\n",
            "Epoch 9/30, Loss: 1.0265\n",
            "Epoch 10/30, Loss: 0.9485\n",
            "Epoch 11/30, Loss: 1.0009\n",
            "Epoch 12/30, Loss: 1.1944\n",
            "Epoch 13/30, Loss: 1.3286\n",
            "Epoch 14/30, Loss: 1.1245\n",
            "Epoch 15/30, Loss: 0.9769\n",
            "Epoch 16/30, Loss: 1.0644\n",
            "Epoch 17/30, Loss: 0.9388\n",
            "Epoch 18/30, Loss: 1.1433\n",
            "Epoch 19/30, Loss: 1.4065\n",
            "Epoch 20/30, Loss: 1.4064\n",
            "Epoch 21/30, Loss: 1.0682\n",
            "Epoch 22/30, Loss: 1.0481\n",
            "Epoch 23/30, Loss: 0.9892\n",
            "Epoch 24/30, Loss: 1.0325\n",
            "Epoch 25/30, Loss: 1.2258\n",
            "Epoch 26/30, Loss: 0.9936\n",
            "Epoch 27/30, Loss: 1.2647\n",
            "Epoch 28/30, Loss: 0.8013\n",
            "Epoch 29/30, Loss: 1.0454\n",
            "Epoch 30/30, Loss: 1.2139\n",
            "F1: 0.3312\\n\n",
            "Best Parameters: {'d': 64, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Best F1: 0.3327\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'d': 64, 'heads': 2, 'layers': 2, 'ff': 64},\n",
              " {'Accuracy': 0.6381861575178998,\n",
              "  'Precision': 0.2198952879581152,\n",
              "  'Recall': 0.6835443037974683,\n",
              "  'F1': 0.33274647887323944,\n",
              "  'ROC_AUC': np.float64(0.7084873619087727),\n",
              "  'Runtime_s': 321.2794826030731})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE with no embeddings"
      ],
      "metadata": {
        "id": "y_O3pBEHHlu8"
      },
      "id": "y_O3pBEHHlu8"
    },
    {
      "cell_type": "code",
      "source": [
        "ft_fn = lambda d, heads, layers, ff: FTTransformer(\n",
        "    n_feat=X_train_structured.shape[1], d=d, heads=heads, layers=layers, ff=ff\n",
        ")\n",
        "\n",
        "manual_grid_search(ft_fn, ft_grid, train_ft,\n",
        "                   X_train_structured, y_train, X_test_structured, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV7noR2K_hjM",
        "outputId": "fc1137bf-d49e-4f6c-cab8-e27c73d1bb92"
      },
      "id": "mV7noR2K_hjM",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'d': 32, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.1991\n",
            "Epoch 2/30, Loss: 0.8340\n",
            "Epoch 3/30, Loss: 1.3992\n",
            "Epoch 4/30, Loss: 0.9532\n",
            "Epoch 5/30, Loss: 1.1528\n",
            "Epoch 6/30, Loss: 0.9582\n",
            "Epoch 7/30, Loss: 1.6137\n",
            "Epoch 8/30, Loss: 0.9077\n",
            "Epoch 9/30, Loss: 1.1395\n",
            "Epoch 10/30, Loss: 1.2135\n",
            "Epoch 11/30, Loss: 0.8906\n",
            "Epoch 12/30, Loss: 1.3362\n",
            "Epoch 13/30, Loss: 1.1875\n",
            "Epoch 14/30, Loss: 1.1487\n",
            "Epoch 15/30, Loss: 1.1922\n",
            "Epoch 16/30, Loss: 1.2288\n",
            "Epoch 17/30, Loss: 0.9584\n",
            "Epoch 18/30, Loss: 1.0934\n",
            "Epoch 19/30, Loss: 0.9449\n",
            "Epoch 20/30, Loss: 1.0557\n",
            "Epoch 21/30, Loss: 1.1899\n",
            "Epoch 22/30, Loss: 1.0070\n",
            "Epoch 23/30, Loss: 1.0888\n",
            "Epoch 24/30, Loss: 0.9368\n",
            "Epoch 25/30, Loss: 1.0466\n",
            "Epoch 26/30, Loss: 1.1823\n",
            "Epoch 27/30, Loss: 1.1365\n",
            "Epoch 28/30, Loss: 1.0321\n",
            "Epoch 29/30, Loss: 0.9935\n",
            "Epoch 30/30, Loss: 1.0128\n",
            "F1: 0.3181\\n\n",
            "Testing: {'d': 32, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.1831\n",
            "Epoch 2/30, Loss: 1.3080\n",
            "Epoch 3/30, Loss: 0.9642\n",
            "Epoch 4/30, Loss: 1.3410\n",
            "Epoch 5/30, Loss: 1.3343\n",
            "Epoch 6/30, Loss: 1.0475\n",
            "Epoch 7/30, Loss: 1.0769\n",
            "Epoch 8/30, Loss: 0.9928\n",
            "Epoch 9/30, Loss: 0.9899\n",
            "Epoch 10/30, Loss: 1.1490\n",
            "Epoch 11/30, Loss: 1.1642\n",
            "Epoch 12/30, Loss: 1.0768\n",
            "Epoch 13/30, Loss: 0.9815\n",
            "Epoch 14/30, Loss: 0.9250\n",
            "Epoch 15/30, Loss: 1.1457\n",
            "Epoch 16/30, Loss: 1.1119\n",
            "Epoch 17/30, Loss: 1.1635\n",
            "Epoch 18/30, Loss: 0.9638\n",
            "Epoch 19/30, Loss: 1.0325\n",
            "Epoch 20/30, Loss: 1.2946\n",
            "Epoch 21/30, Loss: 0.8937\n",
            "Epoch 22/30, Loss: 1.1573\n",
            "Epoch 23/30, Loss: 1.1582\n",
            "Epoch 24/30, Loss: 1.0730\n",
            "Epoch 25/30, Loss: 1.0822\n",
            "Epoch 26/30, Loss: 0.9432\n",
            "Epoch 27/30, Loss: 0.9667\n",
            "Epoch 28/30, Loss: 0.9594\n",
            "Epoch 29/30, Loss: 1.1776\n",
            "Epoch 30/30, Loss: 0.9610\n",
            "F1: 0.3200\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.9827\n",
            "Epoch 2/30, Loss: 1.1321\n",
            "Epoch 3/30, Loss: 1.1690\n",
            "Epoch 4/30, Loss: 1.0069\n",
            "Epoch 5/30, Loss: 0.9450\n",
            "Epoch 6/30, Loss: 0.9122\n",
            "Epoch 7/30, Loss: 1.1062\n",
            "Epoch 8/30, Loss: 1.0933\n",
            "Epoch 9/30, Loss: 1.1512\n",
            "Epoch 10/30, Loss: 1.1361\n",
            "Epoch 11/30, Loss: 1.1956\n",
            "Epoch 12/30, Loss: 1.1373\n",
            "Epoch 13/30, Loss: 1.0152\n",
            "Epoch 14/30, Loss: 0.9221\n",
            "Epoch 15/30, Loss: 1.2879\n",
            "Epoch 16/30, Loss: 1.2670\n",
            "Epoch 17/30, Loss: 1.1275\n",
            "Epoch 18/30, Loss: 1.0579\n",
            "Epoch 19/30, Loss: 1.0174\n",
            "Epoch 20/30, Loss: 1.2195\n",
            "Epoch 21/30, Loss: 1.0256\n",
            "Epoch 22/30, Loss: 1.1281\n",
            "Epoch 23/30, Loss: 0.7787\n",
            "Epoch 24/30, Loss: 1.0186\n",
            "Epoch 25/30, Loss: 0.8782\n",
            "Epoch 26/30, Loss: 1.0446\n",
            "Epoch 27/30, Loss: 0.9053\n",
            "Epoch 28/30, Loss: 1.1032\n",
            "Epoch 29/30, Loss: 1.0020\n",
            "Epoch 30/30, Loss: 1.0637\n",
            "F1: 0.3067\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.0156\n",
            "Epoch 2/30, Loss: 1.0216\n",
            "Epoch 3/30, Loss: 0.9953\n",
            "Epoch 4/30, Loss: 1.1267\n",
            "Epoch 5/30, Loss: 1.0401\n",
            "Epoch 6/30, Loss: 1.1333\n",
            "Epoch 7/30, Loss: 1.3241\n",
            "Epoch 8/30, Loss: 1.2329\n",
            "Epoch 9/30, Loss: 1.0546\n",
            "Epoch 10/30, Loss: 1.2503\n",
            "Epoch 11/30, Loss: 0.8925\n",
            "Epoch 12/30, Loss: 0.9793\n",
            "Epoch 13/30, Loss: 1.1621\n",
            "Epoch 14/30, Loss: 1.0681\n",
            "Epoch 15/30, Loss: 1.0129\n",
            "Epoch 16/30, Loss: 1.2127\n",
            "Epoch 17/30, Loss: 1.4469\n",
            "Epoch 18/30, Loss: 1.2237\n",
            "Epoch 19/30, Loss: 0.9078\n",
            "Epoch 20/30, Loss: 1.0512\n",
            "Epoch 21/30, Loss: 1.0695\n",
            "Epoch 22/30, Loss: 0.8514\n",
            "Epoch 23/30, Loss: 1.1616\n",
            "Epoch 24/30, Loss: 1.0218\n",
            "Epoch 25/30, Loss: 1.2500\n",
            "Epoch 26/30, Loss: 1.4118\n",
            "Epoch 27/30, Loss: 0.9136\n",
            "Epoch 28/30, Loss: 1.0377\n",
            "Epoch 29/30, Loss: 1.0422\n",
            "Epoch 30/30, Loss: 1.1393\n",
            "F1: 0.3100\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.1536\n",
            "Epoch 2/30, Loss: 1.0438\n",
            "Epoch 3/30, Loss: 1.1630\n",
            "Epoch 4/30, Loss: 1.1112\n",
            "Epoch 5/30, Loss: 1.2415\n",
            "Epoch 6/30, Loss: 1.2016\n",
            "Epoch 7/30, Loss: 1.1195\n",
            "Epoch 8/30, Loss: 1.2279\n",
            "Epoch 9/30, Loss: 1.0646\n",
            "Epoch 10/30, Loss: 1.0895\n",
            "Epoch 11/30, Loss: 1.0512\n",
            "Epoch 12/30, Loss: 1.1746\n",
            "Epoch 13/30, Loss: 1.1000\n",
            "Epoch 14/30, Loss: 1.2252\n",
            "Epoch 15/30, Loss: 1.3145\n",
            "Epoch 16/30, Loss: 1.0553\n",
            "Epoch 17/30, Loss: 0.9970\n",
            "Epoch 18/30, Loss: 1.1733\n",
            "Epoch 19/30, Loss: 0.9768\n",
            "Epoch 20/30, Loss: 1.1107\n",
            "Epoch 21/30, Loss: 1.1841\n",
            "Epoch 22/30, Loss: 0.9885\n",
            "Epoch 23/30, Loss: 1.0651\n",
            "Epoch 24/30, Loss: 1.2024\n",
            "Epoch 25/30, Loss: 1.0856\n",
            "Epoch 26/30, Loss: 0.9233\n",
            "Epoch 27/30, Loss: 0.9110\n",
            "Epoch 28/30, Loss: 0.9203\n",
            "Epoch 29/30, Loss: 1.2285\n",
            "Epoch 30/30, Loss: 1.0407\n",
            "F1: 0.3155\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.9633\n",
            "Epoch 2/30, Loss: 0.9708\n",
            "Epoch 3/30, Loss: 1.1268\n",
            "Epoch 4/30, Loss: 1.1043\n",
            "Epoch 5/30, Loss: 1.1815\n",
            "Epoch 6/30, Loss: 1.3981\n",
            "Epoch 7/30, Loss: 1.1020\n",
            "Epoch 8/30, Loss: 1.1219\n",
            "Epoch 9/30, Loss: 1.3863\n",
            "Epoch 10/30, Loss: 1.0448\n",
            "Epoch 11/30, Loss: 1.1824\n",
            "Epoch 12/30, Loss: 1.1537\n",
            "Epoch 13/30, Loss: 1.0119\n",
            "Epoch 14/30, Loss: 0.9925\n",
            "Epoch 15/30, Loss: 1.0200\n",
            "Epoch 16/30, Loss: 1.0316\n",
            "Epoch 17/30, Loss: 0.9402\n",
            "Epoch 18/30, Loss: 1.0866\n",
            "Epoch 19/30, Loss: 0.8985\n",
            "Epoch 20/30, Loss: 0.9484\n",
            "Epoch 21/30, Loss: 0.9038\n",
            "Epoch 22/30, Loss: 1.0735\n",
            "Epoch 23/30, Loss: 1.1618\n",
            "Epoch 24/30, Loss: 0.9434\n",
            "Epoch 25/30, Loss: 1.0090\n",
            "Epoch 26/30, Loss: 0.8876\n",
            "Epoch 27/30, Loss: 0.9447\n",
            "Epoch 28/30, Loss: 0.9978\n",
            "Epoch 29/30, Loss: 1.2581\n",
            "Epoch 30/30, Loss: 0.9450\n",
            "F1: 0.3251\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 1.1184\n",
            "Epoch 2/30, Loss: 1.4135\n",
            "Epoch 3/30, Loss: 1.3220\n",
            "Epoch 4/30, Loss: 0.8313\n",
            "Epoch 5/30, Loss: 1.1220\n",
            "Epoch 6/30, Loss: 0.9241\n",
            "Epoch 7/30, Loss: 1.1474\n",
            "Epoch 8/30, Loss: 1.2501\n",
            "Epoch 9/30, Loss: 0.8974\n",
            "Epoch 10/30, Loss: 0.9780\n",
            "Epoch 11/30, Loss: 1.1149\n",
            "Epoch 12/30, Loss: 1.1454\n",
            "Epoch 13/30, Loss: 1.2955\n",
            "Epoch 14/30, Loss: 0.9599\n",
            "Epoch 15/30, Loss: 0.8309\n",
            "Epoch 16/30, Loss: 1.1538\n",
            "Epoch 17/30, Loss: 0.9724\n",
            "Epoch 18/30, Loss: 1.0687\n",
            "Epoch 19/30, Loss: 1.0719\n",
            "Epoch 20/30, Loss: 1.2810\n",
            "Epoch 21/30, Loss: 1.1536\n",
            "Epoch 22/30, Loss: 1.3628\n",
            "Epoch 23/30, Loss: 1.1280\n",
            "Epoch 24/30, Loss: 1.0304\n",
            "Epoch 25/30, Loss: 1.1827\n",
            "Epoch 26/30, Loss: 1.0288\n",
            "Epoch 27/30, Loss: 1.0825\n",
            "Epoch 28/30, Loss: 1.1550\n",
            "Epoch 29/30, Loss: 1.3510\n",
            "Epoch 30/30, Loss: 1.1807\n",
            "F1: 0.3185\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.8936\n",
            "Epoch 2/30, Loss: 1.0823\n",
            "Epoch 3/30, Loss: 1.3152\n",
            "Epoch 4/30, Loss: 1.1761\n",
            "Epoch 5/30, Loss: 1.0604\n",
            "Epoch 6/30, Loss: 1.2637\n",
            "Epoch 7/30, Loss: 0.8615\n",
            "Epoch 8/30, Loss: 0.9087\n",
            "Epoch 9/30, Loss: 0.9903\n",
            "Epoch 10/30, Loss: 1.1489\n",
            "Epoch 11/30, Loss: 0.9714\n",
            "Epoch 12/30, Loss: 1.2061\n",
            "Epoch 13/30, Loss: 1.0801\n",
            "Epoch 14/30, Loss: 1.0136\n",
            "Epoch 15/30, Loss: 1.2018\n",
            "Epoch 16/30, Loss: 1.1435\n",
            "Epoch 17/30, Loss: 1.2187\n",
            "Epoch 18/30, Loss: 1.1750\n",
            "Epoch 19/30, Loss: 1.1819\n",
            "Epoch 20/30, Loss: 1.0994\n",
            "Epoch 21/30, Loss: 1.2075\n",
            "Epoch 22/30, Loss: 1.1204\n",
            "Epoch 23/30, Loss: 1.1131\n",
            "Epoch 24/30, Loss: 1.2611\n",
            "Epoch 25/30, Loss: 1.1384\n",
            "Epoch 26/30, Loss: 1.1726\n",
            "Epoch 27/30, Loss: 0.9252\n",
            "Epoch 28/30, Loss: 0.9480\n",
            "Epoch 29/30, Loss: 1.0296\n",
            "Epoch 30/30, Loss: 1.0730\n",
            "F1: 0.3278\\n\n",
            "Best Parameters: {'d': 64, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Best F1: 0.3278\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'d': 64, 'heads': 4, 'layers': 2, 'ff': 64},\n",
              " {'Accuracy': 0.6417661097852029,\n",
              "  'Precision': 0.21785714285714286,\n",
              "  'Recall': 0.6618444846292948,\n",
              "  'F1': 0.3278101209135692,\n",
              "  'ROC_AUC': np.float64(0.6977637412548645),\n",
              "  'Runtime_s': 17.673937797546387})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ft_fn = lambda d, heads, layers, ff: FTTransformer(\n",
        "    n_feat=X_train.shape[1], d=d, heads=heads, layers=layers, ff=ff\n",
        ")\n",
        "\n",
        "def train_ft(model, X_train, y_train, X_test, y_test):\n",
        "    import torch\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    X_tr = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_tr = torch.tensor(y_train, dtype=torch.float32)\n",
        "    train_dl = DataLoader(TensorDataset(X_tr, y_tr), batch_size=256, shuffle=True)\n",
        "\n",
        "    return train_and_evaluate_torch_model(model, train_dl, X_test, y_test, criterion=criterion, n_epochs=30)\n"
      ],
      "metadata": {
        "id": "MVq6WSF3HoQG"
      },
      "id": "MVq6WSF3HoQG",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE with embeddings"
      ],
      "metadata": {
        "id": "Rt8IwtPwHnR9"
      },
      "id": "Rt8IwtPwHnR9"
    },
    {
      "cell_type": "code",
      "source": [
        "manual_grid_search(ft_fn, ft_grid, train_ft,\n",
        "                   X_train, y_train, X_test, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQuegrqMHnYq",
        "outputId": "122a5a24-5376-4e33-e9a1-470b6fc5b586"
      },
      "id": "IQuegrqMHnYq",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'d': 32, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.2505\n",
            "Epoch 2/30, Loss: 0.4437\n",
            "Epoch 3/30, Loss: 0.3182\n",
            "Epoch 4/30, Loss: 0.4402\n",
            "Epoch 5/30, Loss: 0.2820\n",
            "Epoch 6/30, Loss: 0.5107\n",
            "Epoch 7/30, Loss: 0.3629\n",
            "Epoch 8/30, Loss: 0.2635\n",
            "Epoch 9/30, Loss: 0.4359\n",
            "Epoch 10/30, Loss: 0.2801\n",
            "Epoch 11/30, Loss: 0.4232\n",
            "Epoch 12/30, Loss: 0.3813\n",
            "Epoch 13/30, Loss: 0.3886\n",
            "Epoch 14/30, Loss: 0.2815\n",
            "Epoch 15/30, Loss: 0.4969\n",
            "Epoch 16/30, Loss: 0.3769\n",
            "Epoch 17/30, Loss: 0.4268\n",
            "Epoch 18/30, Loss: 0.3936\n",
            "Epoch 19/30, Loss: 0.5024\n",
            "Epoch 20/30, Loss: 0.3773\n",
            "Epoch 21/30, Loss: 0.4361\n",
            "Epoch 22/30, Loss: 0.3984\n",
            "Epoch 23/30, Loss: 0.3459\n",
            "Epoch 24/30, Loss: 0.4358\n",
            "Epoch 25/30, Loss: 0.3728\n",
            "Epoch 26/30, Loss: 0.3392\n",
            "Epoch 27/30, Loss: 0.3108\n",
            "Epoch 28/30, Loss: 0.3890\n",
            "Epoch 29/30, Loss: 0.3041\n",
            "Epoch 30/30, Loss: 0.3159\n",
            "F1: 0.0000\\n\n",
            "Testing: {'d': 32, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.3987\n",
            "Epoch 2/30, Loss: 0.4532\n",
            "Epoch 3/30, Loss: 0.3642\n",
            "Epoch 4/30, Loss: 0.2930\n",
            "Epoch 5/30, Loss: 0.2764\n",
            "Epoch 6/30, Loss: 0.3101\n",
            "Epoch 7/30, Loss: 0.2464\n",
            "Epoch 8/30, Loss: 0.2805\n",
            "Epoch 9/30, Loss: 0.2470\n",
            "Epoch 10/30, Loss: 0.4327\n",
            "Epoch 11/30, Loss: 0.2737\n",
            "Epoch 12/30, Loss: 0.4054\n",
            "Epoch 13/30, Loss: 0.3009\n",
            "Epoch 14/30, Loss: 0.4002\n",
            "Epoch 15/30, Loss: 0.4218\n",
            "Epoch 16/30, Loss: 0.3458\n",
            "Epoch 17/30, Loss: 0.4385\n",
            "Epoch 18/30, Loss: 0.3380\n",
            "Epoch 19/30, Loss: 0.3694\n",
            "Epoch 20/30, Loss: 0.2898\n",
            "Epoch 21/30, Loss: 0.3168\n",
            "Epoch 22/30, Loss: 0.3255\n",
            "Epoch 23/30, Loss: 0.2874\n",
            "Epoch 24/30, Loss: 0.3143\n",
            "Epoch 25/30, Loss: 0.3210\n",
            "Epoch 26/30, Loss: 0.2850\n",
            "Epoch 27/30, Loss: 0.4241\n",
            "Epoch 28/30, Loss: 0.4156\n",
            "Epoch 29/30, Loss: 0.3714\n",
            "Epoch 30/30, Loss: 0.5116\n",
            "F1: 0.0215\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4610\n",
            "Epoch 2/30, Loss: 0.3834\n",
            "Epoch 3/30, Loss: 0.3159\n",
            "Epoch 4/30, Loss: 0.3801\n",
            "Epoch 5/30, Loss: 0.3402\n",
            "Epoch 6/30, Loss: 0.3561\n",
            "Epoch 7/30, Loss: 0.4253\n",
            "Epoch 8/30, Loss: 0.4541\n",
            "Epoch 9/30, Loss: 0.3632\n",
            "Epoch 10/30, Loss: 0.4002\n",
            "Epoch 11/30, Loss: 0.3647\n",
            "Epoch 12/30, Loss: 0.3080\n",
            "Epoch 13/30, Loss: 0.3253\n",
            "Epoch 14/30, Loss: 0.4202\n",
            "Epoch 15/30, Loss: 0.2683\n",
            "Epoch 16/30, Loss: 0.2482\n",
            "Epoch 17/30, Loss: 0.4152\n",
            "Epoch 18/30, Loss: 0.4419\n",
            "Epoch 19/30, Loss: 0.3590\n",
            "Epoch 20/30, Loss: 0.3833\n",
            "Epoch 21/30, Loss: 0.3872\n",
            "Epoch 22/30, Loss: 0.3096\n",
            "Epoch 23/30, Loss: 0.3531\n",
            "Epoch 24/30, Loss: 0.2773\n",
            "Epoch 25/30, Loss: 0.3370\n",
            "Epoch 26/30, Loss: 0.3814\n",
            "Epoch 27/30, Loss: 0.3744\n",
            "Epoch 28/30, Loss: 0.4057\n",
            "Epoch 29/30, Loss: 0.4735\n",
            "Epoch 30/30, Loss: 0.4994\n",
            "F1: 0.0000\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.3831\n",
            "Epoch 2/30, Loss: 0.3609\n",
            "Epoch 3/30, Loss: 0.3442\n",
            "Epoch 4/30, Loss: 0.3679\n",
            "Epoch 5/30, Loss: 0.3046\n",
            "Epoch 6/30, Loss: 0.2731\n",
            "Epoch 7/30, Loss: 0.3415\n",
            "Epoch 8/30, Loss: 0.4161\n",
            "Epoch 9/30, Loss: 0.3635\n",
            "Epoch 10/30, Loss: 0.3581\n",
            "Epoch 11/30, Loss: 0.3197\n",
            "Epoch 12/30, Loss: 0.2743\n",
            "Epoch 13/30, Loss: 0.3584\n",
            "Epoch 14/30, Loss: 0.3629\n",
            "Epoch 15/30, Loss: 0.3853\n",
            "Epoch 16/30, Loss: 0.4141\n",
            "Epoch 17/30, Loss: 0.4236\n",
            "Epoch 18/30, Loss: 0.3531\n",
            "Epoch 19/30, Loss: 0.3556\n",
            "Epoch 20/30, Loss: 0.3175\n",
            "Epoch 21/30, Loss: 0.2757\n",
            "Epoch 22/30, Loss: 0.3116\n",
            "Epoch 23/30, Loss: 0.3292\n",
            "Epoch 24/30, Loss: 0.3681\n",
            "Epoch 25/30, Loss: 0.3942\n",
            "Epoch 26/30, Loss: 0.2899\n",
            "Epoch 27/30, Loss: 0.3310\n",
            "Epoch 28/30, Loss: 0.3769\n",
            "Epoch 29/30, Loss: 0.2634\n",
            "Epoch 30/30, Loss: 0.3855\n",
            "F1: 0.0036\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.3321\n",
            "Epoch 2/30, Loss: 0.4001\n",
            "Epoch 3/30, Loss: 0.2695\n",
            "Epoch 4/30, Loss: 0.4067\n",
            "Epoch 5/30, Loss: 0.4327\n",
            "Epoch 6/30, Loss: 0.3171\n",
            "Epoch 7/30, Loss: 0.2766\n",
            "Epoch 8/30, Loss: 0.4213\n",
            "Epoch 9/30, Loss: 0.3069\n",
            "Epoch 10/30, Loss: 0.4245\n",
            "Epoch 11/30, Loss: 0.3889\n",
            "Epoch 12/30, Loss: 0.3617\n",
            "Epoch 13/30, Loss: 0.3923\n",
            "Epoch 14/30, Loss: 0.3254\n",
            "Epoch 15/30, Loss: 0.3678\n",
            "Epoch 16/30, Loss: 0.3720\n",
            "Epoch 17/30, Loss: 0.3410\n",
            "Epoch 18/30, Loss: 0.4371\n",
            "Epoch 19/30, Loss: 0.4149\n",
            "Epoch 20/30, Loss: 0.3522\n",
            "Epoch 21/30, Loss: 0.3446\n",
            "Epoch 22/30, Loss: 0.3220\n",
            "Epoch 23/30, Loss: 0.4315\n",
            "Epoch 24/30, Loss: 0.3087\n",
            "Epoch 25/30, Loss: 0.3809\n",
            "Epoch 26/30, Loss: 0.4537\n",
            "Epoch 27/30, Loss: 0.4509\n",
            "Epoch 28/30, Loss: 0.3535\n",
            "Epoch 29/30, Loss: 0.3998\n",
            "Epoch 30/30, Loss: 0.3109\n",
            "F1: 0.0000\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4280\n",
            "Epoch 2/30, Loss: 0.3569\n",
            "Epoch 3/30, Loss: 0.2887\n",
            "Epoch 4/30, Loss: 0.3215\n",
            "Epoch 5/30, Loss: 0.3418\n",
            "Epoch 6/30, Loss: 0.3673\n",
            "Epoch 7/30, Loss: 0.3362\n",
            "Epoch 8/30, Loss: 0.4363\n",
            "Epoch 9/30, Loss: 0.3782\n",
            "Epoch 10/30, Loss: 0.2541\n",
            "Epoch 11/30, Loss: 0.2905\n",
            "Epoch 12/30, Loss: 0.5052\n",
            "Epoch 13/30, Loss: 0.3259\n",
            "Epoch 14/30, Loss: 0.3172\n",
            "Epoch 15/30, Loss: 0.4199\n",
            "Epoch 16/30, Loss: 0.3348\n",
            "Epoch 17/30, Loss: 0.3470\n",
            "Epoch 18/30, Loss: 0.4105\n",
            "Epoch 19/30, Loss: 0.3494\n",
            "Epoch 20/30, Loss: 0.2726\n",
            "Epoch 21/30, Loss: 0.3716\n",
            "Epoch 22/30, Loss: 0.2538\n",
            "Epoch 23/30, Loss: 0.2998\n",
            "Epoch 24/30, Loss: 0.2956\n",
            "Epoch 25/30, Loss: 0.2408\n",
            "Epoch 26/30, Loss: 0.4018\n",
            "Epoch 27/30, Loss: 0.3384\n",
            "Epoch 28/30, Loss: 0.3668\n",
            "Epoch 29/30, Loss: 0.3607\n",
            "Epoch 30/30, Loss: 0.3222\n",
            "F1: 0.0383\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.2618\n",
            "Epoch 2/30, Loss: 0.4292\n",
            "Epoch 3/30, Loss: 0.3739\n",
            "Epoch 4/30, Loss: 0.3932\n",
            "Epoch 5/30, Loss: 0.3815\n",
            "Epoch 6/30, Loss: 0.3495\n",
            "Epoch 7/30, Loss: 0.4397\n",
            "Epoch 8/30, Loss: 0.2748\n",
            "Epoch 9/30, Loss: 0.3350\n",
            "Epoch 10/30, Loss: 0.4023\n",
            "Epoch 11/30, Loss: 0.3315\n",
            "Epoch 12/30, Loss: 0.4149\n",
            "Epoch 13/30, Loss: 0.3677\n",
            "Epoch 14/30, Loss: 0.3536\n",
            "Epoch 15/30, Loss: 0.3270\n",
            "Epoch 16/30, Loss: 0.2850\n",
            "Epoch 17/30, Loss: 0.4139\n",
            "Epoch 18/30, Loss: 0.2842\n",
            "Epoch 19/30, Loss: 0.3357\n",
            "Epoch 20/30, Loss: 0.4223\n",
            "Epoch 21/30, Loss: 0.3163\n",
            "Epoch 22/30, Loss: 0.2967\n",
            "Epoch 23/30, Loss: 0.4104\n",
            "Epoch 24/30, Loss: 0.4308\n",
            "Epoch 25/30, Loss: 0.3238\n",
            "Epoch 26/30, Loss: 0.2991\n",
            "Epoch 27/30, Loss: 0.3831\n",
            "Epoch 28/30, Loss: 0.3625\n",
            "Epoch 29/30, Loss: 0.4138\n",
            "Epoch 30/30, Loss: 0.4205\n",
            "F1: 0.0000\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4578\n",
            "Epoch 2/30, Loss: 0.3285\n",
            "Epoch 3/30, Loss: 0.3377\n",
            "Epoch 4/30, Loss: 0.3243\n",
            "Epoch 5/30, Loss: 0.3732\n",
            "Epoch 6/30, Loss: 0.2551\n",
            "Epoch 7/30, Loss: 0.3532\n",
            "Epoch 8/30, Loss: 0.3224\n",
            "Epoch 9/30, Loss: 0.4726\n",
            "Epoch 10/30, Loss: 0.3298\n",
            "Epoch 11/30, Loss: 0.3423\n",
            "Epoch 12/30, Loss: 0.3259\n",
            "Epoch 13/30, Loss: 0.3929\n",
            "Epoch 14/30, Loss: 0.3128\n",
            "Epoch 15/30, Loss: 0.3626\n",
            "Epoch 16/30, Loss: 0.3246\n",
            "Epoch 17/30, Loss: 0.3748\n",
            "Epoch 18/30, Loss: 0.3603\n",
            "Epoch 19/30, Loss: 0.1957\n",
            "Epoch 20/30, Loss: 0.3291\n",
            "Epoch 21/30, Loss: 0.2757\n",
            "Epoch 22/30, Loss: 0.3238\n",
            "Epoch 23/30, Loss: 0.3864\n",
            "Epoch 24/30, Loss: 0.2578\n",
            "Epoch 25/30, Loss: 0.3506\n",
            "Epoch 26/30, Loss: 0.2780\n",
            "Epoch 27/30, Loss: 0.3382\n",
            "Epoch 28/30, Loss: 0.2887\n",
            "Epoch 29/30, Loss: 0.2609\n",
            "Epoch 30/30, Loss: 0.3641\n",
            "F1: 0.0250\\n\n",
            "Best Parameters: {'d': 64, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Best F1: 0.0383\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'d': 64, 'heads': 2, 'layers': 2, 'ff': 64},\n",
              " {'Accuracy': 0.8682577565632458,\n",
              "  'Precision': 0.5238095238095238,\n",
              "  'Recall': 0.019891500904159132,\n",
              "  'F1': 0.03832752613240418,\n",
              "  'ROC_AUC': np.float64(0.7126638462138927),\n",
              "  'Runtime_s': 320.8132083415985})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE without embeddings"
      ],
      "metadata": {
        "id": "x5k6Dg8eHne4"
      },
      "id": "x5k6Dg8eHne4"
    },
    {
      "cell_type": "code",
      "source": [
        "ft_fn = lambda d, heads, layers, ff: FTTransformer(\n",
        "    n_feat=X_train_structured.shape[1], d=d, heads=heads, layers=layers, ff=ff\n",
        ")\n",
        "manual_grid_search(ft_fn, ft_grid, train_ft,\n",
        "                   X_train_structured, y_train, X_test_structured, y_test,\n",
        "                   score_key='F1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pQIEJ5wHnnN",
        "outputId": "0e61bb41-0acb-43bb-c41e-a718030c6955"
      },
      "id": "3pQIEJ5wHnnN",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'d': 32, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4617\n",
            "Epoch 2/30, Loss: 0.4016\n",
            "Epoch 3/30, Loss: 0.4108\n",
            "Epoch 4/30, Loss: 0.3859\n",
            "Epoch 5/30, Loss: 0.4300\n",
            "Epoch 6/30, Loss: 0.3646\n",
            "Epoch 7/30, Loss: 0.4258\n",
            "Epoch 8/30, Loss: 0.4068\n",
            "Epoch 9/30, Loss: 0.4197\n",
            "Epoch 10/30, Loss: 0.3299\n",
            "Epoch 11/30, Loss: 0.3811\n",
            "Epoch 12/30, Loss: 0.2757\n",
            "Epoch 13/30, Loss: 0.4052\n",
            "Epoch 14/30, Loss: 0.3530\n",
            "Epoch 15/30, Loss: 0.3823\n",
            "Epoch 16/30, Loss: 0.4156\n",
            "Epoch 17/30, Loss: 0.4342\n",
            "Epoch 18/30, Loss: 0.3895\n",
            "Epoch 19/30, Loss: 0.3768\n",
            "Epoch 20/30, Loss: 0.3817\n",
            "Epoch 21/30, Loss: 0.3236\n",
            "Epoch 22/30, Loss: 0.2994\n",
            "Epoch 23/30, Loss: 0.3456\n",
            "Epoch 24/30, Loss: 0.3095\n",
            "Epoch 25/30, Loss: 0.3767\n",
            "Epoch 26/30, Loss: 0.3536\n",
            "Epoch 27/30, Loss: 0.3182\n",
            "Epoch 28/30, Loss: 0.3765\n",
            "Epoch 29/30, Loss: 0.3512\n",
            "Epoch 30/30, Loss: 0.3458\n",
            "F1: 0.0108\\n\n",
            "Testing: {'d': 32, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.3707\n",
            "Epoch 2/30, Loss: 0.3697\n",
            "Epoch 3/30, Loss: 0.4123\n",
            "Epoch 4/30, Loss: 0.3798\n",
            "Epoch 5/30, Loss: 0.3285\n",
            "Epoch 6/30, Loss: 0.3858\n",
            "Epoch 7/30, Loss: 0.3508\n",
            "Epoch 8/30, Loss: 0.3782\n",
            "Epoch 9/30, Loss: 0.3209\n",
            "Epoch 10/30, Loss: 0.3624\n",
            "Epoch 11/30, Loss: 0.3372\n",
            "Epoch 12/30, Loss: 0.2700\n",
            "Epoch 13/30, Loss: 0.3263\n",
            "Epoch 14/30, Loss: 0.4480\n",
            "Epoch 15/30, Loss: 0.3471\n",
            "Epoch 16/30, Loss: 0.2971\n",
            "Epoch 17/30, Loss: 0.3983\n",
            "Epoch 18/30, Loss: 0.4122\n",
            "Epoch 19/30, Loss: 0.3898\n",
            "Epoch 20/30, Loss: 0.4278\n",
            "Epoch 21/30, Loss: 0.4664\n",
            "Epoch 22/30, Loss: 0.3853\n",
            "Epoch 23/30, Loss: 0.3128\n",
            "Epoch 24/30, Loss: 0.4080\n",
            "Epoch 25/30, Loss: 0.3838\n",
            "Epoch 26/30, Loss: 0.3622\n",
            "Epoch 27/30, Loss: 0.3715\n",
            "Epoch 28/30, Loss: 0.3041\n",
            "Epoch 29/30, Loss: 0.2984\n",
            "Epoch 30/30, Loss: 0.4176\n",
            "F1: 0.0177\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.2752\n",
            "Epoch 2/30, Loss: 0.4067\n",
            "Epoch 3/30, Loss: 0.2746\n",
            "Epoch 4/30, Loss: 0.3387\n",
            "Epoch 5/30, Loss: 0.3002\n",
            "Epoch 6/30, Loss: 0.2936\n",
            "Epoch 7/30, Loss: 0.3884\n",
            "Epoch 8/30, Loss: 0.3022\n",
            "Epoch 9/30, Loss: 0.3260\n",
            "Epoch 10/30, Loss: 0.3876\n",
            "Epoch 11/30, Loss: 0.2929\n",
            "Epoch 12/30, Loss: 0.3600\n",
            "Epoch 13/30, Loss: 0.3647\n",
            "Epoch 14/30, Loss: 0.4496\n",
            "Epoch 15/30, Loss: 0.4407\n",
            "Epoch 16/30, Loss: 0.3481\n",
            "Epoch 17/30, Loss: 0.4259\n",
            "Epoch 18/30, Loss: 0.3851\n",
            "Epoch 19/30, Loss: 0.3354\n",
            "Epoch 20/30, Loss: 0.2850\n",
            "Epoch 21/30, Loss: 0.3228\n",
            "Epoch 22/30, Loss: 0.3499\n",
            "Epoch 23/30, Loss: 0.4613\n",
            "Epoch 24/30, Loss: 0.2813\n",
            "Epoch 25/30, Loss: 0.3110\n",
            "Epoch 26/30, Loss: 0.3061\n",
            "Epoch 27/30, Loss: 0.2728\n",
            "Epoch 28/30, Loss: 0.3647\n",
            "Epoch 29/30, Loss: 0.3470\n",
            "Epoch 30/30, Loss: 0.2759\n",
            "F1: 0.0108\\n\n",
            "Testing: {'d': 32, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.3828\n",
            "Epoch 2/30, Loss: 0.3572\n",
            "Epoch 3/30, Loss: 0.2571\n",
            "Epoch 4/30, Loss: 0.3207\n",
            "Epoch 5/30, Loss: 0.3806\n",
            "Epoch 6/30, Loss: 0.2900\n",
            "Epoch 7/30, Loss: 0.4084\n",
            "Epoch 8/30, Loss: 0.3876\n",
            "Epoch 9/30, Loss: 0.3404\n",
            "Epoch 10/30, Loss: 0.4427\n",
            "Epoch 11/30, Loss: 0.3032\n",
            "Epoch 12/30, Loss: 0.3130\n",
            "Epoch 13/30, Loss: 0.2935\n",
            "Epoch 14/30, Loss: 0.3817\n",
            "Epoch 15/30, Loss: 0.3451\n",
            "Epoch 16/30, Loss: 0.2902\n",
            "Epoch 17/30, Loss: 0.3538\n",
            "Epoch 18/30, Loss: 0.4275\n",
            "Epoch 19/30, Loss: 0.3093\n",
            "Epoch 20/30, Loss: 0.4372\n",
            "Epoch 21/30, Loss: 0.3303\n",
            "Epoch 22/30, Loss: 0.3217\n",
            "Epoch 23/30, Loss: 0.4086\n",
            "Epoch 24/30, Loss: 0.3604\n",
            "Epoch 25/30, Loss: 0.3267\n",
            "Epoch 26/30, Loss: 0.3065\n",
            "Epoch 27/30, Loss: 0.4248\n",
            "Epoch 28/30, Loss: 0.4045\n",
            "Epoch 29/30, Loss: 0.3152\n",
            "Epoch 30/30, Loss: 0.2978\n",
            "F1: 0.0571\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.3992\n",
            "Epoch 2/30, Loss: 0.4250\n",
            "Epoch 3/30, Loss: 0.3465\n",
            "Epoch 4/30, Loss: 0.3251\n",
            "Epoch 5/30, Loss: 0.3785\n",
            "Epoch 6/30, Loss: 0.3465\n",
            "Epoch 7/30, Loss: 0.4048\n",
            "Epoch 8/30, Loss: 0.4335\n",
            "Epoch 9/30, Loss: 0.3522\n",
            "Epoch 10/30, Loss: 0.4366\n",
            "Epoch 11/30, Loss: 0.3208\n",
            "Epoch 12/30, Loss: 0.4172\n",
            "Epoch 13/30, Loss: 0.3136\n",
            "Epoch 14/30, Loss: 0.4209\n",
            "Epoch 15/30, Loss: 0.3625\n",
            "Epoch 16/30, Loss: 0.3494\n",
            "Epoch 17/30, Loss: 0.3654\n",
            "Epoch 18/30, Loss: 0.3322\n",
            "Epoch 19/30, Loss: 0.2724\n",
            "Epoch 20/30, Loss: 0.3958\n",
            "Epoch 21/30, Loss: 0.3915\n",
            "Epoch 22/30, Loss: 0.3952\n",
            "Epoch 23/30, Loss: 0.3103\n",
            "Epoch 24/30, Loss: 0.3234\n",
            "Epoch 25/30, Loss: 0.3735\n",
            "Epoch 26/30, Loss: 0.4301\n",
            "Epoch 27/30, Loss: 0.3077\n",
            "Epoch 28/30, Loss: 0.3350\n",
            "Epoch 29/30, Loss: 0.4249\n",
            "Epoch 30/30, Loss: 0.3546\n",
            "F1: 0.0143\\n\n",
            "Testing: {'d': 64, 'heads': 2, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4272\n",
            "Epoch 2/30, Loss: 0.3927\n",
            "Epoch 3/30, Loss: 0.3796\n",
            "Epoch 4/30, Loss: 0.3416\n",
            "Epoch 5/30, Loss: 0.3679\n",
            "Epoch 6/30, Loss: 0.2622\n",
            "Epoch 7/30, Loss: 0.3696\n",
            "Epoch 8/30, Loss: 0.4222\n",
            "Epoch 9/30, Loss: 0.3340\n",
            "Epoch 10/30, Loss: 0.4256\n",
            "Epoch 11/30, Loss: 0.3153\n",
            "Epoch 12/30, Loss: 0.3906\n",
            "Epoch 13/30, Loss: 0.3626\n",
            "Epoch 14/30, Loss: 0.4181\n",
            "Epoch 15/30, Loss: 0.4060\n",
            "Epoch 16/30, Loss: 0.4153\n",
            "Epoch 17/30, Loss: 0.4356\n",
            "Epoch 18/30, Loss: 0.3096\n",
            "Epoch 19/30, Loss: 0.2987\n",
            "Epoch 20/30, Loss: 0.3304\n",
            "Epoch 21/30, Loss: 0.3231\n",
            "Epoch 22/30, Loss: 0.4050\n",
            "Epoch 23/30, Loss: 0.3471\n",
            "Epoch 24/30, Loss: 0.3142\n",
            "Epoch 25/30, Loss: 0.3714\n",
            "Epoch 26/30, Loss: 0.3030\n",
            "Epoch 27/30, Loss: 0.4063\n",
            "Epoch 28/30, Loss: 0.3783\n",
            "Epoch 29/30, Loss: 0.3303\n",
            "Epoch 30/30, Loss: 0.3981\n",
            "F1: 0.0351\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 1, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4161\n",
            "Epoch 2/30, Loss: 0.4161\n",
            "Epoch 3/30, Loss: 0.4645\n",
            "Epoch 4/30, Loss: 0.4112\n",
            "Epoch 5/30, Loss: 0.3158\n",
            "Epoch 6/30, Loss: 0.3040\n",
            "Epoch 7/30, Loss: 0.2733\n",
            "Epoch 8/30, Loss: 0.3864\n",
            "Epoch 9/30, Loss: 0.3436\n",
            "Epoch 10/30, Loss: 0.3426\n",
            "Epoch 11/30, Loss: 0.4838\n",
            "Epoch 12/30, Loss: 0.3790\n",
            "Epoch 13/30, Loss: 0.3690\n",
            "Epoch 14/30, Loss: 0.3643\n",
            "Epoch 15/30, Loss: 0.3288\n",
            "Epoch 16/30, Loss: 0.3633\n",
            "Epoch 17/30, Loss: 0.2316\n",
            "Epoch 18/30, Loss: 0.3219\n",
            "Epoch 19/30, Loss: 0.3299\n",
            "Epoch 20/30, Loss: 0.3065\n",
            "Epoch 21/30, Loss: 0.3363\n",
            "Epoch 22/30, Loss: 0.3620\n",
            "Epoch 23/30, Loss: 0.3803\n",
            "Epoch 24/30, Loss: 0.3130\n",
            "Epoch 25/30, Loss: 0.2641\n",
            "Epoch 26/30, Loss: 0.3298\n",
            "Epoch 27/30, Loss: 0.3547\n",
            "Epoch 28/30, Loss: 0.2958\n",
            "Epoch 29/30, Loss: 0.4367\n",
            "Epoch 30/30, Loss: 0.5001\n",
            "F1: 0.0108\\n\n",
            "Testing: {'d': 64, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Epoch 1/30, Loss: 0.4504\n",
            "Epoch 2/30, Loss: 0.4393\n",
            "Epoch 3/30, Loss: 0.4419\n",
            "Epoch 4/30, Loss: 0.4640\n",
            "Epoch 5/30, Loss: 0.3291\n",
            "Epoch 6/30, Loss: 0.4271\n",
            "Epoch 7/30, Loss: 0.4381\n",
            "Epoch 8/30, Loss: 0.3075\n",
            "Epoch 9/30, Loss: 0.3661\n",
            "Epoch 10/30, Loss: 0.3666\n",
            "Epoch 11/30, Loss: 0.2776\n",
            "Epoch 12/30, Loss: 0.3150\n",
            "Epoch 13/30, Loss: 0.4770\n",
            "Epoch 14/30, Loss: 0.3315\n",
            "Epoch 15/30, Loss: 0.3486\n",
            "Epoch 16/30, Loss: 0.3673\n",
            "Epoch 17/30, Loss: 0.4483\n",
            "Epoch 18/30, Loss: 0.3970\n",
            "Epoch 19/30, Loss: 0.4071\n",
            "Epoch 20/30, Loss: 0.3957\n",
            "Epoch 21/30, Loss: 0.3757\n",
            "Epoch 22/30, Loss: 0.2801\n",
            "Epoch 23/30, Loss: 0.2989\n",
            "Epoch 24/30, Loss: 0.4367\n",
            "Epoch 25/30, Loss: 0.3570\n",
            "Epoch 26/30, Loss: 0.4279\n",
            "Epoch 27/30, Loss: 0.3584\n",
            "Epoch 28/30, Loss: 0.5808\n",
            "Epoch 29/30, Loss: 0.3443\n",
            "Epoch 30/30, Loss: 0.2358\n",
            "F1: 0.0825\\n\n",
            "Best Parameters: {'d': 64, 'heads': 4, 'layers': 2, 'ff': 64}\n",
            "Best F1: 0.0825\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'d': 64, 'heads': 4, 'layers': 2, 'ff': 64},\n",
              " {'Accuracy': 0.8673031026252983,\n",
              "  'Precision': 0.4716981132075472,\n",
              "  'Recall': 0.045207956600361664,\n",
              "  'F1': 0.08250825082508251,\n",
              "  'ROC_AUC': np.float64(0.6882771554760918),\n",
              "  'Runtime_s': 17.822336435317993})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-layer perceptron"
      ],
      "metadata": {
        "id": "kvKwXfyKSo5P"
      },
      "id": "kvKwXfyKSo5P"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers=(64, 32)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_layers:\n",
        "            layers.extend([nn.Linear(prev_dim, h), nn.ReLU()])\n",
        "            prev_dim = h\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "OnoNcGQg9Nw9"
      },
      "id": "OnoNcGQg9Nw9",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_torch_mlp(model, X_train, y_train, X_test, y_test, sample_weights, n_epochs=30, lr=1e-3):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    weights_tensor = torch.tensor(sample_weights, dtype=torch.float32)\n",
        "\n",
        "    train_ds = TensorDataset(X_train_tensor, y_train_tensor, weights_tensor)\n",
        "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    def weighted_bce(logits, targets, weights):\n",
        "        loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
        "        return (loss * weights).mean()\n",
        "\n",
        "    start = time.time()\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for xb, yb, wb in train_dl:\n",
        "            xb, yb, wb = xb.to(device), yb.to(device), wb.to(device).unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = weighted_bce(logits, yb, wb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prob = torch.sigmoid(model(torch.tensor(X_test, dtype=torch.float32).to(device))).cpu().numpy().ravel()\n",
        "    runtime = time.time() - start\n",
        "    preds = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, preds, prob, runtime)\n"
      ],
      "metadata": {
        "id": "XEkqeKsk9OBx"
      },
      "id": "XEkqeKsk9OBx",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "class_weight_dict = {0: 1.0, 1: pos_weight}\n",
        "sample_weights = np.array([class_weight_dict[y] for y in y_train])\n",
        "\n",
        "model = SimpleMLP(input_dim=X_train.shape[1])\n",
        "model, metrics = train_torch_mlp(model, X_train, y_train, X_test, y_test, sample_weights)\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlS9Jdq09OGW",
        "outputId": "284a3e72-d7e2-4102-ee03-f82ba1554c1c"
      },
      "id": "KlS9Jdq09OGW",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Accuracy': 0.681145584725537, 'Precision': 0.23742454728370221, 'Recall': 0.6401446654611211, 'F1': 0.34637964774951074, 'ROC_AUC': np.float64(0.7173253496189703), 'Runtime_s': 10.50873351097107}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_grid = [\n",
        "    {\n",
        "        'hidden_layer_sizes': h,\n",
        "        'lr': lr,\n",
        "        'alpha': alpha  # L2 regularization (weight decay)\n",
        "    }\n",
        "    for h in [(64, 32), (128, 64), (128, 64, 32)]\n",
        "    for alpha in [0.0001, 0.001, 0.01]\n",
        "    for lr in [0.0005, 0.001, 0.005]\n",
        "]\n"
      ],
      "metadata": {
        "id": "pJTxv5l59uMV"
      },
      "id": "pJTxv5l59uMV",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_fn = lambda hidden_layer_sizes, lr, alpha: (\n",
        "    SimpleMLP(X_train.shape[1], hidden_layer_sizes),\n",
        "    lr,\n",
        "    alpha\n",
        ")\n"
      ],
      "metadata": {
        "id": "OjlvGCHS9wI5"
      },
      "id": "OjlvGCHS9wI5",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pytorch_mlp_grid(model_tuple, X_train, y_train, X_test, y_test):\n",
        "    model, lr, alpha = model_tuple\n",
        "\n",
        "    pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "    class_weight_dict = {0: 1.0, 1: pos_weight}\n",
        "    sample_weights = np.array([class_weight_dict[y] for y in y_train])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    weights_tensor = torch.tensor(sample_weights, dtype=torch.float32)\n",
        "\n",
        "    train_ds = TensorDataset(X_train_tensor, y_train_tensor, weights_tensor)\n",
        "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n",
        "\n",
        "    def weighted_bce(logits, targets, weights):\n",
        "        loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
        "        return (loss * weights).mean()\n",
        "\n",
        "    start = time.time()\n",
        "    for epoch in range(30):\n",
        "        model.train()\n",
        "        for xb, yb, wb in train_dl:\n",
        "            xb, yb, wb = xb.to(device), yb.to(device), wb.to(device).unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = weighted_bce(logits, yb, wb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n",
        "\n",
        "    runtime = time.time() - start\n",
        "    preds = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, preds, prob, runtime)\n"
      ],
      "metadata": {
        "id": "1W8Wyis89wMk"
      },
      "id": "1W8Wyis89wMk",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE with embeddings"
      ],
      "metadata": {
        "id": "uONn20qyFBmw"
      },
      "id": "uONn20qyFBmw"
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_fn = lambda hidden_layer_sizes, lr, alpha: (\n",
        "    SimpleMLP(X_train.shape[1], hidden_layer_sizes),\n",
        "    lr,\n",
        "    alpha\n",
        ")\n",
        "\n",
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=mlp_fn,\n",
        "    param_grid_list=mlp_grid,\n",
        "    train_fn=train_pytorch_mlp_grid,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T841y8l39wTF",
        "outputId": "1ed41f13-ee18-4010-b0a7-53ec2916b5a6"
      },
      "id": "T841y8l39wTF",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3258\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3359\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3211\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3293\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3225\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3251\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3441\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3305\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3152\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3350\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3251\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3295\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3301\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3202\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3193\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3309\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3224\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3206\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3300\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3281\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3163\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3278\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3303\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3240\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3322\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3373\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.01}\n",
            "F1: 0.3322\\n\n",
            "Best Parameters: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.01}\n",
            "Best F1: 0.3441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted BCE without embeddings"
      ],
      "metadata": {
        "id": "QSq7l_euE_s0"
      },
      "id": "QSq7l_euE_s0"
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_fn = lambda hidden_layer_sizes, lr, alpha: (\n",
        "    SimpleMLP(X_train_structured.shape[1], hidden_layer_sizes),\n",
        "    lr,\n",
        "    alpha\n",
        ")\n",
        "\n",
        "\n",
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=mlp_fn,\n",
        "    param_grid_list=mlp_grid,\n",
        "    train_fn=train_pytorch_mlp_grid,\n",
        "    X_train=X_train_structured,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test_structured,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nehgC5Pv-DBo",
        "outputId": "37f4a486-4bdc-4603-e802-7e07a7e844b8"
      },
      "id": "nehgC5Pv-DBo",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3202\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3104\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2847\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3256\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3059\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2889\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3284\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3174\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2805\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3089\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3105\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2736\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3338\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3002\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2573\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3146\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2974\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2759\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2979\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3034\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2697\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3036\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2974\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2826\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.3136\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2818\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.01}\n",
            "F1: 0.2669\\n\n",
            "Best Parameters: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.001}\n",
            "Best F1: 0.3338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3da91c92d96f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pytorch_mlp_grid(model_tuple, X_train, y_train, X_test, y_test):\n",
        "    model, lr, alpha = model_tuple\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    start = time.time()\n",
        "    for epoch in range(30):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n",
        "\n",
        "    runtime = time.time() - start\n",
        "    preds = (prob >= 0.5).astype(int)\n",
        "    return model, metric_dict(y_test, preds, prob, runtime)\n"
      ],
      "metadata": {
        "id": "WOPeP83vEo68"
      },
      "id": "WOPeP83vEo68",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE with embeddings"
      ],
      "metadata": {
        "id": "JQg_4iRfEmOI"
      },
      "id": "JQg_4iRfEmOI"
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_fn = lambda hidden_layer_sizes, lr, alpha: (\n",
        "    SimpleMLP(X_train.shape[1], hidden_layer_sizes),\n",
        "    lr,\n",
        "    alpha\n",
        ")\n",
        "\n",
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=mlp_fn,\n",
        "    param_grid_list=mlp_grid,\n",
        "    train_fn=train_pytorch_mlp_grid,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsr6DZz4OmS9",
        "outputId": "877f85df-0900-48e5-9944-2943b1e3846f"
      },
      "id": "fsr6DZz4OmS9",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1175\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1088\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0983\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0348\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1167\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2208\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0960\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0862\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1005\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1213\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2320\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2182\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1277\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2527\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1674\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1254\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2155\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1536\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1852\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1024\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1424\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1300\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1667\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1453\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0886\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1300\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.01}\n",
            "F1: 0.1624\\n\n",
            "Best Parameters: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.001}\n",
            "Best F1: 0.2527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard BCE w/o embeddings"
      ],
      "metadata": {
        "id": "7-FiLmKaE2De"
      },
      "id": "7-FiLmKaE2De"
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_fn = lambda hidden_layer_sizes, lr, alpha: (\n",
        "    SimpleMLP(X_train_structured.shape[1], hidden_layer_sizes),\n",
        "    lr,\n",
        "    alpha\n",
        ")\n",
        "\n",
        "\n",
        "best_params, best_metrics = manual_grid_search(\n",
        "    model_fn=mlp_fn,\n",
        "    param_grid_list=mlp_grid,\n",
        "    train_fn=train_pytorch_mlp_grid,\n",
        "    X_train=X_train_structured,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test_structured,\n",
        "    y_test=y_test,\n",
        "    score_key='F1'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMxSXFiwE1TS",
        "outputId": "b7a29643-f5b1-44f0-dcf6-c0ec60ab8900"
      },
      "id": "XMxSXFiwE1TS",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0833\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0960\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1377\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0797\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1265\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1728\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1016\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0909\\n\n",
            "Testing: {'hidden_layer_sizes': (64, 32), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1632\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0575\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1673\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1865\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0912\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1131\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2081\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.0956\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1022\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.2077\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1223\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1721\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1803\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1315\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1528\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1659\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.0005, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1673\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.001, 'alpha': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.1917\\n\n",
            "Testing: {'hidden_layer_sizes': (128, 64, 32), 'lr': 0.005, 'alpha': 0.01}\n",
            "F1: 0.1922\\n\n",
            "Best Parameters: {'hidden_layer_sizes': (128, 64), 'lr': 0.005, 'alpha': 0.001}\n",
            "Best F1: 0.2081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-747c523b8164>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.sigmoid(torch.tensor(model(torch.tensor(X_test, dtype=torch.float32).to(device)))).cpu().numpy().ravel()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wchRw722E4hP"
      },
      "id": "wchRw722E4hP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "hl9N_zIrSVjr",
        "kvKwXfyKSo5P"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c21f92b8018345058b8e5c5a59b1ae94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f725be7102a4e9595e64f14a77a3941",
              "IPY_MODEL_8c585ba63ae84cf28d879c1cfe445f5d",
              "IPY_MODEL_a99e4cc07b9940ada94b459c9018c90d"
            ],
            "layout": "IPY_MODEL_8443279c10364026b3550c0e1268edcd"
          }
        },
        "1f725be7102a4e9595e64f14a77a3941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea96df16b2004df4861062ee2023cbed",
            "placeholder": "​",
            "style": "IPY_MODEL_308f3c686c324f8aa66edc82f06f7f07",
            "value": "config.json: 100%"
          }
        },
        "8c585ba63ae84cf28d879c1cfe445f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a4fcc7f171e4336a9337321e86ba40f",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9adfb873bbf44d068aedbf0cc3eca483",
            "value": 385
          }
        },
        "a99e4cc07b9940ada94b459c9018c90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9ac14ad0b74ceebc0ccb378c46d1c6",
            "placeholder": "​",
            "style": "IPY_MODEL_c489b9472fc449b69951cafa9bf567f1",
            "value": " 385/385 [00:00&lt;00:00, 45.7kB/s]"
          }
        },
        "8443279c10364026b3550c0e1268edcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea96df16b2004df4861062ee2023cbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "308f3c686c324f8aa66edc82f06f7f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a4fcc7f171e4336a9337321e86ba40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9adfb873bbf44d068aedbf0cc3eca483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e9ac14ad0b74ceebc0ccb378c46d1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c489b9472fc449b69951cafa9bf567f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb0281855bf04025b016a9e7540da629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e986e60c6cf44f6a92729ace62ca3cb",
              "IPY_MODEL_3fcfc3ab4b85444ba038e4e4bb9d3fbf",
              "IPY_MODEL_f15e958245b341c09eaa75c1f62cbe7d"
            ],
            "layout": "IPY_MODEL_e916b6fd40e6457a9e25c9d7eb3c121b"
          }
        },
        "8e986e60c6cf44f6a92729ace62ca3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fbb4c14ce954cf9a28973f43d0d6442",
            "placeholder": "​",
            "style": "IPY_MODEL_eb3e87111e6046f5aeb295c0ff8419a2",
            "value": "vocab.txt: 100%"
          }
        },
        "3fcfc3ab4b85444ba038e4e4bb9d3fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb3d373ac40744e09bfd00241c7c2179",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38560c946b15427192506e07ff0d0efb",
            "value": 213450
          }
        },
        "f15e958245b341c09eaa75c1f62cbe7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ba727246cd048a68e96c8a230a0ef02",
            "placeholder": "​",
            "style": "IPY_MODEL_ec6350facef44aeea6abb70abbd3e706",
            "value": " 213k/213k [00:00&lt;00:00, 7.17MB/s]"
          }
        },
        "e916b6fd40e6457a9e25c9d7eb3c121b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fbb4c14ce954cf9a28973f43d0d6442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb3e87111e6046f5aeb295c0ff8419a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb3d373ac40744e09bfd00241c7c2179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38560c946b15427192506e07ff0d0efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ba727246cd048a68e96c8a230a0ef02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec6350facef44aeea6abb70abbd3e706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5cce1173a434a09968b17a2c23eaebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b40710d99b1c4b119fa1b3052995ba43",
              "IPY_MODEL_8761f49979ed484eab194fcfc2a4d858",
              "IPY_MODEL_8481f6eecfce47c38d816c053b210741"
            ],
            "layout": "IPY_MODEL_f40ad7eae10f46bc885c3aa38ac091e6"
          }
        },
        "b40710d99b1c4b119fa1b3052995ba43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61409a6c1fc64321838d5a7629675c20",
            "placeholder": "​",
            "style": "IPY_MODEL_c8b8f43bf1c742f89211a2cab87d2987",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8761f49979ed484eab194fcfc2a4d858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a5cb46bf5f4c348e67cdf681ea69d2",
            "max": 435778770,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_805b6750c1f44360813fc7fd334620f0",
            "value": 435778770
          }
        },
        "8481f6eecfce47c38d816c053b210741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2277fcbd1a4b4124ab8f86c81fde97cd",
            "placeholder": "​",
            "style": "IPY_MODEL_44fc305f713d454cbfe97d7df0ba54a2",
            "value": " 436M/436M [00:01&lt;00:00, 309MB/s]"
          }
        },
        "f40ad7eae10f46bc885c3aa38ac091e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61409a6c1fc64321838d5a7629675c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b8f43bf1c742f89211a2cab87d2987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01a5cb46bf5f4c348e67cdf681ea69d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805b6750c1f44360813fc7fd334620f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2277fcbd1a4b4124ab8f86c81fde97cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44fc305f713d454cbfe97d7df0ba54a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba0a0de6d7da4333b2ff55b063404361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae31b37030da4652b7efaa2e555815ea",
              "IPY_MODEL_48d7fec75a4941258209a5755a4d6a6f",
              "IPY_MODEL_0a623ee1b7a84cd281589fb668d26fb2"
            ],
            "layout": "IPY_MODEL_d631c9b673f048e0ae50a9b0a7001beb"
          }
        },
        "ae31b37030da4652b7efaa2e555815ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f207d3ae9ede4ab594603223ddfdd5b8",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e707a16f4646a783f6fbe2c4b8fcbf",
            "value": "model.safetensors: 100%"
          }
        },
        "48d7fec75a4941258209a5755a4d6a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13e272dec364caf805525234291a901",
            "max": 435755888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2aff07fd629b4ab59b5a384290c07fa4",
            "value": 435755888
          }
        },
        "0a623ee1b7a84cd281589fb668d26fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caa2ed2073514e86a338028a1c3e6883",
            "placeholder": "​",
            "style": "IPY_MODEL_52e6d07630ad46e3935f2352408291ab",
            "value": " 436M/436M [00:01&lt;00:00, 271MB/s]"
          }
        },
        "d631c9b673f048e0ae50a9b0a7001beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f207d3ae9ede4ab594603223ddfdd5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e707a16f4646a783f6fbe2c4b8fcbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a13e272dec364caf805525234291a901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aff07fd629b4ab59b5a384290c07fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caa2ed2073514e86a338028a1c3e6883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52e6d07630ad46e3935f2352408291ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}